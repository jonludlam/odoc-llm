{
  "package": "dolmen",
  "embedding_model": "Qwen/Qwen3-Embedding-8B",
  "embedding_dimension": 4096,
  "total_modules": 125,
  "creation_timestamp": "2025-08-18T19:04:54.492889",
  "modules": [
    {
      "module_path": "Dolmen_intf.Stmt",
      "library": "dolmen.intf",
      "description": "This module defines interfaces for top-level statements and declarations in files. It includes operations for handling logical assertions, responses, and command interactions in a proof or logic processing context. It works with abstract syntax trees and logical expressions, enabling concrete implementations for parsing, evaluating, and responding to statements in formal verification tools.",
      "description_length": 393,
      "index": 0,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Lex",
      "library": "dolmen.intf",
      "description": "This module defines the interface for lexers used in parsing first-order logic and SMT-LIB input formats. It includes functions for tokenizing input streams and handling lexical states, working with character streams and token types. Concrete use cases include implementing custom lexers for SMT solvers and formal verification tools.",
      "description_length": 334,
      "index": 1,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term",
      "library": "dolmen.intf",
      "description": "The interfaces define operations for logical reasoning and term manipulation in formats like TFF, THF, Dimacs, and SMT-LIB, with support for arithmetic, bitvectors, arrays, strings",
      "description_length": 180,
      "index": 2,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Parse",
      "library": "dolmen.intf",
      "description": "This module defines the interface for parsers used in the Dolmen library. It includes functions for parsing input streams into structured data, handling errors, and tracking positions within the input. It works with token streams and position markers to support precise error reporting and incremental parsing.",
      "description_length": 310,
      "index": 3,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty",
      "library": "dolmen.intf",
      "description": "This module defines a collection of type interfaces for formal logic and constraint systems, including first-order logic, higher-order logic, SMT-LIB, TPTP, and ZF set theory. It specifies signatures for base types, arithmetic, arrays, bitvectors, floating-point numbers, and strings, ensuring consistent type representations across different logical frameworks. Concrete use cases include building and verifying type-correct expressions in theorem provers, SMT solvers, and formal verification tools.",
      "description_length": 501,
      "index": 4,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Map",
      "library": "dolmen.intf",
      "description": "This module defines a map interface with operations for creating, querying, and transforming associative collections. It works with key-value pairs where keys are ordered and values can be arbitrary. Concrete use cases include symbol table management, configuration settings storage, and tracking state changes in interpreters or compilers.",
      "description_length": 340,
      "index": 5,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Id",
      "library": "dolmen.intf",
      "description": "This module defines interfaces for identifier implementations used in logic languages, specifying required operations through signatures like `Logic` and `Escape`. It works with abstract identifier types that support comparison, hashing, and scoped naming operations. Concrete use cases include defining symbol tables for formal languages and managing bound variables in theorem proving or logic programming systems.",
      "description_length": 416,
      "index": 6,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Language",
      "library": "dolmen.intf",
      "description": "Defines a common interface for language modules, specifying required components like term representations, parsing, and pretty-printing functions. Works with abstract syntax trees and language-specific data structures. Enables consistent handling of different formal languages within the Dolmen ecosystem, such as for logic encodings or proof formats.",
      "description_length": 351,
      "index": 7,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Tok",
      "library": "dolmen.intf",
      "description": "This module defines a `descr` record type for representing token descriptions, including fields like `article`, `kind`, `lexeme`, and an optional `hint`. It is used to provide structured metadata about tokens in parsing or lexing workflows. Concrete use cases include describing lexical elements in compilers, interpreters, or syntax highlighters.",
      "description_length": 347,
      "index": 8,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Location",
      "library": "dolmen.intf",
      "description": "This module defines data types and operations for representing and manipulating file locations, primarily used by parsers to track positions of expressions in source files. It includes functions for creating, comparing, and combining location information, as well as exceptions that carry location data to report parsing errors. Concrete use cases include error reporting in parsers, tracking source code spans for diagnostics, and enabling precise location-based transformations in language tools.",
      "description_length": 498,
      "index": 9,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Pretty",
      "library": "dolmen.intf",
      "description": "This module defines types for specifying pretty-printing annotations, including name handling, operator position, and associativity. It works with strings and custom types to represent formatting directives for printers. Concrete use cases include configuring how expressions or operators are displayed in parsed or generated code.",
      "description_length": 331,
      "index": 10,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Msg",
      "library": "dolmen.intf",
      "description": "This module provides operations to create and manipulate delayed-formatted messages using closures that write to a formatter. It works with the type `t`, which represents functions taking a formatter and producing output, enabling proper composition of messages with Format's boxes and break hints. Concrete use cases include building structured error messages, logging components, and pretty-printed output where nested message formatting must preserve layout semantics.",
      "description_length": 471,
      "index": 11,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ext",
      "library": "dolmen.intf",
      "description": "This module defines interfaces for extending logic systems, focusing on propositional and first-order logic operations. It works with logical expressions, terms, and proof contexts to support tasks like formula manipulation, theorem proving, and solver integration. Concrete use cases include implementing custom logic transformations, building proof assistants, and extending existing theorem provers with domain-specific reasoning.",
      "description_length": 433,
      "index": 12,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Tag",
      "library": "dolmen.intf",
      "description": "This module defines interfaces for attaching metadata to AST nodes using tags. It includes specialized tag types for different logical frameworks, such as SMT-LIB and ZF. Concrete use cases include tracking source locations, annotations, or solver-specific attributes in formal verification tools.",
      "description_length": 297,
      "index": 13,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf",
      "library": "dolmen.intf",
      "description": "This module provides core interfaces for building and manipulating formal logic systems. It includes components for parsing, lexing, and representing logical terms, types, and statements, along with utilities for identifiers, maps, and pretty-printing. Concrete use cases include implementing theorem provers, SMT solvers, and formal verification tools that require precise handling of logical expressions and structured input/output.",
      "description_length": 434,
      "index": 14,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_dimacs.Make.Parser",
      "library": "dolmen.dimacs",
      "description": "Parses DIMACS input files into lists of logical statements using lexer functions. It operates on `lexbuf` streams and produces `statement` values, either for an entire file or incrementally per statement. Useful for reading SAT problem instances where each clause is represented as a sequence of integers followed by zero.",
      "description_length": 322,
      "index": 15,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_dimacs.Make.Lexer",
      "library": "dolmen.dimacs",
      "description": "Handles lexical analysis of DIMACS input by converting character streams into tokens. It processes `Lexing.lexbuf` input using the `token` function and associates descriptive error information with each token via `descr`. This module is used to parse DIMACS formulas by transforming raw input into structured tokens for further processing.",
      "description_length": 339,
      "index": 16,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_dimacs.Make",
      "library": "dolmen.dimacs",
      "description": "This module generates a parser for the DIMACS format, which is used to represent SAT problem instances. It processes input streams into structured tokens and logical statements, supporting both full and incremental parsing of DIMACS files. Key operations include file resolution, complete parsing of DIMACS content, and incremental statement extraction, making it suitable for handling large or interactive SAT solver inputs.",
      "description_length": 425,
      "index": 17,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_dimacs",
      "library": "dolmen.dimacs",
      "description": "Handles parsing and processing of DIMACS-formatted input for SAT solving. Provides functions to read clauses, interpret logical statements, and build term structures from input streams. Works with propositional logic expressions represented in CNF (Conjunctive Normal Form). Useful for interfacing with SAT solvers or analyzing logical formulas in DIMACS format.",
      "description_length": 362,
      "index": 18,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_ae.Make.Parser",
      "library": "dolmen.ae",
      "description": "Parses Alt-ergo input files or individual statements from a lexing buffer. It produces a list of statements for full files or an optional statement for incremental parsing. Designed for processing logical expressions and commands in the Alt-ergo native format.",
      "description_length": 260,
      "index": 19,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_ae.Make.Lexer",
      "library": "dolmen.ae",
      "description": "This module defines the lexical analysis component for parsing Alt-ergo's native format, handling the conversion of character streams into tokens. It includes functions for token generation from lexing buffers and associating descriptive error messages with tokens. It is used during input parsing to break down source code into meaningful syntactic units for further processing.",
      "description_length": 379,
      "index": 20,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_ae.Make",
      "library": "dolmen.ae",
      "description": "This module generates a parser for the native Alt-ergo input format, producing statement lists from files, stdin, or string contents. It supports full and incremental parsing workflows, with error handling and token management. Concrete use cases include reading and processing logical expressions and commands from Alt-ergo source files or interactive input streams.",
      "description_length": 367,
      "index": 21,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_ae",
      "library": "dolmen.ae",
      "description": "This module defines the core components for parsing and representing Alt-ergo input files, including identifiers, terms, and statements. It provides typed interfaces for constructing and manipulating logical expressions and declarations specific to the Alt-ergo language. Concrete use cases include building abstract syntax trees (ASTs) for theorem proving tasks and enabling precise translation of Alt-ergo source code into internal representations for solver interaction.",
      "description_length": 473,
      "index": 22,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_tptp",
      "library": "dolmen.tptp",
      "description": "Handles parsing and pretty-printing of TPTP (Thousands of Problems for Theorem Provers) format, specifically supporting version 6.3.0 and the latest available version. It works with abstract syntax trees representing logical formulas and problem statements. Enables reading and writing TPTP files for automated theorem proving tasks, such as encoding first-order logic problems or importing existing TPTP benchmarks.",
      "description_length": 416,
      "index": 23,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_zf.Make.Parser",
      "library": "dolmen.zf",
      "description": "Parses Zipperposition format input into statements using lexer buffers. It supports parsing entire files into a list of statements or incrementally parsing one statement at a time. The module works with token streams and statement structures defined by the language implementation.",
      "description_length": 281,
      "index": 24,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_zf.Make.Lexer",
      "library": "dolmen.zf",
      "description": "This module defines the lexical analysis component for parsing the Zipperposition format. It provides functions to convert raw input into structured tokens and associate descriptive error information with each token. The primary operations include token generation from a lexing buffer and mapping tokens to error descriptions, working directly with `token` and `lexbuf` types. It is used to process input streams into tokens for further parsing stages.",
      "description_length": 453,
      "index": 25,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_zf.Make",
      "library": "dolmen.zf",
      "description": "This module generates a parser for the Zipperposition format, providing functions to parse input files, standard input, or string contents into a lazy list or incrementally process statements. It works with token streams and statement structures through the Lexer and Parser modules, handling lexical analysis and error recovery during parsing. Concrete use cases include reading and parsing Zipperposition format files incrementally or all at once, supporting scenarios like interactive input processing or batch file analysis.",
      "description_length": 528,
      "index": 26,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_zf",
      "library": "dolmen.zf",
      "description": "Handles parsing and manipulation of logical terms and statements in the Zipperposition input format. Provides operations for constructing, analyzing, and transforming formulas, including variable binding, term comparison, and statement normalization. Works with identifiers, terms, and logical statements to support tasks like formula simplification and proof preprocessing.",
      "description_length": 374,
      "index": 27,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2.Response",
      "library": "dolmen.smtlib2",
      "description": "This module handles parsing and processing of SMT-LIBv2 response files, supporting operations to read and interpret solver outputs according to specified versions. It works with response data structures that represent solver statuses, models, and error messages. Concrete use cases include validating SMT solver outputs, extracting model values, and handling version-specific response formats during automated theorem proving tasks.",
      "description_length": 432,
      "index": 28,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2.Script",
      "library": "dolmen.smtlib2",
      "description": "Handles parsing and manipulation of SMT-LIBv2 script files, supporting version selection with concrete variants like `V2_6 and `Poly. It provides functions to read, write, and transform SMT scripts, enabling tasks like benchmark preprocessing or solver interaction. Useful for tools that require precise SMT-LIB syntax handling, such as theorem provers or formal verification pipelines.",
      "description_length": 386,
      "index": 29,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2",
      "library": "dolmen.smtlib2",
      "description": "This module provides versioned parsing and processing capabilities for SMT-LIBv2 scripts and responses, supporting concrete operations like reading, writing, and transforming SMT files and interpreting solver outputs. It works with versioned data structures representing script commands and response statuses, models, and errors. Concrete use cases include preprocessing SMT benchmarks, validating solver outputs, and extracting model values for formal verification tasks.",
      "description_length": 472,
      "index": 30,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_tptp_v6_3_0.Make.Lexer",
      "library": "dolmen_tptp_v6_3_0",
      "description": "This module defines the lexical analysis component for parsing TPTP input, focusing on token generation and error handling. It processes character streams using `lexbuf` to produce language-specific tokens and associates descriptive error information with each token. It is used to transform raw input into structured tokens that feed into the corresponding parser for TPTP formulae and commands.",
      "description_length": 396,
      "index": 31,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_tptp_v6_3_0.Make.Parser",
      "library": "dolmen_tptp_v6_3_0",
      "description": "Parses TPTP language input into statements using lexer functions. It processes lex buffers to produce either a list of statements from a complete file or individual statements incrementally. Useful for reading TPTP files or handling interactive input where statements are parsed one at a time.",
      "description_length": 293,
      "index": 32,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_tptp_v6_3_0.Make",
      "library": "dolmen_tptp_v6_3_0",
      "description": "This module generates a TPTP parser that processes character streams into structured tokens and statements. It supports full and incremental parsing of TPTP files, handling input from files, standard input, or string contents, and includes file resolution for include directives. Concrete use cases include reading and processing TPTP formulae and commands, either all at once or interactively line by line.",
      "description_length": 407,
      "index": 33,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_tptp_v6_3_0",
      "library": "dolmen_tptp_v6_3_0",
      "description": "This module parses and represents the TPTP language version 6.3.0, handling input of logical statements, terms, and identifiers. It processes first-order logic formulas, including quantifiers, predicates, and function applications, along with TPTP-specific constructs like include directives and problem statements. Use cases include reading and analyzing TPTP problem files for automated theorem proving and formal logic processing.",
      "description_length": 433,
      "index": 34,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Make.Lexer",
      "library": "dolmen_smtlib2_v6_script",
      "description": "This module defines the lexical analysis component for parsing SMT-LIB input. It provides functions to convert character streams into tokens and associate descriptive error information with each token. It operates on `lexbuf` structures from OCaml's standard library, producing `token` values specific to the SMT-LIB grammar. Use this module to tokenize SMT-LIB scripts during parsing, enabling error reporting with meaningful token descriptions.",
      "description_length": 446,
      "index": 35,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Make.Parser",
      "library": "dolmen_smtlib2_v6_script",
      "description": "Parses SMT-LIB v2.6 input files or individual statements from lexing buffers. It generates a list of parsed statements from a file or returns an optional statement for incremental parsing. Designed for processing SMT-LIB scripts in a streaming or batch manner.",
      "description_length": 260,
      "index": 36,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Print",
      "library": "dolmen_smtlib2_v6_script",
      "description": "Prints SMT-LIB identifiers, automatically quoting them when necessary according to lexical conventions. Works directly with formatted output channels and identifier values. Useful for generating valid SMT-LIB script output from internal name representations.",
      "description_length": 258,
      "index": 37,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Make",
      "library": "dolmen_smtlib2_v6_script",
      "description": "This module generates a parser for SMT-LIB v2.6 input, producing token streams and structured statements from files, standard input, or string contents. It supports both batch parsing of entire inputs and incremental parsing for streaming or interactive use, with error handling tied to lexical analysis. Concrete use cases include processing SMT solvers' input scripts, validating SMT-LIB syntax, and implementing language tools that require precise parsing of formal logic expressions.",
      "description_length": 487,
      "index": 38,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script",
      "library": "dolmen_smtlib2_v6_script",
      "description": "Handles parsing and printing of SMT-LIB identifiers and scripts, with support for automatic quoting based on lexical rules. Works directly with identifier values and formatted output channels to generate valid SMT-LIB script output. Enables precise script generation and manipulation for SMT solver interaction.",
      "description_length": 311,
      "index": 39,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_line",
      "library": "dolmen.line",
      "description": "Consumes all characters on the current line from a lexing buffer, using custom newline and synchronization actions. Works directly with `Lexing.lexbuf` to handle line-based input processing. Useful for parsers needing to skip or process entire lines during lexing.",
      "description_length": 264,
      "index": 40,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_icnf.Make.Parser",
      "library": "dolmen.icnf",
      "description": "Parses iCNF language input into structured statements using lexer buffers. It provides `file` to parse an entire file into a list of statements and `input` to parse one statement at a time, returning `None` at end-of-file. Works directly with `Lexing.lexbuf` and `statement` types, suitable for reading from files or interactive input streams.",
      "description_length": 343,
      "index": 41,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_icnf.Make.Lexer",
      "library": "dolmen.icnf",
      "description": "Handles lexical analysis of the iCNF format by converting input into tokens. It processes `lexbuf` streams and raises exceptions on invalid input. Associates descriptive error messages with each token type for parsing diagnostics.",
      "description_length": 230,
      "index": 42,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_icnf.Make",
      "library": "dolmen.icnf",
      "description": "This module generates a parser for the iCNF format, providing functions to parse files or input streams into structured statements. It works with `Lexing.lexbuf` for lexical analysis and processes input incrementally or all at once, returning lazy or step-by-step results. Concrete use cases include reading iCNF files, handling include directives with relative paths, and parsing large or interactive input such as from standard input.",
      "description_length": 436,
      "index": 43,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_icnf",
      "library": "dolmen.icnf",
      "description": "This module provides functions for parsing and processing formulas in the iCNF (Incremental Conjunctive Normal Form) format. It works with logical terms and statements represented as clauses, variables, and assertions. Concrete use cases include reading and validating iCNF input files for SAT solvers and formal verification tools.",
      "description_length": 332,
      "index": 44,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Response.Make",
      "library": "dolmen.class",
      "description": "This module implements a response language for formal proofs, providing functions to parse and process proof statements incrementally or in full from various input sources. It works with lex buffers, token streams, and structured proof statements, supporting operations like error recovery during parsing and file resolution for included content. Concrete use cases include reading and validating proof scripts from files or interactive sessions, handling large inputs efficiently, and managing include paths relative to source files.",
      "description_length": 534,
      "index": 45,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Logic.Make",
      "library": "dolmen.class",
      "description": "This module implements language-specific parsing for formal logic formats including SMT-LIB, TPTP, and DIMACS. It provides functions to parse entire files or streams into logic statements, supporting both batch and incremental parsing modes. Key operations include language detection from extensions, file resolution, and structured parsing with error handling, primarily used in proof script interpreters and logic verifiers.",
      "description_length": 426,
      "index": 46,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Response",
      "library": "dolmen.class",
      "description": "This module defines response formats for formal proof systems, supporting operations to encode and decode proof results in specific response languages. It works with abstract syntax trees and proof terms, enabling concrete interactions with theorem provers. Use cases include parsing prover outputs and generating standardized responses for proof checking tools.",
      "description_length": 362,
      "index": 47,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_class.Logic",
      "library": "dolmen.class",
      "description": "This module defines core operations for constructing and manipulating logical expressions in formal proof systems. It works with abstract syntax trees representing logical formulas, including quantifiers, connectives, and predicates. Concrete use cases include encoding proof obligations, performing logical transformations, and supporting theorem proving tasks.",
      "description_length": 362,
      "index": 48,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class",
      "library": "dolmen.class",
      "description": "This module provides operations for constructing logical expressions and handling proof system responses. It works with abstract syntax trees for logical formulas and proof terms. Concrete use cases include encoding proof obligations, performing logical transformations, parsing prover outputs, and generating standardized proof responses.",
      "description_length": 339,
      "index": 49,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.String.Reg_Lang",
      "library": "dolmen.std",
      "description": "This module defines constant symbols for constructing and manipulating regular languages as terms. It supports operations like concatenation, union, intersection, complement, Kleene star, and more, along with utilities like range constraints and power loops. These operations are used to represent and reason about string constraints in formal verification tasks, particularly in SMT solving contexts.",
      "description_length": 401,
      "index": 50,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Rat",
      "library": "dolmen.std",
      "description": "This module introduces arithmetic operations (addition, multiplication, division) and comparison operators for rational numbers within symbolic term expressions, alongside predicates to identify rational constants among term symbols. It operates on constant symbols of type `Dolmen_std.Expr.Term.Const.t`, enabling precise type inspection and manipulation in formal verification contexts. These capabilities are particularly useful for theorem proving tasks requiring exact numeric reasoning with rational values embedded in logical terms.",
      "description_length": 539,
      "index": 51,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Array",
      "library": "dolmen.std",
      "description": "This module defines constant symbols for array operations, specifically array selection and storage. It provides the `const`, `select`, and `store` values, which represent array constants, read operations, and write operations, respectively. These are used to model array expressions within first-order logic terms.",
      "description_length": 315,
      "index": 52,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.String",
      "library": "dolmen.std",
      "description": "This module provides low-level string manipulation operations and regular language membership checks for formal reasoning in theorem proving contexts. It works with string constants represented as `Dolmen_std.Expr.Term.Const.t` values and includes utilities for constructing/manipulating regular languages via the `Reg_Lang` submodule. Specific use cases involve verifying string constraints, analyzing regular language inclusions, and handling term-level string operations like concatenation, substring extraction, and lexicographic comparisons during SMT solving.",
      "description_length": 565,
      "index": 53,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Int",
      "library": "dolmen.std",
      "description": "This module provides operations for constructing and manipulating integer constant symbols, including arithmetic operations (addition, subtraction, multiplication, exponentiation, division, remainder), comparison operators, rounding functions, and numerical property tests like divisibility checks or integer/rational verification. It operates on values of type Dolmen_std.Expr.Term.Const.t, enabling the creation of term expressions used in logical reasoning, symbolic computation, and verification of numerical constraints.",
      "description_length": 525,
      "index": 54,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Float",
      "library": "dolmen.std",
      "description": "This module provides operations for floating-point arithmetic (addition, multiplication, square root), comparisons (less-than, equality), classification tests (zero, NaN checks), and conversions between floating-point, real numbers, and bitvector representations. It operates on integer tuples encoding bit-width and value parameters, alongside signed/unsigned bitvector tuples, to model IEEE floating-point formats and rounding modes. These features are used in formal verification to ensure correctness of numerical algorithms, handle edge cases like infinities or overflow, and enable symbolic reasoning across mixed-type computations.",
      "description_length": 638,
      "index": 55,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Bitv",
      "library": "dolmen.std",
      "description": "This module offers bitvector creation, arithmetic, bitwise operations, and transformations like concatenation, extraction, and shifting, alongside signed/unsigned comparisons and conversions to/from integers. It operates on term-level constant symbols parameterized by integer bit widths or values, aligning with SMT-LIB semantics for formal verification tasks such as modeling hardware or low-level system behaviors.",
      "description_length": 417,
      "index": 56,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.String.RegLan",
      "library": "dolmen.std",
      "description": "This module implements operations for constructing and manipulating regular languages over strings, including primitives for creating singleton languages, character ranges, concatenation, union, intersection, Kleene star and cross, complement, and bounded repetition. It works directly with `Dolmen_std.Expr.Term.t` values representing string terms in the SMT-LIB format. These functions support formal verification tasks involving string constraints, such as modeling regex patterns or validating string transformations in program analysis.",
      "description_length": 541,
      "index": 57,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Real",
      "library": "dolmen.std",
      "description": "This module provides arithmetic operations (addition, multiplication, exponentiation), comparisons, and rounding functions (floor, ceiling) for real number constants, alongside truncation and type-checking utilities to identify integers or rationals. It operates on real values embedded in the term constant symbol system, specifically designed for numerical reasoning in formal verification or theorem proving workflows.",
      "description_length": 421,
      "index": 58,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Bitv",
      "library": "dolmen.std",
      "description": "This module supports bitwise manipulation (concatenation, rotation, logical operations), arithmetic (addition, multiplication, signed/unsigned division), and comparisons (equality, ordering) on bitvectors represented as typed SMT terms. It operates on `Dolmen_std.Expr.Term.t` values to model bitvector expressions and their interactions with integers in SMT logic. Specific use cases include formal verification of arithmetic circuits, low-level system code analysis, and proving properties of bit-level algorithms via SMT solving.",
      "description_length": 532,
      "index": 59,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Subst.Var",
      "library": "dolmen.std",
      "description": "This module provides variable keys for substitutions, supporting operations to retrieve, check existence, bind, and remove variable-value associations in substitution maps. It works with key-value pairs where keys are variable identifiers and values can be arbitrary terms or expressions. Concrete use cases include managing variable replacements during term rewriting or implementing scoping mechanisms in formal logic systems.",
      "description_length": 428,
      "index": 60,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Float",
      "library": "dolmen.std",
      "description": "This module provides operations for constructing and manipulating SMT floating-point terms, including arithmetic (addition, multiplication, fused multiply-add), comparisons (less-than, equality), rounding, classification (zero, NaN checks), and conversions to bitvectors and real numbers. It operates on `Dolmen_std.Expr.Term.t` values representing floating-point terms, adhering to the IEEE 754-2008 standard for precision and special value handling (e.g., infinities, subnormals). These capabilities are used for SMT-LIB type coercion, formal verification of floating-point computations, and modeling low-level numeric behavior in theorem proving.",
      "description_length": 649,
      "index": 61,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.String",
      "library": "dolmen.std",
      "description": "This module provides string manipulation operations for SMT-LIB compliance, including creation, concatenation, substring extraction, and regex-based replacement, alongside lexicographic comparisons and numeric conversions. It operates on `Dolmen_std.Expr.Term.t` values, with submodules extending support to regular language operations like regex membership testing, Kleene star, and bounded repetition. These capabilities enable formal verification tasks involving string constraints, such as analyzing programmatic string behaviors or solving symbolic string equations in automated reasoning tools.",
      "description_length": 600,
      "index": 62,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Real",
      "library": "dolmen.std",
      "description": "This module provides arithmetic operations (addition, multiplication, division variants like Euclidean and floor division), comparisons (inequalities), and type conversions (to integers, rationals) for real numbers represented as `Dolmen_std.Expr.Term.t` values. It supports SMT-LIB semantics for real arithmetic, enabling precise modeling of mathematical constraints in formal verification tasks such as proving properties of systems involving real-valued expressions or solving numerical constraints. Key use cases include symbolic computation and SMT solver integration for domains requiring exact real number manipulation.",
      "description_length": 626,
      "index": 63,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Ty.Var",
      "library": "dolmen.std",
      "description": "This module handles type variables, including creation, comparison, and tagging operations. It supports concrete operations like printing, hashing, and checking wildcards, as well as managing tagged data on type variables. Use cases include representing and manipulating type variables in a type system, such as tracking type constraints or annotations during type inference.",
      "description_length": 375,
      "index": 64,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Field",
      "library": "dolmen.std",
      "description": "This module directly handles record fields as term constants, providing operations to manipulate tags and their associated values. It supports setting, adding, retrieving, and removing both singular and list-based tag values, with functions for equality, comparison, and hashing. Concrete use cases include managing metadata or annotations on record fields during term processing, such as tracking source locations or type information.",
      "description_length": 435,
      "index": 65,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const",
      "library": "dolmen.std",
      "description": "This module supports the construction and manipulation of constant symbols in logical expressions, encompassing logical connectives, quantifiers, and n-ary operations, alongside numeric and data-type constants like integers, bitvectors, arrays, and strings. It operates on `t` values, organized into submodules for domain-specific operations, enabling symbolic reasoning over first-order terms. These capabilities are utilized in formal verification tasks such as SMT solving, theorem proving, and semantic analysis of programs involving arithmetic, memory models, or string constraints.",
      "description_length": 587,
      "index": 66,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Int",
      "library": "dolmen.std",
      "description": "This module provides arithmetic operations (addition, multiplication, exponentiation, division with Euclidean or truncation/floor semantics), comparisons (less than, greater or equal), and conversions to other numeric types. It operates on integer terms represented as `Dolmen_std.Expr.Term.t`, using string-based constants to prevent overflow and support SMT-LIB-compliant integer handling. Designed for SMT solvers requiring precise division/remainder operations and type-safe manipulation of integers in formal verification contexts.",
      "description_length": 536,
      "index": 67,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Rat",
      "library": "dolmen.std",
      "description": "This module offers arithmetic operations (addition, subtraction, multiplication, exact division, remainder) and comparisons (less than, greater than) alongside rounding functions (floor, ceiling, truncate, round) and type tests for integers and rationals. It operates on TPTP-compatible arithmetic terms, enabling formal verification tasks that require precise rational number handling, exact division, and conversions between numeric types like integers and reals.",
      "description_length": 465,
      "index": 68,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Cstr",
      "library": "dolmen.std",
      "description": "This module handles algebraic datatype constructors, providing operations for type-checking, tagging, and manipulating constructor values. It supports concrete tasks like retrieving constructor types, managing pattern matching arities, and setting or querying tagged metadata associated with constructors. Use cases include implementing pattern matching logic, tracking constructor-specific annotations, and ensuring type consistency during term construction.",
      "description_length": 459,
      "index": 69,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Array",
      "library": "dolmen.std",
      "description": "Implements array operations including constant array creation, element selection, and element storage. Works with first-order polymorphic terms representing arrays and their indices and values. Used for modeling and manipulating array-based data structures in formal verification tasks.",
      "description_length": 286,
      "index": 70,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Var",
      "library": "dolmen.std",
      "description": "This module manages typed variables in first-order terms, providing operations to create variables with associated types, retrieve their types, and manipulate metadata via tags. It supports tag-based annotations with functions to set, get, and unset single values or lists of values, including safe accessors for list tags. Use cases include representing and annotating logical variables in term structures during type checking or theorem proving tasks.",
      "description_length": 453,
      "index": 71,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Stats.Floats",
      "library": "dolmen.std",
      "description": "This module manages time statistics by collecting and summarizing float values associated with named events. It supports creating a statistic, adding time measurements, and printing the collected data in a formatted way. Use it to track and display performance metrics like function execution times or system latencies.",
      "description_length": 319,
      "index": 72,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Subst",
      "library": "dolmen.std",
      "description": "This module implements substitution maps for variable-term associations, supporting operations like binding lookup, insertion, filtering, and merging. It works with key-value pairs where keys are variable identifiers and values are arbitrary terms or expressions. Concrete use cases include managing variable replacements during term rewriting and implementing scoping mechanisms in formal logic systems.",
      "description_length": 404,
      "index": 73,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Maps.Make",
      "library": "dolmen.std",
      "description": "Implements a map structure parameterized by a key-ordering module, supporting efficient key-based operations. It provides functions for adding, finding, and updating bindings, along with iteration and folding over key-value pairs. Useful for managing symbol tables, configuration settings, or any keyed data where ordered access is needed.",
      "description_length": 339,
      "index": 74,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Maps.Int",
      "library": "dolmen.std",
      "description": "This module implements a map data structure specialized for integer keys, supporting operations like insertion, lookup (with and without exceptions), and iteration. It provides functions to add key-value pairs, retrieve values by key, and traverse or fold over the map's contents. Concrete use cases include managing symbol tables indexed by integer identifiers or tracking dynamic state in algorithms requiring fast integer key lookups.",
      "description_length": 437,
      "index": 75,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Normalize.Smtlib",
      "library": "dolmen.std",
      "description": "This module provides a mapper to normalize terms parsed from SMT-LIB files by converting specific syntactic constructs into their corresponding Dolmen term builtins. It handles SMT-LIB's representation of equality and other builtins, transforming them into the standard term representations used internally. This enables consistent semantic processing of SMT-LIB input across the toolchain.",
      "description_length": 390,
      "index": 76,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Extensions.Smtlib2",
      "library": "dolmen.std",
      "description": "This module manages SMT-LIB 2 extensions, allowing creation, activation, and lookup of custom statement handlers. It works with a custom type `t` representing extensions, each tied to a name and a list of statements. Concrete use cases include enabling MaxSMT support and defining custom SMT commands that are conditionally allowed during parsing.",
      "description_length": 347,
      "index": 77,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Normalize.Tptp",
      "library": "dolmen.std",
      "description": "This module provides a mapper to normalize terms parsed from TPTP problem files by converting syntactic constructions into their corresponding built-in term symbols. It operates on terms represented using `Dolmen_std.Term.t`, transforming language-specific representations\u2014such as TPTP's equality syntax\u2014into a standardized internal form. A concrete use case is ensuring that equality expressions in TPTP input are consistently represented using the built-in equality symbol rather than as regular function applications.",
      "description_length": 520,
      "index": 78,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Expr.Id",
      "library": "dolmen.std",
      "description": "This module manages identifiers with associated metadata, providing operations to create, compare, hash, and print them. It supports attaching tags, including single values and lists, with functions to query, update, and remove these tags. Use cases include representing symbolic expressions with provenance information, such as variables or constants in a formal verification system.",
      "description_length": 384,
      "index": 79,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Print",
      "library": "dolmen.std",
      "description": "This module provides printers for expression components like identifiers, types, terms, and formulas, with support for custom naming, positioning, and tag/index display. It handles data types such as `id`, `type_`, `term`, `formula`, and related variants, allowing precise control over pretty-printing behavior. Concrete use cases include generating human-readable output for type definitions, expressions, and logical formulas during debugging or logging.",
      "description_length": 456,
      "index": 80,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Transformer.Make",
      "library": "dolmen.std",
      "description": "This module implements a parser for a typed language, handling tokenization, error reporting, and file resolution. It processes input sources like files or strings into structured statements, supporting incremental parsing and environment-based lookups. Concrete use cases include parsing domain-specific languages with location tracking and custom error messages.",
      "description_length": 364,
      "index": 81,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Id.Map",
      "library": "dolmen.std",
      "description": "Implements standard map operations for identifier keys, including insertion, lookup (with and without exceptions), and iteration. Works with associative maps from identifiers to arbitrary values. Useful for managing symbol tables, environment bindings, or configuration settings indexed by identifiers.",
      "description_length": 302,
      "index": 82,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Namespace.Map",
      "library": "dolmen.std",
      "description": "This module implements maps keyed by namespaces, supporting standard operations like insertion, lookup, and iteration. It provides functions for adding bindings, retrieving values with or without exceptions, and folding or iterating over key-value pairs. Concrete use cases include managing symbol tables or configuration settings scoped to named namespaces.",
      "description_length": 358,
      "index": 83,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Name.Map",
      "library": "dolmen.std",
      "description": "Implements a map structure specialized for `Dolmen_std.Name.t` keys, providing operations to add, find, and update bindings with optional or exception-based lookups. Supports iteration and folding over mapped values, enabling efficient management of named symbol tables or configuration settings. Useful for tracking symbol definitions or managing scoped variables in parsers and interpreters.",
      "description_length": 393,
      "index": 84,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Ty",
      "library": "dolmen.std",
      "description": "This module offers type manipulation capabilities for formal logic systems, focusing on operations like unification, substitution, and polymorphism handling. It works with structured type representations including primitive types (integers, reals, strings), composite types (arrow, array), and type variables with support for wildcard instantiation and tag-based metadata. Key applications include type inference, polymorphic type expansion, and managing type-level computations in expression frameworks.",
      "description_length": 504,
      "index": 85,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Term.S",
      "library": "dolmen.std",
      "description": "This module provides ordered set operations for managing collections of identifiers, supporting element insertion, deletion, and selection alongside set-theoretic operations like union and intersection. It works with ordered sets of `Dolmen_std.Id.t`, preserving physical equality and enabling transformations via mapping, filtering, and partitioning over ordered elements. Use cases include symbolic manipulation requiring ordered traversal, subset construction via sequences, and equivalence checks with precise ordering control.",
      "description_length": 531,
      "index": 86,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Escape.Make",
      "library": "dolmen.std",
      "description": "This module creates escapers for identifiers in specific languages, ensuring valid string representations by handling escaping and renaming. It works with identifiers from the `Id` module, applying custom escape and rename functions to avoid syntax issues or name clashes during output. Use it when generating code or pretty-printing terms that require safe identifier handling across different language grammars.",
      "description_length": 413,
      "index": 87,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Maps.String",
      "library": "dolmen.std",
      "description": "This module implements a map data structure specialized for string keys, supporting operations like insertion, lookup, iteration, and folding. It provides functions to add key-value pairs, retrieve values with or without exceptions, and traverse or transform the map's contents. Concrete use cases include managing symbol tables, configuration settings, or any keyed data where efficient string-based lookups and updates are required.",
      "description_length": 434,
      "index": 88,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Tags",
      "library": "dolmen.std",
      "description": "This module defines tags and associated operations for annotating terms and formulas with metadata used during printing, rewriting, and logical processing. It works with terms and formulas in the `Expr` module, supporting annotations like variable bindings, rewrite rules, associativity/commutativity flags, and naming directives. Concrete use cases include controlling identifier printing format, marking terms as rewrite rules or AC expressions, and attaching triggers or filters to quantified formulas.",
      "description_length": 505,
      "index": 89,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Stats.Float",
      "library": "dolmen.std",
      "description": "This module implements basic statistics tracking for floating-point values, offering operations to create, update, and print named float statistics. It works directly with float values and maintains an internal state that can be queried or reset. Concrete use cases include tracking performance metrics like execution time or memory usage across different components of a system.",
      "description_length": 379,
      "index": 90,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Stats",
      "library": "dolmen.std",
      "description": "Tracks and manages statistical data for floating-point values, providing operations to record, summarize, and output named metrics. Works with float values to measure and report performance data such as execution times or resource usage. Use to monitor and analyze numerical metrics across different parts of a system.",
      "description_length": 318,
      "index": 91,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Vec",
      "library": "dolmen.std",
      "description": "This module implements resizeable arrays with operations to create, copy, and manipulate vectors, including adding or removing elements at specific positions or the end of the vector. It works with elements of any type `'a` and maintains an internal array with a specified capacity. Concrete use cases include dynamically growing arrays for accumulating results during parsing or managing a mutable list of terms in a solver.",
      "description_length": 425,
      "index": 92,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Transformer",
      "library": "dolmen.std",
      "description": "Handles parsing and transformation of input streams into structured data, supporting operations like tokenization, filtering, and mapping over sequences. Works with input channels, strings, and custom token types to enable precise data extraction and conversion. Useful for implementing parsers for configuration files, log processing, or data import pipelines.",
      "description_length": 361,
      "index": 93,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Term",
      "library": "dolmen.std",
      "description": "This module manipulates terms through logical connectives, arithmetic operations, and type-level constructs like quantifiers and binders, alongside language-specific features such as conditionals, pattern matching, and annotations for SMT solving. It operates on terms represented via a `descr` type (encompassing symbols, builtins, and structured expressions) and supports variables, types, S-expressions, and attributes, enabling formal verification, automated reasoning, and error reporting with precise location tracking.",
      "description_length": 525,
      "index": 94,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Loc",
      "library": "dolmen.std",
      "description": "This module provides operations for creating, manipulating, and comparing source code location data derived from lexing buffers and file metadata, including functions to format location details, convert between compact and full representations, and extract filenames. It operates on `loc` and `t` types that encapsulate positions, line/column offsets, and file information,",
      "description_length": 373,
      "index": 95,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Answer",
      "library": "dolmen.std",
      "description": "This module creates and manipulates answer values representing solver results, including unsatisfiability, satisfiability with optional models, and errors. It works with terms, locations, function definitions, and descriptions to construct answers. Concrete use cases include generating unsat responses, defining functions and recursive functions for models, and creating error messages with optional source locations.",
      "description_length": 418,
      "index": 96,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Statement",
      "library": "dolmen.std",
      "description": "This module provides a comprehensive framework for constructing and manipulating logical statements aligned with SMT-LIB and TPTP standards, including declarations (types, functions), definitions (recursive or otherwise), assertions, proof commands, and file-level operations like imports. It operates on terms, identifiers, symbols, and locations to model formal type systems and logical theories, supporting use cases such as theorem proving, SMT solver interaction, and logic configuration management. Additional utilities enable structured pretty-printing of statements for readability and interoperability.",
      "description_length": 611,
      "index": 97,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Misc",
      "library": "dolmen.std",
      "description": "This module offers functions for lexicographic comparisons, hash composition, string and list transformations, option chaining, and structured pretty-printing, operating on data types like lists, options, and string-based inputs. It also includes abstractions for handling input sources (files, streams, buffers) by converting them into lexing buffers or filenames while managing resource cleanup and error reporting. These capabilities support tasks like parsing, data serialization, and input preprocessing in compilers or interpreters.",
      "description_length": 538,
      "index": 98,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Escape",
      "library": "dolmen.std",
      "description": "This module escapes and renames identifiers to ensure valid syntax in target languages. It provides `smap` for character-wise string transformation and `rename` for appending unique indices after a separator. Use it to safely generate identifiers in code output or term pretty-printing, avoiding syntax errors or name collisions.",
      "description_length": 329,
      "index": 99,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Maps",
      "library": "dolmen.std",
      "description": "This module provides map implementations for integer and string keys, as well as a customizable map generator for ordered keys. It supports key-value insertion, safe and unsafe lookups, iteration, and folding operations. Use cases include managing symbol tables, tracking algorithm state with integer keys, and handling configuration data with string keys.",
      "description_length": 356,
      "index": 100,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Tok",
      "library": "dolmen.std",
      "description": "This module defines a structured representation of lexical tokens, including their kind, lexeme, article, and optional hint. It provides functions to construct and print token descriptions with customizable formatting. Useful for building parsers or interpreters where detailed token metadata is required during lexical analysis or error reporting.",
      "description_length": 348,
      "index": 101,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr",
      "library": "dolmen.std",
      "description": "This module provides operations for constructing, manipulating, and analyzing logical terms and expressions, including support for variables, constants, function application, and binding constructs. It works with structured representations of terms, formulas, and their components, enabling precise handling of logical connectives, quantifiers, and user-defined operators. Concrete use cases include building and transforming logical formulas during proof search, implementing term rewriting systems, and supporting formal verification tasks involving symbolic expressions.",
      "description_length": 573,
      "index": 102,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Builtin",
      "library": "dolmen.std",
      "description": "This module defines built-in constants and operations used in typed expressions, primarily for formal logic and theorem proving applications. It includes builtins for arithmetic, boolean logic, arrays, bitvectors, floating-point numbers, strings, and regular expressions, each with specific type signatures and semantics. These builtins are used to represent and manipulate expressions with well-defined behaviors, such as equality checks, quantifiers, array operations, bitvector arithmetic, and string manipulations, enabling precise reasoning in formal verification tasks.",
      "description_length": 575,
      "index": 103,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Path",
      "library": "dolmen.std",
      "description": "This module represents and manipulates paths that identify variables and constants after typechecking. It supports operations to construct local and absolute paths, rename their base identifiers, and format them for output. It is used to track the origin of named entities in a module hierarchy, such as resolving constant references across modules.",
      "description_length": 349,
      "index": 104,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Name",
      "library": "dolmen.std",
      "description": "This module defines a type `t` for representing structured names, including simple, indexed, and qualified forms, with operations to create and manipulate these names. It provides hashing, equality, comparison, and printing functions for use in symbol tables and parsers. The `Map` submodule implements maps keyed by these structured names, enabling efficient lookups and management of named entities in language processing tasks.",
      "description_length": 430,
      "index": 105,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Maps_string",
      "library": "dolmen.std",
      "description": "This module implements a map data structure specialized for string keys, supporting operations like insertion, lookup, iteration, and folding. It provides functions to add key-value pairs, retrieve values via `find_exn` or `find_opt`, and traverse or accumulate over the map's contents. Concrete use cases include managing symbol tables, configuration settings, or any mapping from string identifiers to associated data.",
      "description_length": 420,
      "index": 106,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Msg",
      "library": "dolmen.std",
      "description": "This module handles message formatting and reporting, providing functions to generate and manipulate structured messages. It works with message types that include severity levels, locations, and formatted text. Concrete use cases include error reporting during parsing or type checking, and generating diagnostic output in tools like linters or compilers.",
      "description_length": 355,
      "index": 107,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Namespace",
      "library": "dolmen.std",
      "description": "This module defines a type `t` representing distinct namespaces for identifiers, such as variables, terms, attributes, and declarations, along with a `value` type for literal constants like integers, rationals, and bitvectors. It provides operations for hashing, equality checks, comparison, and formatted printing of namespaces, as well as predefined namespace constants for common categories. Concrete use cases include organizing identifiers in a symbolic reasoning system, distinguishing between syntactic categories in formal languages, and tagging literals with their respective value types for precise parsing and evaluation.",
      "description_length": 632,
      "index": 108,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Timer",
      "library": "dolmen.std",
      "description": "This module provides functions to measure elapsed time between a start and stop point. It works with a timer type `t` that represents a timing session. A concrete use case is benchmarking the execution time of a function or code block.",
      "description_length": 235,
      "index": 109,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Tag",
      "library": "dolmen.std",
      "description": "This module manages typed key-value associations using tags, where each tag uniquely identifies a value or a list of values within a map. It supports operations to create tags, set and retrieve values, add to or remove from tagged entries, and iterate over all bindings. Concrete use cases include tracking metadata across different components of a system, such as annotating terms with source locations or accumulating diagnostic messages during analysis.",
      "description_length": 456,
      "index": 110,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Extensions",
      "library": "dolmen.std",
      "description": "Manages SMT-LIB 2 extensions with operations to create, activate, and look up custom statement handlers. It works with a custom type `t` representing extensions, associated with names and lists of statements. Enables features like MaxSMT support and defining conditional SMT commands during parsing.",
      "description_length": 299,
      "index": 111,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Pretty",
      "library": "dolmen.std",
      "description": "This module defines types for specifying pretty-printing annotations, including name representations, operator positions, and associativity. It works with strings and variants to describe formatting rules for symbols and operators. Concrete use cases include configuring how logical or mathematical expressions are rendered in output, such as distinguishing prefix vs. infix notation or controlling associativity in printed terms.",
      "description_length": 430,
      "index": 112,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std",
      "library": "dolmen.std",
      "description": "This module provides operations for formal logic processing, such as answer generation, term manipulation, and SMT-LIB extensions, working with terms, types, locations, identifiers, and maps to support theorem proving, model generation, and error reporting. It also includes utilities for time measurement, lexical token representation, input stream transformation, and dynamic array manipulation, operating on timing sessions, structured tokens, data streams, and resizeable arrays for tasks like benchmarking, parsing, and result accumulation.",
      "description_length": 545,
      "index": 113,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Print.L.M",
      "library": "dolmen_smtlib2_poly",
      "description": "This module implements an ordered polymorphic map structure with operations for key-based querying, transformation, and sequence conversion, supporting use cases like symbol table management in SMT-LIB parsing. It works with maps (`t`) where keys are ordered strings and values are arbitrary types, offering functions for filtering, folding, and bidirectional conversion to sequences of key-value pairs. Specific applications include handling SMT-LIB declarations and maintaining ordered environments for logical expressions.",
      "description_length": 525,
      "index": 114,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Make.Lexer",
      "library": "dolmen_smtlib2_poly",
      "description": "This module defines the lexical analysis component for parsing SMT-LIB input. It provides functions to convert character streams into tokens and associate descriptive error information with each token. It operates on `lexbuf` structures from OCaml's standard `Lexing` module and produces `token` values specific to the SMT-LIB grammar. A concrete use case is processing SMT-LIB script files into a stream of tokens for subsequent parsing and semantic analysis.",
      "description_length": 460,
      "index": 115,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Make.Parser",
      "library": "dolmen_smtlib2_poly",
      "description": "Parses SMT-LIB 2 input from a lexical buffer into a list of statements or individual statements. It operates on `lexbuf` inputs using a tokenization function and produces `statement` values, handling both complete files and incremental parsing. Useful for reading SMT-LIB scripts from files or interactive sources like REPLs.",
      "description_length": 325,
      "index": 116,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Print.L",
      "library": "dolmen_smtlib2_poly",
      "description": "This module handles lexing and printing of SMT-LIB tokens, including symbols, strings, and reserved words. It provides functions for converting lexical elements into token values, managing symbol tables, and processing quoted or keyword-based identifiers. Concrete uses include parsing SMT-LIB input streams, handling symbol resolution during printing, and maintaining reserved word mappings for accurate syntax representation.",
      "description_length": 427,
      "index": 117,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Print",
      "library": "dolmen_smtlib2_poly",
      "description": "This module provides functions for printing and formatting SMT-LIB tokens, including symbol categorization, character classification, and identifier output. It operates on strings, characters, and symbol types to support precise lexical representation. Concrete uses include converting internal names to SMT-LIB identifiers, determining symbol quoting requirements, and validating token characters during output generation.",
      "description_length": 423,
      "index": 118,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Make",
      "library": "dolmen_smtlib2_poly",
      "description": "This module generates a parser for the SMT-LIB 2 input format, providing functions to tokenize and parse SMT-LIB scripts into statements. It works with lexical buffers and custom token and statement types, supporting both full and incremental parsing from files, standard input, or string contents. Concrete use cases include processing SMT-LIB scripts for formal verification tools and interactive theorem provers, where syntax errors need to be handled gracefully during incremental parsing.",
      "description_length": 493,
      "index": 119,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly",
      "library": "dolmen_smtlib2_poly",
      "description": "This module handles the parsing and printing of SMT-LIB tokens, focusing on symbol formatting, character validation, and identifier output. It operates on strings, characters, and symbol types to ensure correct lexical representation. Concrete uses include converting internal names to SMT-LIB identifiers, determining when symbols need quoting, and validating characters in tokens during output generation.",
      "description_length": 407,
      "index": 120,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_smtlib2_v6_response.Make.Lexer",
      "library": "dolmen_smtlib2_v6_response",
      "description": "This module defines the lexical analysis component for parsing Smtlib input, handling the conversion of character streams into tokens. It includes functions for token generation from a lexing buffer and associating descriptive error information with tokens. It is used during the initial stages of parsing Smtlib scripts to break input into meaningful syntactic units for further processing.",
      "description_length": 391,
      "index": 121,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_response.Make.Parser",
      "library": "dolmen_smtlib2_v6_response",
      "description": "Parses SMT-LIB v2.6 input into abstract syntax trees using lexer functions. It processes lexbuf streams to produce lists of statements or individual statements incrementally. Designed for interpreting SMT solver responses from files or interactive sessions.",
      "description_length": 257,
      "index": 122,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_response.Make",
      "library": "dolmen_smtlib2_v6_response",
      "description": "This module generates a parser for SMT-LIB v2.6 input, producing abstract syntax trees from character streams. It works with token streams and statement structures, supporting both full and incremental parsing of SMT solver responses from files, standard input, or string contents. Concrete use cases include interpreting SMT-LIB scripts in interactive sessions and processing large SMT solver outputs line by line without loading the entire input.",
      "description_length": 448,
      "index": 123,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_smtlib2_v6_response",
      "library": "dolmen_smtlib2_v6_response",
      "description": "This module processes SMT-LIB 2.6 response data, handling parsing and representation of solver outputs including success, unsat, and model responses. It works with identifier, term, and statement types to capture the structure of SMT queries and results. Concrete use cases include interpreting responses from SMT solvers during formal verification tasks and extracting model values for further analysis.",
      "description_length": 404,
      "index": 124,
      "embedding_norm": 0.9999999403953552
    }
  ],
  "filtering": {
    "total_modules_in_package": 131,
    "meaningful_modules": 125,
    "filtered_empty_modules": 6,
    "retention_rate": 0.9541984732824428
  },
  "statistics": {
    "max_description_length": 649,
    "min_description_length": 180,
    "avg_description_length": 414.856,
    "embedding_file_size_mb": 1.8120355606079102
  }
}