{
  "package": "dolmen",
  "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
  "embedding_dimension": 1024,
  "total_modules": 258,
  "creation_timestamp": "2025-07-16T00:03:52.464113",
  "modules": [
    {
      "module_path": "Dolmen_intf.Ty.Thf-Const",
      "library": "dolmen.intf",
      "description": "This module represents and manipulates type constants in a logical context, providing operations to create, compare, and print them. It supports data types with associated arity and metadata tags, enabling concrete use cases such as modeling type-level symbols in theorem proving or type inference systems. Key functions include symbol creation with arity, tag manipulation, and formatted output for debugging or serialization.",
      "description_length": 427,
      "index": 0,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty.Zf_Arith",
      "library": "dolmen.intf",
      "description": "This module defines the type `t` and a value `int` representing the type of integers, specifically for use in typing TPTP (Thousands of Problems for Theorem Provers) expressions. It provides the foundational type interface required for arithmetic operations in theorem proving contexts. Concrete use cases include representing integer types in logical formulas and ensuring correct typing during theorem verification.",
      "description_length": 417,
      "index": 1,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Ty.Smtlib_Bitv",
      "library": "dolmen.intf",
      "description": "This module defines the interface for working with SMT-LIB bitvector types. It provides operations to create fixed-size bitvectors and exposes a partial view of bitvector types for inspection. Concrete use cases include defining bitvector variables and constraints in SMT solvers, such as specifying register widths or memory addresses.",
      "description_length": 336,
      "index": 2,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Logic",
      "library": "dolmen.intf",
      "description": "This module provides constructors for logical connectives (equality, implication, quantifiers), arithmetic operations (integer and bitvector manipulations), and higher-order terms (lambda binders, conditionals, pattern matching) used in parsing formal languages like TPTP, SMT-LIB, and Zipperposition. It operates on a polymorphic term type `t` alongside identifiers (`id`) and optional source locations, supporting both first-order and higher-order logic representations with language-specific annotations and syntactic extensions. Key use cases include building typed logical expressions, encoding proof obligations, and handling format-specific constructs such as SMT-LIB's triggers or TPTP's THF higher-order terms.",
      "description_length": 719,
      "index": 3,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Ae_Array",
      "library": "dolmen.intf",
      "description": "Implements operations for functional arrays, supporting `select` to retrieve values at specific indices and `store` to create new arrays with updated values at given indices. Works with the abstract type `t` representing terms in a logical context. Useful for modeling array manipulations in formal verification tasks, such as symbolic execution or constraint solving.",
      "description_length": 368,
      "index": 4,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Array",
      "library": "dolmen.intf",
      "description": "Implements operations for functional arrays in SMT-LIB style, providing constant array creation, element selection, and array updates. Works with index and element types represented as term types. Enables modeling array-based logic in formal verification tasks, such as array property checking and symbolic execution.",
      "description_length": 317,
      "index": 5,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Tptp_Tff_Arith-Int",
      "library": "dolmen.intf",
      "description": "This module provides arithmetic operations and type coercion functions for symbolic terms modeled as an abstract type `t`, enabling manipulation of integer and rational number expressions. It supports addition, subtraction, multiplication, division variants (Euclidean, truncating, flooring), inequality comparisons, rounding, and type checks, catering to formal verification systems and symbolic computation tasks where terms require evaluation or conversion to concrete numeric types like `int` or `rat`.",
      "description_length": 506,
      "index": 6,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Zf_Base",
      "library": "dolmen.intf",
      "description": "This module defines core logical operations and term constructors for working with formulas in a first-order logic setting. It provides functions for building and manipulating logical expressions such as negation, disjunction, conjunction, implication, equivalence, equality, disequality, and if-then-else terms. These operations are essential for encoding logical constraints and reasoning tasks in formal verification and automated theorem proving.",
      "description_length": 450,
      "index": 7,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Bitv",
      "library": "dolmen.intf",
      "description": "This module provides low-level bitvector manipulation and arithmetic operations, including bitwise logic, shifts, modular arithmetic, and comparisons, all adhering to SMT-LIB semantics. It operates on a fixed-size bitvector type `t`, supporting creation from binary strings, concatenation, extraction, and sign/zero extensions. The functionality is designed for applications in formal verification and symbolic execution where precise bit-level reasoning and compatibility with SMT solvers are required.",
      "description_length": 503,
      "index": 8,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Lex.S",
      "library": "dolmen.intf",
      "description": "Implements lexer operations for parsing input streams into tokens. It defines the token type and provides functions to generate tokens from a lex buffer, associating each with a descriptive error message. Used to build custom lexers for domain-specific languages or structured data formats.",
      "description_length": 290,
      "index": 9,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Ty.Tptp_Base",
      "library": "dolmen.intf",
      "description": "This module defines the minimal signature required for typing TPTP (Thousands of Problems for Theorem Provers) expressions. It includes two core values: `prop`, representing the type of propositions, and `base`, representing an arbitrary base type used in TPTP logic expressions. It is used to ensure type systems conform to the requirements for encoding and manipulating TPTP logical terms.",
      "description_length": 391,
      "index": 10,
      "embedding_norm": 1.0000001192092896
    },
    {
      "module_path": "Dolmen_intf.Ty.Ae_Arith",
      "library": "dolmen.intf",
      "description": "This module defines the interface for arithmetic types used in a solver's type system. It includes specific values for integer and real types, along with a view type for inspecting arithmetic types. It supports operations that distinguish between integer and real number representations in logical expressions.",
      "description_length": 310,
      "index": 11,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Real",
      "library": "dolmen.intf",
      "description": "This module provides operations for constructing and manipulating real arithmetic expressions in SMT-LIB format, including constants, arithmetic operations, and comparisons. It works with real numbers represented as strings to preserve precision and supports expressions like addition, subtraction, multiplication, division, and inequalities. Concrete use cases include building and evaluating SMT-LIB formulas involving real-valued variables and constraints.",
      "description_length": 459,
      "index": 12,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Thf-Var",
      "library": "dolmen.intf",
      "description": "This module handles typed variables in terms, providing creation, type retrieval, and tag management operations. It works with variables represented by the type `t`, along with associated type information and tags. Concrete use cases include managing variable bindings during term construction and attaching metadata like type annotations or solver-specific information.",
      "description_length": 370,
      "index": 13,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty.Ae_Bitv",
      "library": "dolmen.intf",
      "description": "This module defines the interface for bitvector types used in term typing, specifically providing a function to create fixed-size bitvectors. It works with bitvector data structures represented by the abstract type `t`. A concrete use case is defining bitvector types of specific widths when modeling low-level arithmetic operations in a type system.",
      "description_length": 350,
      "index": 14,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Tptp_Thf_Core-Const",
      "library": "dolmen.intf",
      "description": "This module defines constant symbols for logical connectives, quantifiers, and conditional expressions used in TPTP THF (Typed Higher-Order Form) syntax. It provides values representing boolean constants, logical operations (such as negation, conjunction, disjunction, implication), equality, and higher-order quantification. These constants are used to construct and manipulate logical formulas in automated theorem proving tasks.",
      "description_length": 431,
      "index": 15,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Ty.Smtlib_Real",
      "library": "dolmen.intf",
      "description": "This module defines the type `t` and the value `real` to represent and work with real number expressions in SMT-LIB arithmetic. It is used to model real-valued terms and operations such as addition, multiplication, and comparisons in SMT solvers. Concrete use cases include building and manipulating real-number expressions for formal verification and constraint solving tasks.",
      "description_length": 377,
      "index": 16,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Id.Escape",
      "library": "dolmen.intf",
      "description": "This module provides functions for escaping and unescaping identifier names, ensuring they conform to specific syntactic requirements. It operates on the type `t` defined in the parent identifier module, transforming their string representations. Concrete use cases include preparing identifiers for output in formats requiring escaped characters or parsing input with encoded names.",
      "description_length": 383,
      "index": 17,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Tff-Field",
      "library": "dolmen.intf",
      "description": "This module defines operations for comparing and manipulating constant symbols within a term interface. It primarily works with the abstract type `t` representing term fields, supporting concrete use cases such as symbol resolution and equality checks in formal logic processing. The `compare` function enables ordering of constant symbols, essential for maintaining consistent term representations in logical frameworks.",
      "description_length": 421,
      "index": 18,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Tptp_Tff_Core",
      "library": "dolmen.intf",
      "description": "This module defines core logical and boolean operations for TPTP TFF (Typed First-Order Form) terms, including conjunction, disjunction, negation, implication, equivalence, and equality checks. It works with the abstract type `t` representing logical terms, supporting construction of complex logical expressions. Concrete use cases include encoding first-order logic formulas, building proof obligations, and representing logical constraints in automated reasoning tools.",
      "description_length": 472,
      "index": 19,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Tff-Cstr",
      "library": "dolmen.intf",
      "description": "This module defines operations for working with algebraic datatype constructors, including retrieving their type, comparing symbols, and determining argument types during pattern matching. It operates on constructor symbols and their associated types, providing precise type information for term construction. It is used to ensure correct typing when applying constructors in expressions and pattern definitions.",
      "description_length": 412,
      "index": 20,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Real_Int",
      "library": "dolmen.intf",
      "description": "This module defines the interface for SMT-LIB real and integer arithmetic operations. It provides functions to retrieve the type of a term and organizes integer and real number operations into separate submodules. It is used to implement and manipulate typed terms in SMT solvers and formal verification tools.",
      "description_length": 310,
      "index": 21,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Arith_Common",
      "library": "dolmen.intf",
      "description": "This module defines arithmetic operations over first-order terms, including construction of constants from strings, unary negation, addition, subtraction, multiplication, and comparison operators (less than, less or equal, greater than, greater or equal). It works with two main types: `t` representing arithmetic terms and `cst` for constants. It is used to model and manipulate arithmetic expressions in SMT-LIB compliant solvers, particularly for building and evaluating logical formulas involving integer or real arithmetic.",
      "description_length": 528,
      "index": 22,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Map.S",
      "library": "dolmen.intf",
      "description": "This module implements a polymorphic map with operations for key-based value retrieval, addition, and transformation. It supports efficient lookups using `find_exn` and `find_opt`, and updates via `add` and `find_add`. Typical use cases include managing symbol tables, configuration settings, or any keyed data where fast access and modification are required.",
      "description_length": 359,
      "index": 23,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_String_String",
      "library": "dolmen.intf",
      "description": "This module provides comprehensive string manipulation operations and lexicographic analysis for an abstract string type `t` that supports Unicode and SMT-LIB semantics. It includes substring extraction, replacement, indexing, and regex pattern matching via `in_re`, alongside strict lexicographic ordering comparisons. These capabilities are particularly useful for formal verification tasks involving string constraints and SMT-LIB-compliant solver integrations.",
      "description_length": 464,
      "index": 24,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Language.S",
      "library": "dolmen.intf",
      "description": "This module defines the core components and operations for implementing specific languages within the Dolmen framework. It includes types for files, tokens, and statements, along with functions for locating files, parsing entire inputs, and incrementally processing statements. It is used to build language-specific parsers and lexers that can handle input from files, standard input, or string contents, particularly useful for processing domain-specific languages with support for include directives and error recovery.",
      "description_length": 521,
      "index": 25,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Parse.S",
      "library": "dolmen.intf",
      "description": "This module defines a parser interface for processing input using a lexer and returning structured statements. It includes functions to parse an entire file into a list of statements or to parse individual statements incrementally from a lex buffer. The module works with token and statement types, enabling use cases like reading and interpreting structured text files or handling interactive input streams.",
      "description_length": 408,
      "index": 26,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Thf-Const",
      "library": "dolmen.intf",
      "description": "This module defines operations for creating and manipulating constant symbols in terms, including functions to construct constants with a given type, retrieve their type, and tag them with additional metadata. It works with the abstract type `t` representing term constants, along with types like `ty` for their associated types and `path` for their identifiers. Concrete use cases include building and annotating logical constants during term construction in theorem proving or formal verification tasks.",
      "description_length": 505,
      "index": 27,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Stmt.Logic",
      "library": "dolmen.intf",
      "description": "This module provides operations for constructing logical statements, managing solver state, and interacting with theorem provers in formal logic frameworks. It works with logical terms, types, solver configurations, and prover-specific data structures to handle tasks like declaring symbols, defining inductive types, asserting and manipulating logical formulas, and retrieving proofs or models. Specific use cases include parsing and processing languages like TPTP and SMT-LIB, managing assertion stacks, and implementing logical directives for verification tasks.",
      "description_length": 565,
      "index": 28,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Thf",
      "library": "dolmen.intf",
      "description": "This module enables defining algebraic datatypes and records with operations to construct and type terms like variables, constants, and constructors, while managing type relationships through type variables and annotations. It supports logical term manipulation via quantifiers, lambda abstractions, let-bindings, and pattern matching, which are essential for formal verification tasks requiring precise term analysis and transformation. The integration of field accessors and constructor testing makes it suitable for modeling structured data in theorem proving or symbolic computation systems.",
      "description_length": 595,
      "index": 29,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Ty.Smtlib_Base",
      "library": "dolmen.intf",
      "description": "This module defines the core type interface for SMT-LIB propositions, centered around the `t` type and the `prop` value representing proposition types. It ensures consistent typing for logical expressions in SMT solvers. Concrete use cases include defining and manipulating propositional logic terms within SMT-LIB-based tools.",
      "description_length": 327,
      "index": 30,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Float-Float",
      "library": "dolmen.intf",
      "description": "This module provides operations for constructing and manipulating floating-point numbers according to IEEE 754 and SMT-LIB semantics, including arithmetic (addition, multiplication, square root), rounding, comparisons, and classification (e.g., detecting NaNs or infinities). It works with a term type `t` representing SMT expressions, enabling conversions between floating-point formats, bitvectors, and real numbers. These operations are used in formal verification tasks requiring precise modeling of floating-point behavior, such as verifying numerical algorithms or hardware designs.",
      "description_length": 588,
      "index": 31,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Ae_Base",
      "library": "dolmen.intf",
      "description": "This module defines core logical operations and term constructions for working with formulas in a first-order logic setting. It provides functions for building equalities, boolean connectives (negation, disjunction, conjunction, implication, equivalence, XOR), conditionals, and constraints such as distinctness and triggers. These operations manipulate terms and variables represented by the abstract types `t` and `term_var`, enabling precise formula encoding and manipulation for automated reasoning tasks.",
      "description_length": 509,
      "index": 32,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Tptp_Tff_Arith-Rat",
      "library": "dolmen.intf",
      "description": "This module provides arithmetic operations (addition, subtraction, multiplication, division with various rounding modes) and numeric transformations (floor, ceiling, truncation) for a term type `t` representing numeric expressions in the TPTP TFF logic. It supports conversions between numeric types (integers, rationals, reals) and includes exact division for rationals, alongside predicates to test term properties like being an integer or rational. These capabilities are designed for manipulating typed numeric expressions in formal verification or automated reasoning workflows involving TPTP TFF.",
      "description_length": 602,
      "index": 33,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Dimacs",
      "library": "dolmen.intf",
      "description": "This module represents propositional logic terms in DIMACS format. It provides logical negation via the `neg` function and works with the abstract type `t` representing clauses or literals. It is used for encoding and manipulating logical formulas in conjunctive normal form (CNF) for SAT solving.",
      "description_length": 297,
      "index": 34,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty.Smtlib_Real_Int",
      "library": "dolmen.intf",
      "description": "This module defines the type `t` for representing SMT-LIB real and integer expression types, along with values `int` and `real` to denote those respective types. It provides a polymorphic variant type `view` and a function `view` to inspect whether a type is an integer or real. It is used to distinguish and operate on numeric expression types in SMT-LIB parsing and type checking workflows.",
      "description_length": 392,
      "index": 35,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Ty.Smtlib_Int",
      "library": "dolmen.intf",
      "description": "This module defines the type `t` and the value `int` to represent and work with integer expressions in the context of SMT-LIB integer arithmetic. It provides the necessary typing infrastructure specifically for handling integer operations such as addition, multiplication, and comparisons. Concrete use cases include building and manipulating integer terms within an SMT solver interface that conforms to the SMT-LIB standard.",
      "description_length": 426,
      "index": 36,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Language.S-Lexer",
      "library": "dolmen.intf",
      "description": "This module defines the lexical analysis component for parsing input into tokens. It includes functions to generate tokens from a lexing buffer and associate descriptive error messages with each token. It is used to implement custom lexers for specific languages within the Dolmen framework.",
      "description_length": 291,
      "index": 37,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Ae_Arith",
      "library": "dolmen.intf",
      "description": "This module defines core arithmetic term operations for integer and real numbers, including literal creation and type retrieval. It supports terms annotated with semantic triggers specific to Alt-Ergo's arithmetic reasoning. Concrete use cases include constructing and manipulating arithmetic expressions in SMT solvers or formal verification tools.",
      "description_length": 349,
      "index": 38,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Tag.Ae_Base",
      "library": "dolmen.intf",
      "description": "This module defines the minimal set of tags required to typecheck Alt-Ergo's core/base theory. It includes tags for marking predicates, enabling AC (associative-commutative) treatment of terms, naming formulas, and specifying triggers and filters for quantified formulas. These tags attach metadata directly to AST nodes, enabling precise control over formula processing and reasoning behavior in Alt-Ergo.",
      "description_length": 406,
      "index": 39,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Bvconv",
      "library": "dolmen.intf",
      "description": "This module implements bitvector to natural number conversion and integer to bitvector conversion. It operates on bitvector terms, enabling transformations between numeric types within SMT-LIB compliant term representations. Concrete use cases include encoding arithmetic conversions in formal verification tasks and manipulating bitvector expressions in theorem proving.",
      "description_length": 371,
      "index": 40,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Tag.S",
      "library": "dolmen.intf",
      "description": "This module defines the core operations for creating and managing tags that can be attached to AST nodes. It provides the `create` function, which generates a new tag with an optional pretty-printing function for values of that tag's type. These tags are used to associate arbitrary typed metadata with abstract syntax tree elements during parsing, analysis, or transformation tasks.",
      "description_length": 383,
      "index": 41,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty.Smtlib_Float",
      "library": "dolmen.intf",
      "description": "This module defines type constructors and views for SMT-LIB bitvector and floating-point types. It supports creating fixed-size bitvector and float types with specified precision and provides a classification view for type inspection. Concrete use cases include modeling IEEE 754 floating-point types and bitvector operations in SMT solvers.",
      "description_length": 341,
      "index": 42,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Float-Bitv",
      "library": "dolmen.intf",
      "description": "This module provides a function `mk` that constructs bitvector literals from strings. It operates on a term type `t` representing SMT-LIB bitvector expressions. Useful for parsing and building bitvector terms in SMT solvers or formal verification tools.",
      "description_length": 253,
      "index": 43,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Stmt.Response",
      "library": "dolmen.intf",
      "description": "This module handles parsing and constructing prover responses, specifically for SAT/UNSAT results and optional certificates. It defines core types like `t`, `id`, `term`, `location`, and `defs`, and provides operations to create error messages, unsatisfiability responses, and satisfiability responses with models. Concrete use cases include building function definitions and recursive function groups to represent models in SMT solver responses.",
      "description_length": 446,
      "index": 44,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Ty.Smtlib_Array",
      "library": "dolmen.intf",
      "description": "This module defines the type and operations for SMT-LIB arrays, including the `array` function to construct functional array types and a `view` type to inspect the structure of types, supporting operations like array type construction and type analysis in SMT contexts. It works with types representing SMT expressions, including integers, reals, bitvectors, and arrays. Concrete use cases include building and analyzing SMT array types during type checking and constraint solving in SMT-LIB compliant tools.",
      "description_length": 508,
      "index": 45,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Float-Real",
      "library": "dolmen.intf",
      "description": "This module provides a function `mk` that constructs real number literals from strings. It works with the term type `t` representing SMT-LIB real values. Use it to parse and create real number expressions in SMT-LIB format for theorem proving tasks.",
      "description_length": 249,
      "index": 46,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty.Zf_Base",
      "library": "dolmen.intf",
      "description": "This module defines the core type `t` and a value `prop` representing the type of propositions. It is used to model logical propositions in TPTP (Thousands of Problems for Theorem Provers) typing systems. Concrete use cases include defining and manipulating logical formulas in automated theorem proving contexts.",
      "description_length": 313,
      "index": 47,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Id.Logic",
      "library": "dolmen.intf",
      "description": "This module defines identifiers with specific namespaces such as `sort`, `var`, `term`, `attr`, `decl`, and `track`, supporting operations to construct identifiers via `mk`, indexed names via `indexed`, qualified names via `qualified`, and tracked identifiers via `tracked`. It works with strings and lists to represent names and paths, enabling precise identification and tagging in formal language processing. Concrete use cases include managing variable and type namespaces in SMT solvers, tracking term origins in input files, and handling annotations and declarations in formal specifications.",
      "description_length": 598,
      "index": 48,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Ae_Arith-Real",
      "library": "dolmen.intf",
      "description": "This module provides arithmetic operations on real numbers, including addition, subtraction, multiplication, division, exponentiation, and comparison operators. It works with the `t` type representing terms in a formal language, typically used for real-valued expressions. Concrete use cases include constructing and manipulating real-number expressions in theorem proving or formal verification contexts.",
      "description_length": 405,
      "index": 49,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Tff",
      "library": "dolmen.intf",
      "description": "This module enables the construction and manipulation of first-order polymorphic terms, emphasizing algebraic datatypes (with constructors and destructors), records, and logical expressions. It supports operations like quantification (`forall`, `exists`), pattern matching, let-bindings, and term tagging, while enforcing type correctness through variable, constant, and field access mechanisms. Designed for formal verification or typed language implementations, it provides utilities to track free variables and manage structured data representations like ADTs and records in logical contexts.",
      "description_length": 595,
      "index": 50,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Float_Real",
      "library": "dolmen.intf",
      "description": "This module implements operations for handling floating-point and real-number literals in SMT-LIB format. It provides a concrete type `t` representing these numeric terms and a function `mk` to construct real-number literals from strings. It is used specifically for parsing and manipulating real-number expressions in SMT solvers.",
      "description_length": 331,
      "index": 51,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Float",
      "library": "dolmen.intf",
      "description": "This module defines operations for working with floating-point terms in SMT-LIB, including creation, manipulation, and type inspection. It handles floating-point values using dedicated constructors and interacts with real, bitvector, and float submodules for arithmetic and conversion operations. Concrete use cases include modeling IEEE-754 floating-point arithmetic in formal verification tasks, such as proving properties of numerical computations in software or hardware.",
      "description_length": 475,
      "index": 52,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_String_RegLan",
      "library": "dolmen.intf",
      "description": "This module represents regular languages over strings using operations such as concatenation, union, intersection, Kleene star, and complement. It provides functions to construct and manipulate regular languages, including ranges of characters, string repetition via power and loop, and set operations. Concrete use cases include parsing and analyzing SMT-LIB string constraints, implementing string pattern matchers, and modeling valid input formats for verification tasks.",
      "description_length": 474,
      "index": 53,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Thf-Cstr",
      "library": "dolmen.intf",
      "description": "This module defines operations for working with algebraic datatype constructors, including retrieving their type, comparing them, and determining argument types for pattern matching. It operates on constructor symbols and their associated types, providing the necessary tools to type-check pattern matching expressions. Concrete use cases include type-checking implementations for functional languages and symbolic manipulation in theorem proving systems.",
      "description_length": 455,
      "index": 54,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Ty.Thf",
      "library": "dolmen.intf",
      "description": "This module provides operations for constructing and analyzing higher-order logic types, including function type formation (`arrow`), polymorphic quantification (`pi`), and type variable binding, alongside utilities for substitution, free variable tracking, and type comparison. It operates on a composite type system built from variables (`Var.t`) and constants (`Const.t`), supporting constructs like universally quantified (Pi) types and function types. These capabilities are tailored for applications requiring rigorous polymorphic type manipulation, such as formal verification systems or languages with rank-1 polymorphism.",
      "description_length": 630,
      "index": 55,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Tptp_Thf_Core_Const",
      "library": "dolmen.intf",
      "description": "This module defines core logical and Boolean operations for terms, including constants for true and false, negation, conjunction, disjunction, implication, equivalence, and quantifiers. It works with the abstract term type `t` to represent logical expressions and formulas. Concrete use cases include constructing and manipulating logical statements in theorem proving or formal verification tasks.",
      "description_length": 398,
      "index": 56,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Real_Int-Real",
      "library": "dolmen.intf",
      "description": "This module provides arithmetic operations and comparisons for real and integer numbers, including addition, subtraction, multiplication, division, negation, and ordering relations. It supports a combined integer-real numeric type, allowing for mixed-type operations and predicates like checking if a real is an integer or computing its floor. Concrete use cases include formal verification tasks involving SMT arithmetic, such as modeling constraints over real and integer values in theorem proving or program analysis.",
      "description_length": 520,
      "index": 57,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ext.Logic",
      "library": "dolmen.intf",
      "description": "Implements core logic operations for term manipulation, including variable binding, quantifier handling, and logical connective processing. Works with abstract syntax trees represented through `term` and `statement` types, alongside location tracking. Enables concrete SMT-LIB 2.0 parsing and semantic analysis by providing foundational logic constructs.",
      "description_length": 354,
      "index": 58,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Float_Bitv",
      "library": "dolmen.intf",
      "description": "This module implements operations for bitvector literals in the context of SMT-LIB float requirements, specifically providing a function to construct bitvector terms from string representations. It works with a concrete term type that represents bitvectors as strings. A typical use case involves parsing and constructing bitvector expressions for SMT solvers according to the SMT-LIB standard.",
      "description_length": 394,
      "index": 59,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Base",
      "library": "dolmen.intf",
      "description": "This module defines core SMT-LIB logical operations and term constructions, including boolean connectives (negation, disjunction, conjunction, implication, equivalence), conditional expressions, equality chains, and distinctness constraints. It works with the abstract term type `t` and constraint type `cstr`, representing logical formulas and term-level conditions. Concrete use cases include building and manipulating SMT formulas for verification tasks, constraint solving, and program analysis.",
      "description_length": 499,
      "index": 60,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Ty.Tptp_Arith",
      "library": "dolmen.intf",
      "description": "This module defines the type representations and equality operations required for TPTP arithmetic types, including integers, rationals, and reals. It provides the values `int`, `rat`, and `real` to denote these numeric types, along with a function to check type equality. It is used to ensure consistent typing in TPTP-related logic processing tasks.",
      "description_length": 350,
      "index": 61,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Tff-Const",
      "library": "dolmen.intf",
      "description": "This module handles constant symbols in terms, providing operations to create, tag, and compare them. It works with constant values of type `t`, along with associated types like `ty` for types and `tag` for metadata. Concrete use cases include building and manipulating typed constants in a term language, such as assigning and retrieving tags for symbol metadata during term processing.",
      "description_length": 387,
      "index": 62,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty.Ae_Base",
      "library": "dolmen.intf",
      "description": "This module defines core type representations for booleans, unit, integers, and reals, providing direct values to reference these base types. It works with a single abstract type `t` that represents type instances within a typing context. Concrete use cases include defining and distinguishing primitive types in a type system for a logic or programming language.",
      "description_length": 363,
      "index": 63,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_String-String",
      "library": "dolmen.intf",
      "description": "This module supports symbolic string manipulation and pattern matching operations for SMT-LIB-compliant terms, focusing on lexicographic comparisons, concatenation, substring extraction, and regex-based replacement. It operates on a custom symbolic string type `t`, integrating regular language checks via the `RegLan` module for pattern membership verification. Designed for formal verification tasks, it enables analysis of string constraints in SMT solvers, particularly for security or program correctness applications involving Unicode-aware string transformations and structured text pattern matching.",
      "description_length": 607,
      "index": 64,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Tag.Smtlib_Base",
      "library": "dolmen.intf",
      "description": "This module defines tags for SMT-LIB base theory constructs, primarily for annotating terms and formulas with metadata. It includes tags for naming formulas (`named`), specifying multi-triggers for quantifiers (`triggers`), and marking terms as rewrite rules (`rwrt`). These tags are used during parsing and processing of SMT-LIB files to capture attributes that influence solver behavior or proof generation.",
      "description_length": 409,
      "index": 65,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Tptp_Tff_Arith",
      "library": "dolmen.intf",
      "description": "This module defines the interface for representing and manipulating TPTP arithmetic terms, including integer, rational, and real literals. It provides functions to construct literal values and retrieve term types, working with the `t` and `ty` types. Concrete use cases include parsing and type-checking arithmetic expressions in TPTP TFF formulas. The `Int`, `Rat`, and `Real` submodules offer domain-specific operations for each numeric type.",
      "description_length": 444,
      "index": 66,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_String-String-RegLan",
      "library": "dolmen.intf",
      "description": "This module implements operations on regular languages over strings, including language construction, concatenation, union, intersection, Kleene star, complement, and bounded repetition. It supports string-based regular expressions with operations like range, power, and loop, enabling precise lexicon and grammar constraints. Concrete use cases include defining character sets, validating string patterns, and expressing repetition bounds in SMT-LIB string constraints.",
      "description_length": 470,
      "index": 67,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty.Tff-Var",
      "library": "dolmen.intf",
      "description": "This module manages type variables, providing operations to create named variables and wildcards, compare and print them, and attach metadata via tags. It supports concrete use cases like tracking type variables during type inference or storing additional information such as type constraints or annotations. The primary data type is `t`, representing a type variable, along with associated tag-based metadata operations.",
      "description_length": 421,
      "index": 68,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty.Thf-Var",
      "library": "dolmen.intf",
      "description": "This module manages type variables with support for named variables, wildcards, and tagged metadata. It provides operations to create variables, compare and print them, and attach or retrieve arbitrary data via tags. Concrete use cases include representing and manipulating type variables during type inference or constraint solving.",
      "description_length": 333,
      "index": 69,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Tptp_Tff_Arith_Common",
      "library": "dolmen.intf",
      "description": "This module provides arithmetic operations (addition, subtraction, multiplication, division variants) and numeric transformations (floor, ceiling, rounding) alongside type conversion utilities for handling integers, rationals, and reals within a TPTP TFF term system. It operates on a polymorphic term type `t`, supporting both numeric value manipulation and rationality checks through dedicated predicates and conversion functions. These capabilities are particularly useful in formal verification contexts requiring precise arithmetic reasoning over mixed numeric domains and symbolic term representations.",
      "description_length": 608,
      "index": 70,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Ae_Arith-Int",
      "library": "dolmen.intf",
      "description": "This module defines arithmetic operations and comparisons for integer terms, including addition, subtraction, multiplication, exponentiation, negation, and Euclidean division. It supports constructing and manipulating integer expressions with functions like `add`, `mul`, `pow`, and comparisons such as `lt`, `le`, `gt`, and `ge`. These operations are used to build and reason about integer arithmetic within formal verification or theorem proving contexts.",
      "description_length": 457,
      "index": 71,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_String_String-RegLan",
      "library": "dolmen.intf",
      "description": "This module implements operations on regular languages over strings, including concatenation, union, intersection, Kleene star, and complement. It provides concrete language primitives such as empty, all, and character ranges, and supports string-based regular expression construction and manipulation. Use cases include parsing, string constraint solving, and formal verification tasks involving regular expressions.",
      "description_length": 417,
      "index": 72,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Language.S-Parser",
      "library": "dolmen.intf",
      "description": "Parses input files or streams into lists of statements using a given lexer function. It operates on lex buffers and produces either a list of statements or an optional statement for incremental parsing. Useful for reading source files or interactive input where statements are processed one at a time.",
      "description_length": 301,
      "index": 73,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Thf-Field",
      "library": "dolmen.intf",
      "description": "This module defines operations for comparing and manipulating constant symbols in the context of terms. It primarily works with the abstract type `t` representing term constants, supporting direct comparisons between them. Concrete use cases include managing symbol ordering and equality checks in term-based data structures or logic processing tasks.",
      "description_length": 351,
      "index": 74,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty.Smtlib_String",
      "library": "dolmen.intf",
      "description": "This module defines the type `t` and standard type values such as `int`, `string`, and `string_reg_lang`. It provides the foundational type definitions used to represent SMT-LIB string theories, including integers, strings, and regular languages. These types support operations like string concatenation, length computation, and membership checks for regular expressions in SMT solvers.",
      "description_length": 386,
      "index": 75,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Tag.Zf_Base",
      "library": "dolmen.intf",
      "description": "This module defines tags for annotating AST nodes with metadata related to printing and processing. It includes a flag to mark terms as rewrite rules, and tags to control identifier printing behavior using exact strings, infix, or prefix notation. These tags are used during pretty-printing or transformation passes over the AST to influence how terms and symbols are rendered or interpreted.",
      "description_length": 392,
      "index": 76,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Int",
      "library": "dolmen.intf",
      "description": "This module defines operations for SMT-LIB integer arithmetic, including construction of integer constants from strings, arithmetic operations (addition, subtraction, multiplication, division, remainder, absolute value), and comparison operators (less than, less or equal, greater than, greater or equal). It works with integer terms represented by the type `t` and constants of type `cst` parsed from strings to avoid precision issues. Concrete use cases include building and manipulating integer expressions for SMT solvers, such as encoding constraints like `x + y < 10`, checking divisibility conditions, or performing arithmetic simplifications.",
      "description_length": 650,
      "index": 77,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Location.S",
      "library": "dolmen.intf",
      "description": "This module handles the creation and manipulation of file location information using lexing buffers and positions. It operates on types `t` for locations and `file` for file metadata, providing functions to construct positions from lexbufs, track newlines, and update file offsets. Concrete use cases include tracking source code positions during parsing and managing file metadata for error reporting in lexers.",
      "description_length": 412,
      "index": 78,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Ae_Arith_Common",
      "library": "dolmen.intf",
      "description": "This module defines arithmetic operations and comparisons for a term type, including addition, subtraction, multiplication, exponentiation, and ordering relations. It works with the abstract type `t` representing terms in an arithmetic context. Concrete use cases include building and manipulating arithmetic expressions and constraints in formal verification or theorem proving tasks.",
      "description_length": 385,
      "index": 79,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Real_Int-Int",
      "library": "dolmen.intf",
      "description": "This module provides arithmetic operations and comparisons for integer terms, including addition, subtraction, multiplication, division, and ordering relations. It supports integer constants represented as strings to avoid overflow, and includes operations like absolute value, remainder, and divisibility checks. It is used for constructing and manipulating integer expressions in SMT-LIB compliant solvers.",
      "description_length": 408,
      "index": 80,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty.Ae_Array",
      "library": "dolmen.intf",
      "description": "This module defines the type `t` and operations for constructing functional array types, including `int` for integer indices and `array` for mapping one type to another. It works with type representations to support array-like structures in a type system. Concrete use cases include defining and manipulating array types within a typed intermediate representation, such as in a compiler or formal verification tool.",
      "description_length": 415,
      "index": 81,
      "embedding_norm": 0.9999998807907104
    },
    {
      "module_path": "Dolmen_intf.Term.Zf_Arith",
      "library": "dolmen.intf",
      "description": "Implements arithmetic operations over integers with functions for addition, subtraction, multiplication, and comparison. Works directly with the term type `t` and string representations of integer literals. Enables constructing and manipulating integer expressions in formal verification tasks.",
      "description_length": 294,
      "index": 82,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Tptp_Tff_Arith-Real",
      "library": "dolmen.intf",
      "description": "This module provides arithmetic operations (addition, multiplication, division, remainder) and comparisons (equality, ordering) for real numbers, along with conversions between integers, rationals, and reals, and exact division on reals. It operates on terms represented by the type `t`, which encodes logical expressions in a TPTP TFF (Typed First-Order Form) framework. These capabilities are specifically used in theorem proving systems to model and verify properties involving real arithmetic, numeric type constraints, and precise division operations within formal proofs.",
      "description_length": 577,
      "index": 83,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Ae_Bitv",
      "library": "dolmen.intf",
      "description": "This module defines interfaces for bitvector operations including bitwise logic (AND, OR, XOR, etc.), arithmetic (addition, subtraction, multiplication with modular behavior), shifts, sign/zero extensions, and comparisons (signed/unsigned). It operates on a custom bitvector type `t` with semantics aligned to SMT-LIB standards for modular arithmetic and sign handling. These interfaces are designed for formal verification tasks requiring precise bitvector reasoning, such as SMT solvers or hardware/software analysis tools.",
      "description_length": 525,
      "index": 84,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Ext.Logic-Smtlib2",
      "library": "dolmen.intf",
      "description": "Handles parsing and processing of SMT-LIB 2 custom statements not defined in the standard specification. It allows extending the logic with user-defined statements by providing a function that matches statement identifiers to custom parsing logic. This is useful for integrating domain-specific extensions into SMT solvers while maintaining compatibility with standard SMT-LIB 2 syntax.",
      "description_length": 386,
      "index": 85,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Zf_Arith-Int",
      "library": "dolmen.intf",
      "description": "Implements integer arithmetic operations including negation, addition, subtraction, and multiplication, along with comparison operators for ordering. Works directly with term representations of integers, enabling symbolic manipulation and evaluation. Useful for building and evaluating arithmetic expressions in formal verification or theorem proving contexts.",
      "description_length": 360,
      "index": 86,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Term.Tptp_Thf_Core",
      "library": "dolmen.intf",
      "description": "This module defines the minimal term structure required to represent TPTP THF (Typed Higher-Order Form) logic, including constants, distinctness constraints, and core term and type representations. It works with terms (`t`) and types (`ty`), along with a `Const` module for constant values. It is used to encode logical expressions involving constants and distinctness conditions in automated theorem proving contexts.",
      "description_length": 418,
      "index": 87,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Tff-Var",
      "library": "dolmen.intf",
      "description": "This module handles typed variables in terms, providing creation, type retrieval, and tag manipulation operations. It works with variables represented by the abstract type `t`, along with associated type information and tags. Concrete use cases include managing variable bindings during term construction and attaching metadata via tags for analysis or transformation tasks.",
      "description_length": 374,
      "index": 88,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_Float_Float",
      "library": "dolmen.intf",
      "description": "This module implements arithmetic operations (addition, fused multiply-add, square roots), comparisons (total and ordered variants), and classification predicates (zero, infinity, NaN checks) for SMT-LIB floating-point terms. It operates on a term type `t` with dedicated constant handling (`cst`), supporting bitvector reinterpretation, precision adjustments, and IEEE 754-2008 rounding modes. These capabilities enable formal verification of numerical code involving edge-case floating-point behavior, such as compiler optimizations or safety-critical system modeling.",
      "description_length": 570,
      "index": 89,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term.Smtlib_String",
      "library": "dolmen.intf",
      "description": "This module provides functions for converting SMT-LIB terms to and from string representations, handling parsing and pretty-printing. It operates on term data structures that conform to the SMT-LIB standard, enabling direct manipulation of terms in their textual format. Use cases include reading SMT-LIB input files, generating SMT-LIB output for solvers, and debugging term structures through their string representations.",
      "description_length": 424,
      "index": 90,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty.Tff-Const",
      "library": "dolmen.intf",
      "description": "This module defines operations for creating and manipulating type constants with associated metadata. It supports concrete operations such as comparing constants, printing them, and managing tags with arbitrary values. Typical use cases include building and managing symbolic representations of types with attached properties, such as in type inference or constraint solving systems.",
      "description_length": 383,
      "index": 91,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Parse",
      "library": "dolmen.intf",
      "description": "This module defines the core interface for parsers in the Dolmen project, centered on transforming input streams into structured logical expressions and constraint statements. It provides operations to parse files or incremental input using lexers, producing abstract syntax trees or intermediate representations, with support for SMT-LIB and custom domain-specific languages. Key data types include input streams, tokens, and statements, while primary operations enable parsing from files or buffers, and handling interactive input. Submodules extend this interface with specific parsing strategies, such as reading entire files into statement lists or processing input line by line.",
      "description_length": 684,
      "index": 92,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Ty",
      "library": "dolmen.intf",
      "description": "This module defines type interfaces for formal logic and constraint systems, including first-order logic, SMT-LIB, TPTP, and ZF set theory, with core operations for building and verifying type-correct expressions. It provides base types like integers, reals, bitvectors, strings, and arrays, along with type constructors for functions, polymorphism, and higher-order logic, enabling precise modeling of logical and arithmetic expressions. Submodules handle specific type systems such as TPTP propositions and arithmetic, SMT-LIB bitvectors, floats, arrays, and strings, supporting tasks like theorem proving, type inference, and constraint solving. Examples include constructing fixed-size bitvectors for low-level arithmetic, defining array types for SMT solvers, and manipulating higher-order logic types with polymorphic quantification.",
      "description_length": 839,
      "index": 93,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Language",
      "library": "dolmen.intf",
      "description": "This module provides a unified framework for defining and processing formal languages with support for parsing, type-checking, and evaluating terms. It works with abstract syntax trees, type environments, and term representations, enabling precise semantic handling of domain-specific languages. The core module manages input sources, token streams, and statement parsing, while the lexical submodule generates tokens and handles error reporting. Together, they support building compilers or interpreters that process code from files, strings, or interactive input with robust error recovery and include support.",
      "description_length": 612,
      "index": 94,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Stmt",
      "library": "dolmen.intf",
      "description": "This module defines interfaces for top-level statements and declarations in files, focusing on logic and response handling for domain-specific languages in formal verification and logic programming. It provides core operations for processing logical constructs, managing solver state, and interpreting prover responses, with key data types such as `t`, `id`, `term`, and `location`. The first child module enables constructing logical statements, asserting formulas, and managing solver configurations, while the second specializes in parsing and building prover responses, including SAT/UNSAT results and model certificates. Together, they support tasks like parsing SMT-LIB or TPTP files, managing assertion stacks, and extracting models or proofs from solver outputs.",
      "description_length": 770,
      "index": 95,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Ext",
      "library": "dolmen.intf",
      "description": "This module provides interfaces for extending logic systems with custom theories and solvers, operating on abstract representations of terms, formulas, and proof rules. It supports term manipulation with variable binding and logical connectives through `term` and `statement` types, and enables integration of domain-specific extensions via customizable parsing and semantic analysis. One submodule implements core logic operations for SMT-LIB 2.0 parsing and quantifier handling, while another allows defining custom SMT-LIB statements that extend standard syntax with user-defined logic. Examples include building domain-specific reasoning engines and adding custom solver directives to theorem provers.",
      "description_length": 705,
      "index": 96,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Term",
      "library": "dolmen.intf",
      "description": "This module provides interfaces for constructing and manipulating terms used in formal verification and automated reasoning, combining logical, arithmetic, and data structure operations with support for SMT-LIB and TPTP standards. It defines a polymorphic term type `t` alongside identifiers, variables, constants, and annotations, enabling precise modeling of logical formulas, arithmetic expressions, and structured data such as arrays, bitvectors, strings, and algebraic datatypes. Submodules offer specific operations like logical connectives, quantifiers, integer and real arithmetic, bitvector manipulations, array reads/writes, string transformations, and floating-point operations, all designed to support theorem proving, constraint solving, and symbolic execution. Examples include building typed logical expressions with quantifiers, encoding arithmetic constraints over integers or reals, modeling array-based programs, and representing bitvector or string operations with SMT-LIB compatibility.",
      "description_length": 1007,
      "index": 97,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Msg",
      "library": "dolmen.intf",
      "description": "This module provides operations to create and manipulate delayed text messages using closures that accept a formatter, enabling proper composition with Format's boxes and break hints. It works with the type `t`, which represents a message as a function from a formatter to unit. Concrete use cases include building nested or reusable message components that maintain correct formatting behavior when combined, such as generating error messages that are later embedded within larger diagnostic outputs.",
      "description_length": 501,
      "index": 98,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Tag",
      "library": "dolmen.intf",
      "description": "This module provides interfaces for attaching metadata to AST nodes, enabling frameworks like SMT-LIB and ZF to associate information such as source locations, type annotations, and solver attributes with terms. It supports creating and managing typed tags through operations like `create`, while submodules define specific tags for core theories, SMT-LIB constructs, and printing controls. Tags can mark predicates, specify quantifier triggers, name formulas, or influence term rendering, allowing precise control over processing, reasoning, and output formatting. Examples include attaching `named` tags to formulas, defining rewrite rules with `rwrt`, and using `triggers` to guide quantifier instantiation.",
      "description_length": 710,
      "index": 99,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Map",
      "library": "dolmen.intf",
      "description": "This module provides a polymorphic map interface for creating and manipulating key-value associations with ordered keys. It supports core operations like insertion, lookup, iteration, and folding, enabling tasks such as managing symbol tables or caching indexed computations. Submodules extend this functionality with optimized implementations and additional utilities for key-based retrieval and transformation. For example, `find_opt` allows safe value lookup, while `add` and `find_add` enable efficient map updates.",
      "description_length": 519,
      "index": 100,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Tok",
      "library": "dolmen.intf",
      "description": "This module defines a `descr` record type for representing token descriptions, including fields like `article`, `kind`, `lexeme`, and an optional `hint`. It is used to provide structured metadata about tokens in parsing or lexing workflows. Concrete use cases include associating human-readable information with lexical tokens for error reporting or documentation generation.",
      "description_length": 375,
      "index": 101,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Pretty",
      "library": "dolmen.intf",
      "description": "This module defines types for specifying annotations used in pretty printing, such as exact or renamed identifiers, infix/prefix positions, and left/right associativity. It works with strings and variants to represent formatting directives for structured data. Concrete use cases include customizing the display of expressions, operators, and declarations in parsers and pretty printers.",
      "description_length": 387,
      "index": 102,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Lex",
      "library": "dolmen.intf",
      "description": "This module defines the interface for lexers used in parsing first-order logic and SMT-LIB input streams, providing functions for tokenizing input, managing lexing states, and handling whitespace and comments. It includes the core token type and operations to generate and process tokens, enabling the parsing of logical expressions, quantifiers, and solver commands. The child module implements essential lexer operations, associating tokens with error messages and supporting the creation of custom lexers for domain-specific languages. Together, they allow developers to build robust, structured input parsers with precise error handling and state management.",
      "description_length": 662,
      "index": 103,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_intf.Location",
      "library": "dolmen.intf",
      "description": "This module provides data types `t` and `file` for representing file locations and metadata, with operations to construct, compare, and update positions based on lexing buffers. It supports tracking expression spans and parsing errors by integrating with exceptions that carry location information. The child module extends this by enabling precise position tracking during lexing, handling newline updates and file offset management. Together, they facilitate detailed error reporting and source code analysis in parsers and language tools.",
      "description_length": 541,
      "index": 104,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf.Id",
      "library": "dolmen.intf",
      "description": "This module defines interfaces for identifiers used in logic languages, specifying core operations through the `Logic` and `Escape` module types. It supports identifiers that can be logically manipulated and properly escaped, working with strings and structured names to ensure syntactic correctness and semantic consistency. The `Escape` submodule handles encoding and decoding of identifier names for input/output, while the `Namespaced` submodule introduces identifiers under specific categories like `var`, `term`, and `decl`, with constructors for indexed, qualified, and tracked names. Examples include escaping identifiers for SMT-LIB output, creating namespaced variables in theorem provers, and tracking term origins in formal verification tools.",
      "description_length": 755,
      "index": 105,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_intf",
      "library": "dolmen.intf",
      "description": "This module provides a comprehensive framework for parsing, representing, and manipulating formal logic and constraint systems, supporting languages like SMT-LIB, TPTP, and ZF set theory. It centers on data types such as terms, types, statements, tokens, and locations, with operations for parsing input streams, building typed logical expressions, managing solver state, and generating precise error messages with source tracking. Users can define custom theories, extend existing syntax with domain-specific constructs, and build tools that process logical formulas with support for arithmetic, bitvectors, arrays, and higher-order logic. Specific applications include constructing SMT-LIB parsers, implementing theorem provers with custom inference rules, and developing verified compilers with rich type systems and detailed diagnostics.",
      "description_length": 841,
      "index": 106,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_tptp_v6_3_0.Make.Parser",
      "library": "dolmen_tptp_v6_3_0",
      "description": "Parses TPTP language input into statement structures using lexer buffers. It provides functions to parse either an entire file into a list of statements or individual statements incrementally. This module is used for processing TPTP problem files or incremental input streams in automated theorem proving workflows.",
      "description_length": 315,
      "index": 107,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_tptp_v6_3_0.Make.Lexer",
      "library": "dolmen_tptp_v6_3_0",
      "description": "This module defines the lexical analysis component for parsing TPTP input, converting character streams into structured tokens. It operates on `lexbuf` input, producing `token` values and associating descriptive error information with each token. It is used directly by the parser to process TPTP formulas and problem files into abstract syntax trees.",
      "description_length": 351,
      "index": 108,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_tptp_v6_3_0.Term",
      "library": "dolmen_tptp_v6_3_0",
      "description": "This module provides functions to construct and manipulate higher-order logic terms, including logical connectives (e.g., conjunction, disjunction, implication), quantifiers (universal and existential), term constructors (variables, constants, type annotations), type operations (function types, products, unions), lambda abstraction, conditionals, let bindings, and description operators. It operates on terms (`t`), identifiers (`id`), and optional source location tracking (`location`), forming a structured representation for formal logic expressions. These capabilities are critical for automated theorem proving systems that require precise parsing, manipulation, and verification of logical and type-theoretic statements.",
      "description_length": 728,
      "index": 109,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_tptp_v6_3_0.Id",
      "library": "dolmen_tptp_v6_3_0",
      "description": "This module handles the creation and manipulation of identifiers within specific namespaces for terms and declarations in the TPTP language. It provides the `mk` function to construct identifiers from a namespace and string, and defines standard namespaces for terms and declarations. Concrete use cases include generating properly scoped identifiers when parsing or building TPTP expressions and statements.",
      "description_length": 408,
      "index": 110,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_tptp_v6_3_0.Statement",
      "library": "dolmen_tptp_v6_3_0",
      "description": "This module represents TPTP language statements with support for annotations, logical formulas, and directives. It operates on data types including terms, identifiers, and locations to construct and register statements like THF, TFF, FOF, and CNF. Concrete use cases include parsing and registering TPTP input files, handling include directives, and attaching annotations to logical terms.",
      "description_length": 389,
      "index": 111,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_tptp_v6_3_0.Make",
      "library": "dolmen_tptp_v6_3_0",
      "description": "This module generates a parser for the TPTP format, enabling both full and incremental parsing of input from files, standard input, or strings. It structures TPTP statements using a lexer and parser, and includes utilities for file resolution relative to a given directory. The main data types include `token` and structured statements, with operations for parsing entire files or individual statements on demand. Example uses include streaming TPTP problem files into a list of statements or processing incremental input in theorem proving workflows.",
      "description_length": 551,
      "index": 112,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_tptp_v6_3_0",
      "library": "dolmen_tptp_v6_3_0",
      "description": "This module parses and processes TPTP v6.3.0 input, handling identifiers, terms, and statements while providing structured representations for logical and type-theoretic expressions. It supports the full TPTP syntax through a lexer and parser that can process input from files, strings, or streams, and includes utilities for managing namespaces, resolving includes, and tracking source locations. The module enables the construction and manipulation of higher-order logic terms with support for connectives, quantifiers, lambda abstraction, and type operations, all structured around core data types like terms (`t`), identifiers (`id`), and statements. Example uses include reading and analyzing TPTP problem files, transforming logical formulas, and integrating TPTP input into theorem proving systems with precise term manipulation and annotation handling.",
      "description_length": 860,
      "index": 113,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.String.Reg_Lang",
      "library": "dolmen.std",
      "description": "This module defines constant symbols for constructing and manipulating regular languages as first-order terms. It provides operations for creating basic languages like empty, all, and singleton strings, as well as combining languages through concatenation, union, intersection, and Kleene star. These constructs are used to represent regular expressions directly within term logic, enabling precise symbolic reasoning over string constraints.",
      "description_length": 442,
      "index": 114,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Rat",
      "library": "dolmen.std",
      "description": "This module enables arithmetic operations (addition, multiplication, division variants), comparisons, and rounding of rational number constants embedded in term expressions, while providing predicates to distinguish rational symbols from other constant terms. Its features are particularly valuable in formal verification and constraint-solving scenarios requiring precise manipulation of numerical expressions within logical formulas.",
      "description_length": 435,
      "index": 115,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Float",
      "library": "dolmen.std",
      "description": "This module supports floating-point arithmetic operations (addition, multiplication, FMA), comparisons (less-than, equality), classification tests (NaN, infinity checks), and conversions between floating-point values, real numbers, and bitvectors. It operates on integer tuples encoding bitvector representations of floats, enabling symbolic manipulation of IEEE 754-compliant floating-point terms. These capabilities are used in formal verification tasks requiring precise modeling of floating-point behavior, such as validating numerical algorithms or hardware designs.",
      "description_length": 571,
      "index": 116,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Int",
      "library": "dolmen.std",
      "description": "This module supports arithmetic operations, comparisons, and divisibility checks on integer and rational constants within first-order terms, including unary negation, exponentiation, rounding functions, and type-testing predicates. It operates on numeric values embedded in term structures, enabling symbolic manipulation and constraint reasoning. These capabilities are particularly useful in formal verification and automated reasoning contexts where integer arithmetic and type constraints must be rigorously analyzed.",
      "description_length": 521,
      "index": 117,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Array",
      "library": "dolmen.std",
      "description": "This module defines constant symbols for array operations, specifically array selection and storage. It provides the `const`, `select`, and `store` values, which represent array constants, read operations, and write operations, respectively. These are used to model array expressions within first-order logic terms.",
      "description_length": 315,
      "index": 118,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.String",
      "library": "dolmen.std",
      "description": "This module provides operations for manipulating string constants embedded in terms, supporting lexicographic comparisons, substring extraction, code point conversion, and regular language membership testing. Its core functionality enables precise string constraint analysis through direct API operations and a dedicated submodule for regular languages. The submodule allows construction and manipulation of regular expressions as first-order terms, supporting operations like concatenation, union, intersection, and Kleene star. Together, they enable symbolic reasoning over string constraints in formal verification tasks, such as checking whether a term belongs to a regular language or extracting substrings for further analysis.",
      "description_length": 733,
      "index": 119,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Real",
      "library": "dolmen.std",
      "description": "This module provides arithmetic operations (addition, subtraction, multiplication, division, exponentiation, remainders), comparison operators, and rounding or truncation functions for real number constants represented as `Dolmen_std.Expr.Term.Const.t`. It supports manipulation and type testing of these constants within term expressions, particularly in contexts like formal verification or symbolic computation requiring precise real number handling.",
      "description_length": 453,
      "index": 120,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const.Bitv",
      "library": "dolmen.std",
      "description": "This module enables the manipulation of bitvector constants within first-order term expressions, providing bitwise operations (e.g., AND, XOR), arithmetic computations (addition, multiplication, division), and transformations (concatenation, extraction, shifting). It operates on bitvector values represented as term constants, supporting both signed and unsigned semantics for comparisons and extensions. These operations are particularly suited for applications in symbolic computation, formal verification, and constraint-solving domains requiring precise bit-level modeling.",
      "description_length": 578,
      "index": 121,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.String.RegLan",
      "library": "dolmen.std",
      "description": "This module implements operations on regular languages for string terms, including language construction, concatenation, union, intersection, Kleene star, complement, and bounded repetition. It works directly with `Dolmen_std.Expr.Term.t` values representing string languages, supporting both atomic string terms and complex regular expressions. Concrete use cases include defining string constraints in SMT solvers, such as matching patterns, validating input formats, and expressing string transformations.",
      "description_length": 508,
      "index": 122,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Array",
      "library": "dolmen.std",
      "description": "This module provides operations for constructing and manipulating array terms in a first-order logic context. It supports creating constant arrays, selecting elements by index, and updating array elements. These functions are used to model array-based logic in formal verification tasks, such as representing memory states or indexed data structures in program analysis.",
      "description_length": 370,
      "index": 123,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Const",
      "library": "dolmen.std",
      "description": "This module handles constant symbols in first-order logic terms, offering operations to construct and manipulate typed constants such as integers, rationals, reals, bitvectors, strings, and arrays. It provides logical connectives, equality, and conditionals for term assembly, while its submodules enable domain-specific operations: arithmetic and comparisons on numbers, bitvector manipulations, array reads/writes, and string operations including regular language membership. For example, it can model array updates symbolically using `select` and `store`, validate floating-point computations under IEEE rules, or analyze string constraints against regular expressions. The integration of typed constants with structured term operations supports formal verification, constraint solving, and symbolic reasoning tasks.",
      "description_length": 819,
      "index": 124,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Subst.Var",
      "library": "dolmen.std",
      "description": "This module represents variables used in substitutions, providing operations to bind, retrieve, check existence, and remove variable-value mappings in a substitution environment. It works with variable keys and arbitrary value types, supporting precise manipulation of variable bindings during term rewriting or evaluation. Concrete use cases include managing variable assignments in logic solvers or symbolic computation systems.",
      "description_length": 430,
      "index": 125,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Ty.Var",
      "library": "dolmen.std",
      "description": "This module handles type variables, including creation, comparison, and tagging operations. It supports concrete data types like strings, integers, and tagged values, with functions to manage metadata via tags. Use cases include representing and manipulating type variables in type systems, tracking wildcards, and associating contextual data through tags.",
      "description_length": 356,
      "index": 126,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Var",
      "library": "dolmen.std",
      "description": "This module manages typed variables within terms, providing operations to create variables with associated types, retrieve their type and tags, and manipulate tagged values. It supports hash-consing, equality checks, and comparison for efficient use in data structures like hash tables and sets. Concrete use cases include representing and manipulating first-order logic variables with metadata in theorem proving or symbolic computation tasks.",
      "description_length": 444,
      "index": 127,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Float",
      "library": "dolmen.std",
      "description": "This module enables precise manipulation of SMT-LIB floating-point expressions through arithmetic operations (addition, division, square roots), comparison predicates (equality, ordering), and classification checks (NaN, infinity detection). It operates on polymorphic term structures representing floating-point values while supporting conversions to and from bitvectors, real numbers, and integers. These capabilities are particularly useful in formal verification contexts requiring rigorous handling of floating-point semantics or interfacing with SMT solvers that model hardware/software numerical behavior.",
      "description_length": 612,
      "index": 128,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Int",
      "library": "dolmen.std",
      "description": "This module provides arithmetic operations (addition, subtraction, multiplication, division, remainder with various rounding modes), comparisons (`\u2264`, `>`, etc.), and type conversions for integer terms adhering to the SMT-LIB standard. It operates on integer expressions represented as `Dolmen_std.Expr.Term.t`, enabling precise modeling of integer constraints in SMT logic. These capabilities are particularly useful for formal verification tasks involving integer arithmetic, such as proving properties of programs or hardware circuits where integer overflows, divisions, or inequalities must be rigorously checked.",
      "description_length": 617,
      "index": 129,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Expr.Ty.Const",
      "library": "dolmen.std",
      "description": "This module provides operations for constructing and manipulating type constants with specified arities, handling metadata through tags, and performing standard operations like equality checks, hashing, and comparison. It works with type constants and expressions representing built-in types such as integers, arrays, strings, and bitvectors, and is used in formal logic contexts for type analysis, symbolic manipulation, and defining annotated type symbols.",
      "description_length": 458,
      "index": 130,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Cstr",
      "library": "dolmen.std",
      "description": "This module handles algebraic data type constructors, providing operations to inspect and manipulate their structure. It supports typed equality, hashing, and comparison, along with functions to retrieve and modify constructor tags and associated values. It is used during type-checking of pattern matching and for managing constructor metadata in a typed manner.",
      "description_length": 363,
      "index": 131,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.String",
      "library": "dolmen.std",
      "description": "This module provides string manipulation and regular language operations for SMT-LIB compliant terms, centered around `Dolmen_std.Expr.Term.t` as the core data type. It supports concatenation, substring extraction, lexicographic comparisons, and regex-based pattern matching through the integrated `RegLan` module, enabling construction and analysis of complex string constraints. Operations include both direct term transformations and high-level regular language combinators like union, Kleene star, and intersection. Examples include validating input formats, expressing string transformations, and defining membership constraints in formal verification tasks.",
      "description_length": 663,
      "index": 132,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Subst.S",
      "library": "dolmen.std",
      "description": "This module implements concrete substitution operations for managing key-value associations in a substitution map. It supports binding, retrieving, checking existence, and removing key-value pairs where keys are of type `'a key` and values are of type `'b`. Useful for maintaining and modifying mappings during expression transformations or variable replacements.",
      "description_length": 363,
      "index": 133,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Real",
      "library": "dolmen.std",
      "description": "This module provides arithmetic operations (division, Euclidean division, truncation, rounding) and comparisons (inequalities, exponentiation) for real numbers represented as first-order logical terms. It operates on polymorphic term structures that support SMTLib-compatible real number semantics, enabling precise manipulation of real-valued expressions in formal verification contexts. Specific use cases include constraint solving, theorem proving, and symbolic analysis where real arithmetic must align with SMT solver requirements.",
      "description_length": 537,
      "index": 134,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Field",
      "library": "dolmen.std",
      "description": "This module directly manages record fields as term constants, offering operations to print, hash, compare, and check equality of fields. It supports attaching, retrieving, and modifying tagged values and lists of values, including getting the last value of a list tag or unsetting a tag. Concrete use cases include building and manipulating structured data representations in a first-order logic context, such as encoding algebraic data types or managing term attributes in a theorem prover.",
      "description_length": 491,
      "index": 135,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Bitv",
      "library": "dolmen.std",
      "description": "This module provides bitwise logic, arithmetic operations, and bit-level manipulations (such as concatenation, extraction, shifting, and rotation) for SMT bitvectors, alongside comparisons and conversions between bitvectors and integers. It operates on typed terms representing bitvectors or integers, encoded as `Dolmen_std.Expr.Term.t` values to ensure SMTLIB compliance. These capabilities are essential for formal verification tasks requiring precise modeling of hardware or low-level software behavior, such as register-transfer-level circuits or binary arithmetic analysis.",
      "description_length": 579,
      "index": 136,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term.Rat",
      "library": "dolmen.std",
      "description": "This module supports arithmetic operations (addition, subtraction, multiplication, division with rounding modes), comparisons (less than, greater than), and type conversions (to integers, rationals, reals) for rational numbers represented as terms. It operates on values of type `Dolmen_std.Expr.Term.t`, which encode arithmetic expressions in TPTP logic, enabling precise manipulation of rational constants and expressions. These capabilities are critical in formal verification and automated reasoning tasks requiring exact arithmetic and type consistency within first-order logic frameworks.",
      "description_length": 594,
      "index": 137,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Ty",
      "library": "dolmen.std",
      "description": "This module combines type construction with variable and constant management to support polymorphic type manipulation, unification, and attribute-rich type expressions. It provides core data types like type variables, type constants, and structured types (arrows, universal quantifiers), along with operations for substitution, comparison, and tagged metadata handling. You can build complex type expressions, perform type inference with wildcards, and annotate types with attributes for use in formal verification or logic programming. Submodules enable fine-grained control over type variables and constants, supporting tasks like tracking contextual metadata or defining annotated built-in types.",
      "description_length": 699,
      "index": 138,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Stats.Floats",
      "library": "dolmen.std",
      "description": "Tracks and manages floating-point statistics, such as timing data, under a given name. It supports adding values, printing current statistics, and maintaining a running record of collected data. Useful for profiling code execution times and aggregating performance metrics.",
      "description_length": 273,
      "index": 139,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Stats.Float",
      "library": "dolmen.std",
      "description": "This module defines a float-valued statistic with operations to create, update, and print its value. It works with a named statistic type that holds a float. Useful for tracking and displaying metrics like performance counters or configuration values.",
      "description_length": 251,
      "index": 140,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Name.Map",
      "library": "dolmen.std",
      "description": "Implements a map structure specialized for `Dolmen_std.Name.t` keys, providing operations to add, find, iterate, and fold over bindings. Supports efficient key-based access with both exception and option-based lookups. Useful for managing symbol tables or environments where named entities require associated metadata during parsing or analysis tasks.",
      "description_length": 351,
      "index": 141,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Id.Map",
      "library": "dolmen.std",
      "description": "Implements standard map operations for identifier-keyed data structures, including insertion, lookup, and iteration. Works with associative maps where keys are identifiers and values can be arbitrary. Useful for managing symbol tables, environment mappings, and identifier-to-value associations in compilers or formal verification tools.",
      "description_length": 337,
      "index": 142,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Namespace.Map",
      "library": "dolmen.std",
      "description": "This module implements maps keyed by namespaces, supporting standard operations like insertion, lookup, and iteration. It provides functions for adding bindings, finding values by namespace (with options for exception or optional return), and folding or iterating over namespace-value pairs. Concrete use cases include managing symbol tables or configuration settings scoped to hierarchical namespaces.",
      "description_length": 402,
      "index": 143,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Print",
      "library": "dolmen.std",
      "description": "This module configures and provides printers for expression components like identifiers, types, terms, and formulas, with options to control tag and index visibility during output. It supports customizable pretty-printing through tags such as `name` and `pos`, influencing how identifiers and structured expressions are rendered. Use cases include generating human-readable representations of expressions for debugging, logging, or interfacing with external tools.",
      "description_length": 464,
      "index": 144,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Subst",
      "library": "dolmen.std",
      "description": "This module manages substitutions as finite maps from variables to arbitrary values, enabling operations like iteration, mapping, folding, filtering, and merging. It provides utilities for inspecting, comparing, hashing, and printing substitutions, with customizable handling of keys and values. The variable module allows binding, retrieving, and manipulating variable-value pairs, while the substitution operations module supports structured updates and queries on key-value associations. Together, they facilitate tasks like managing variable bindings during term rewriting, symbolic computation, or logic solver execution.",
      "description_length": 626,
      "index": 145,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Term.S",
      "library": "dolmen.std",
      "description": "This module supports creation, modification, and comparison of sets of identifiers, including union, intersection, difference, and predicate-based filtering, alongside ordered traversal and extremal element retrieval. It operates on sets of `Dolmen_std.Id.t`, enabling conversion to and from lists/sequences while preserving uniqueness and order. Use cases include managing unique identifiers, dependency resolution, and scenarios requiring ordered set processing or aggregation.",
      "description_length": 479,
      "index": 146,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Maps.Int",
      "library": "dolmen.std",
      "description": "This module implements a map structure specialized for integer keys, supporting operations like insertion, lookup (with and without exceptions), and iteration. It provides functions to add key-value pairs, retrieve values by key, and traverse or fold over the map's contents. Concrete use cases include managing symbol tables indexed by integers or tracking dynamic state with integer identifiers.",
      "description_length": 397,
      "index": 147,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Escape.Make",
      "library": "dolmen.std",
      "description": "This module creates and manages identifier escapers for specific languages, ensuring valid and safe string representations of identifiers. It works with a custom identifier type and provides functions to define escaping and renaming strategies, automatically handling name clashes during printing. Use it when generating code or output that requires identifiers to conform to language-specific syntax rules and avoid injection vulnerabilities.",
      "description_length": 443,
      "index": 148,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Maps.Make",
      "library": "dolmen.std",
      "description": "Implements a map structure parameterized by a key-ordering module, supporting empty map creation, key-based lookups with and without exceptions, value insertion that shadows existing keys, and map traversal via iteration and folding. Works with any ordered key type defined by the Ord module and associated value types. Useful for maintaining sorted associative collections where keys require custom comparison logic, such as symbol tables indexed by complex data types.",
      "description_length": 470,
      "index": 149,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Maps.String",
      "library": "dolmen.std",
      "description": "This module implements a map data structure specialized for string keys, supporting operations like insertion, lookup, iteration, and folding. It provides functions to add key-value pairs, retrieve values via `find_exn` or `find_opt`, and traverse or transform the map using `iter` and `fold`. Concrete use cases include managing symbol tables, configuration settings, or any keyed data where efficient string-based lookups and transformations are required.",
      "description_length": 457,
      "index": 150,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Normalize.Tptp",
      "library": "dolmen.std",
      "description": "This module provides a mapper to normalize terms parsed from TPTP problem files by converting syntactic constructions into corresponding Dolmen term builtins. It specifically handles TPTP-specific reductions, such as equality and type symbols, mapping them to their semantic equivalents in Dolmen's term representation. Use this mapper when processing TPTP input to ensure terms align with Dolmen's internal builtin semantics for further analysis or translation.",
      "description_length": 462,
      "index": 151,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Transformer.Make",
      "library": "dolmen.std",
      "description": "This module implements a parser for transforming input sources into structured statements using a token stream. It handles incremental parsing, error reporting with custom messages, and environment-based file resolution. Key operations include parsing from files, standard input, or string contents, producing a lazy list of statements or an iterator over them.",
      "description_length": 361,
      "index": 152,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Term",
      "library": "dolmen.std",
      "description": "This module enables the construction and manipulation of first-order polymorphic terms with rich support for algebraic data types, logical operations, and type system constructs. It provides core data types for variables, constants, and structured terms, along with operations for term substitution, pattern matching, and logical quantification, all integrated with typed arithmetic, bitvector, and string operations from its submodules. You can define and analyze complex data structures like arrays and records, perform symbolic reasoning with floating-point or integer arithmetic, and express constraints over strings and regular languages for formal verification. Specific capabilities include modeling memory with array reads and writes, validating numeric properties with SMT arithmetic, and encoding structured data via algebraic types and typed fields.",
      "description_length": 860,
      "index": 153,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Extensions.Smtlib2",
      "library": "dolmen.std",
      "description": "This module manages SMT-LIB 2 extensions, allowing creation, activation, and lookup of custom statement handlers. It works with a custom type `t` representing extensions, each having a name and a list of associated statements. Concrete use cases include enabling MaxSMT support and defining custom SMT-LIB commands that parse and construct terms into statements.",
      "description_length": 362,
      "index": 154,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr.Tags",
      "library": "dolmen.std",
      "description": "This module defines tags and associated operations for annotating terms and formulas with metadata such as binding information, printing directives, and logical properties. It works primarily with terms and tags from the `Expr` module, along with strings and positional information for identifiers. These tags control behaviors like variable binding, rewriting, associativity, infix/prefix notation, and trigger/filter attachment in automated reasoning tasks.",
      "description_length": 459,
      "index": 155,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Expr.Id",
      "library": "dolmen.std",
      "description": "This module manages identifiers with associated metadata, providing operations to create, compare, hash, and print them. It supports attaching tags to identifiers, allowing for storing, retrieving, and modifying both single values and lists of values linked to each tag. Concrete use cases include tracking variables in a symbolic computation system, managing named entities with positional information, and handling extensible metadata in a type-safe way.",
      "description_length": 456,
      "index": 156,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Normalize.Smtlib",
      "library": "dolmen.std",
      "description": "This module provides a mapper to normalize terms parsed from SMT-LIB files by converting specific syntactic constructs into their corresponding built-in term representations. It handles conversions such as mapping the \"=\" symbol to the built-in equality operator, aligning parsed expressions with Dolmen's internal term structure. It is specifically used when processing SMT-LIB input to ensure semantic consistency with Dolmen's term definitions.",
      "description_length": 447,
      "index": 157,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Answer",
      "library": "dolmen.std",
      "description": "This module creates and manages answers for logical queries, supporting operations to construct unsatisfiable, satisfiable with optional model, and error responses. It works with terms, locations, and function definitions to build structured outputs for theorem proving tasks. Functions like `unsat`, `sat`, and `fun_def` enable precise modeling of logical results, while `print` outputs them in a readable format.",
      "description_length": 414,
      "index": 158,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Escape",
      "library": "dolmen.std",
      "description": "This module handles the transformation and escaping of identifiers to ensure they conform to language-specific syntax rules, preventing injection vulnerabilities. It provides core operations like `smap`, which maps characters in a string based on their position, and `rename`, which appends incrementing indices to generate unique names. The child module extends this functionality by offering language-specific escapers that automate safe identifier generation, making it ideal for code generation or serialization tasks where identifier validity and uniqueness are critical. Together, they enable precise control over identifier formatting and escaping strategies across different contexts.",
      "description_length": 692,
      "index": 159,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Stats",
      "library": "dolmen.std",
      "description": "This module tracks and analyzes floating-point values across named categories, aggregating statistics such as averages, counts, and extremes. It supports dynamic creation and management of named statistics, enabling real-time monitoring and profiling of numerical data. Child modules provide concrete operations for recording values, maintaining running statistics, and printing summaries, with support for custom metric types and performance counters. For example, it can track function execution times, monitor system load, or aggregate configuration values over time.",
      "description_length": 570,
      "index": 160,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Statement",
      "library": "dolmen.std",
      "description": "This module provides operations for constructing and manipulating logical statements, type declarations, and prover interaction commands in a formal logic framework. It works with terms, types, identifiers, and structured statement data to model SMT-LIB-inspired constructs like assertions, definitions, inductive types, and proof directives. Specific use cases include formal verification tasks requiring precise logical modeling, prover state management (e.g., push/pop assertions), and generating structured output for theorem proving workflows.",
      "description_length": 548,
      "index": 161,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Msg",
      "library": "dolmen.std",
      "description": "This module defines a type `t` representing messages, typically used for reporting errors, warnings, or informational content during parsing or processing tasks. It supports operations for creating, formatting, and manipulating messages, including attaching locations and severity levels. Concrete use cases include generating diagnostic output in compilers or interpreters, such as syntax error messages with source positions.",
      "description_length": 427,
      "index": 162,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Extensions",
      "library": "dolmen.std",
      "description": "This module enables the creation and management of SMT-LIB 2 extensions, supporting the registration and activation of custom statement handlers. It centers around a type `t` that represents extensions by name and associated statements, offering operations to add, retrieve, and enable these extensions. With it, users can implement features like MaxSMT support or define new SMT-LIB commands that parse input into terms and construct corresponding statements. For example, an extension named \"maxsmt\" can introduce a custom `assert-soft` command that processes weighted assertions.",
      "description_length": 582,
      "index": 163,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Tag",
      "library": "dolmen.std",
      "description": "This module manages typed key-value associations using tags, enabling operations like setting, getting, and iterating over tagged values. It supports individual values and lists, with functions to add, update, or remove entries in a map. Use cases include tracking configuration options, accumulating values under typed keys, and managing state with type-safe bindings.",
      "description_length": 369,
      "index": 164,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Timer",
      "library": "dolmen.std",
      "description": "This module provides functions to measure elapsed time between a start and stop point. It works with a timer type `t` that represents a timing session. A concrete use case is benchmarking the execution time of a function or code block.",
      "description_length": 235,
      "index": 165,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Namespace",
      "library": "dolmen.std",
      "description": "This module defines a sum type for representing namespaces such as variables, sorts, terms, and literals like integers and bitvectors, with operations for equality, comparison, and formatted output. It supports key use cases like managing identifier scopes in SMT solvers and distinguishing term types in logical expressions. The associated map module enables structured storage and retrieval of values keyed by these namespaces, allowing for symbol table management and scoped configuration handling. Together, they facilitate parsing, evaluation, and solver workflows by combining precise namespace classification with efficient, typed mapping operations.",
      "description_length": 657,
      "index": 166,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Tok",
      "library": "dolmen.std",
      "description": "This module defines a structured representation of lexical tokens, including their kind, lexeme, article, and optional hint. It provides functions to construct and print token descriptions, primarily used for parsing and error reporting in formal language processing. Concrete use cases include generating detailed syntax error messages and representing parsed language constructs in a structured format.",
      "description_length": 404,
      "index": 167,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Loc",
      "library": "dolmen.std",
      "description": "This module handles source code location data through operations like tracking during parsing, converting between compact and full location representations, and comparing positions. It works with Lexing positions, file metadata, and structured types for locations (`loc` and `t`), supporting use cases such as error reporting with precise line/column information and managing location-aware data in compiler pipelines.",
      "description_length": 418,
      "index": 168,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Name",
      "library": "dolmen.std",
      "description": "This module represents structured names used in parsed files, supporting three forms: simple strings, indexed symbols with a base and list of indexes, and qualified names with a path and base. It provides hash, equality, comparison, and printing operations, along with creation functions for each variant, enabling precise manipulation of symbol references in formal languages. The included map module allows efficient storage and retrieval of values keyed by these structured names, supporting operations like add, find, iterate, and fold with both exception and option-based lookups. Together, they facilitate building symbol tables or environments where names carry structural and contextual meaning, such as in SMT-LIB or module systems.",
      "description_length": 741,
      "index": 169,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Maps",
      "library": "dolmen.std",
      "description": "This module provides map structures specialized for different key types, including integers, strings, and arbitrary ordered types. Each implementation supports core operations like insertion, lookup (with and without exceptions), iteration, and folding, tailored to their respective key domains. For example, integer-keyed maps can efficiently track dynamic state by ID, string-keyed maps can manage configuration settings, and the parameterized variant can handle symbol tables with custom-ordered keys like timestamps or complex identifiers.",
      "description_length": 543,
      "index": 170,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Builtin",
      "library": "dolmen.std",
      "description": "This module defines built-in constants, functions, and predicates used in typed expressions to represent semantically meaningful operations. It works with data types such as propositions, integers, rationals, reals, arrays, bitvectors, floating-point numbers, and strings, along with algebraic datatypes and regular expressions. Concrete use cases include expressing equality and arithmetic operations with precise semantics, handling type coercions, defining quantifiers, manipulating arrays and bitvectors, performing floating-point computations, and working with string and regular expression operations in formal verification tasks.",
      "description_length": 636,
      "index": 171,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Maps_string",
      "library": "dolmen.std",
      "description": "This module implements a map data structure specialized for string keys, supporting operations like insertion, lookup, iteration, and folding. It provides functions to add key-value pairs, retrieve values with or without exceptions, and traverse or transform the map's contents. Use cases include managing symbol tables, configuration settings, or any keyed data where efficient string-based lookups and updates are required.",
      "description_length": 425,
      "index": 172,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Misc",
      "library": "dolmen.std",
      "description": "This module combines lexicographic comparison logic, hash composition, and string/list manipulation utilities with input source normalization for parsing workflows. It operates on variant input representations (files, buffers), optional values, and collections, enabling use cases like parser infrastructure setup, error context preservation during input handling, and resource-efficient data transformation pipelines. Key operations include file extension extraction, option-aware mapping, and lexbuf abstraction to support structured data processing and pretty-printing tasks.",
      "description_length": 578,
      "index": 173,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Expr",
      "library": "dolmen.std",
      "description": "This module provides a comprehensive framework for constructing, manipulating, and analyzing typed expressions with support for algebraic data types, polymorphism, and symbolic computation. It includes core data types for expressions, types, substitutions, and identifiers, with operations for type-safe construction, deconstruction, and transformation of terms in formal verification and logic programming contexts. Submodules enable polymorphic type manipulation with unification, customizable pretty-printing of expressions, structured substitution management, first-order term construction with arithmetic and logical operations, metadata tagging for terms and identifiers, and fine-grained identifier handling with extensible metadata. Examples include modeling memory using arrays, performing type inference with annotated types, generating readable term representations, and encoding constraints over strings and integers for SMT solving.",
      "description_length": 945,
      "index": 174,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Id",
      "library": "dolmen.std",
      "description": "This module provides identifiers as a fundamental data type, supporting creation, comparison, and naming operations. It includes a child module for standard map operations, enabling efficient management of identifier-keyed data structures with functions for insertion, lookup, and iteration. These maps are ideal for implementing symbol tables, environments, or identifier-to-value associations in compilers and formal systems. Together, the module and its submodule allow precise handling of named entities and their associated data in structured contexts.",
      "description_length": 557,
      "index": 175,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Transformer",
      "library": "dolmen.std",
      "description": "This module transforms input sources into structured statements through a token stream, supporting incremental parsing and detailed error reporting. It processes input from files, standard input, or strings, producing a lazy list or iterator of parsed statements. The environment-based file resolution allows context-aware parsing, and custom error messages aid in debugging malformed input. For example, it can parse a script file into an AST for further processing or validate input from stdin in an interactive environment.",
      "description_length": 526,
      "index": 176,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Term",
      "library": "dolmen.std",
      "description": "This module provides a comprehensive framework for building and manipulating logical, arithmetic, and type-level expressions, centered around the `Term.t` data type. It supports core operations such as constructing propositional connectives, quantifiers, lambda abstractions, and language-specific features like let-bindings and pattern matching, enriched with location tracking and annotations for precise error reporting. The integrated identifier set module enables efficient management of unique identifiers, supporting operations like union, intersection, and ordered traversal over `Dolmen_std.Id.t`, which is essential for dependency resolution and symbol table management. Together, these components facilitate applications in formal verification, SMT solving, and parsing logical frameworks such as SMT-LIB and TPTP.",
      "description_length": 825,
      "index": 177,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Pretty",
      "library": "dolmen.std",
      "description": "This module defines types for specifying pretty-printing annotations, including name representations, operator positions, and associativity. It works with strings and variants to describe formatting rules for printed expressions. Concrete use cases include configuring how logical or mathematical expressions are formatted in output, such as specifying infix or prefix notation and associativity for operators.",
      "description_length": 410,
      "index": 178,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_std.Path",
      "library": "dolmen.std",
      "description": "This module represents and manipulates paths that identify variables and constants after typechecking. It supports operations to construct local and absolute paths, rename their base identifiers, and format them for output. It works with string lists for absolute paths and a private variant type for path values, primarily used in symbol resolution and module scoping contexts.",
      "description_length": 378,
      "index": 179,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std.Vec",
      "library": "dolmen.std",
      "description": "This module implements resizeable arrays with operations to create, modify, and access elements by index. It supports dynamic vectors of any type `'a`, allowing controlled growth and shrinkage of the underlying array. Use cases include building dynamic collections, managing buffers with variable-sized data, and implementing stacks or queues where elements are frequently added or removed.",
      "description_length": 390,
      "index": 180,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_std",
      "library": "dolmen.std",
      "description": "This module integrates tools for formal logic modeling, identifier management, and structured data processing, enabling precise handling of logical queries, theorem proving, and SMT-LIB extensions. Core data types include logical statements, identifiers, terms, and expressions, with operations to construct, transform, and analyze them, along with support for tracking statistics, managing source locations, and pretty-printing structured output. Users can define custom SMT-LIB commands, generate safe identifiers for code generation, track performance metrics, and parse input into logical expressions with detailed error reporting. Example workflows include building and validating logical assertions, profiling execution times, or extending SMT solvers with domain-specific constructs.",
      "description_length": 790,
      "index": 181,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_line",
      "library": "dolmen.line",
      "description": "Consumes all characters on the current line from a lexing buffer, using customizable newline and synchronization functions. Works directly with `Lexing.lexbuf` to handle input processing tasks. Useful for implementing parsers that require line-by-line analysis or preprocessing.",
      "description_length": 278,
      "index": 182,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Make.Parser",
      "library": "dolmen_smtlib2_poly",
      "description": "Parses SMT-LIB 2 input from a lexing buffer using a provided token function. It supports parsing entire files into a list of statements or incrementally parsing one statement at a time. Designed for processing SMT-LIB scripts in environments requiring precise syntactic analysis.",
      "description_length": 279,
      "index": 183,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Make.Lexer",
      "library": "dolmen_smtlib2_poly",
      "description": "This module defines the lexical analysis component for parsing SMT-LIB input. It provides functions to convert raw input into structured tokens and associate descriptive error information with each token. It operates on `lexbuf` input, producing a custom `token` type used in the parsing process.",
      "description_length": 296,
      "index": 184,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Print.L",
      "library": "dolmen_smtlib2_poly",
      "description": "This module processes SMT-LIB2 syntax into tokens and formats them for output, supporting operations like symbol conversion, token binding, and lexing control. It defines core types such as tokens, symbol mappings, and lexing states, enabling precise parsing and printing of SMT-LIB2 expressions. Submodules extend this functionality to handle specific token categories, such as reserved words and string literals, with dedicated utilities for validation and transformation. For example, it can parse an SMT-LIB2 assertion into a structured token stream or format a bound symbol back into its original syntax.",
      "description_length": 609,
      "index": 185,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Statement",
      "library": "dolmen_smtlib2_poly",
      "description": "The module provides operations for managing SMT solver interactions, including state transitions (reset, push/pop, exit), declaring types and functions, asserting constraints, and querying models, proofs, or unsat cores. It manipulates SMT-LIBv2 statements through a solver state type (`t`) that encapsulates terms, identifiers, and solver-specific data structures. This supports use cases like formal verification, constraint solving, and proof reconstruction in interactive theorem proving workflows.",
      "description_length": 502,
      "index": 186,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Id",
      "library": "dolmen_smtlib2_poly",
      "description": "This module handles the creation and manipulation of identifiers within specific namespaces, such as sorts, terms, and attributes. It provides functions to construct basic and indexed identifiers using strings and string lists. Useful for generating SMT-LIB compliant identifiers with proper\u547d\u540d\u7a7a\u95f4\u9694\u79bb and indexing.",
      "description_length": 311,
      "index": 187,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Extension",
      "library": "dolmen_smtlib2_poly",
      "description": "This module defines handlers for parsing and processing non-standard SMT-LIB statements, extending the language input with custom statement types. It works with abstract syntax tree nodes for terms and statements, along with location tracking data. Concrete use cases include adding support for custom SMT solver commands or domain-specific extensions to SMT-LIB scripts.",
      "description_length": 371,
      "index": 188,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Make",
      "library": "dolmen_smtlib2_poly",
      "description": "This module generates a parser for the SMT-LIB format, enabling lazy or incremental parsing of input from files, stdin, or strings, with support for error recovery. It processes token streams produced from lexing buffers, allowing full script evaluation or step-by-step statement parsing, and defines core types like `token` and `statement` for representing parsed elements. The included lexical analysis module converts raw input into structured tokens with error metadata, while the parsing module consumes these tokens to build SMT-LIB statements. Example uses include loading and evaluating SMT scripts from disk or implementing a REPL that parses and executes statements line by line.",
      "description_length": 689,
      "index": 189,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Term",
      "library": "dolmen_smtlib2_poly",
      "description": "This module constructs and manipulates SMT-LIB terms, supporting operations like constant creation, function application, quantification, pattern matching, and term annotations. It works with terms representing logical expressions, types, and S-expressions, along with identifiers and locations for source tracking. Concrete use cases include building and processing formulas for SMT solvers, handling term annotations, and managing local bindings and type parameters in SMT-LIB input.",
      "description_length": 485,
      "index": 190,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_poly.Print",
      "library": "dolmen_smtlib2_poly",
      "description": "This module handles the classification and formatting of characters and symbols according to SMT-LIB syntax, determining whether symbols are printable and in what form\u2014simple, quoted, or unprintable. It includes predicates for checking digits, letters, and valid symbol components, which are essential for constructing and validating identifiers. The child module processes SMT-LIB2 syntax into structured tokens, supporting operations like symbol conversion, token binding, and lexing control using types such as tokens, symbol mappings, and lexing states. Together, they enable tasks like parsing assertions into token streams, validating reserved words, and correctly formatting symbols for output.",
      "description_length": 701,
      "index": 191,
      "embedding_norm": 0.9999998807907104
    },
    {
      "module_path": "Dolmen_smtlib2_poly",
      "library": "dolmen_smtlib2_poly",
      "description": "This module parses and represents SMT-LIB2 input, handling identifiers, terms, statements, and extensions, while supporting concrete syntax for logical expressions, declarations, and commands used in SMT solvers. It provides core data types such as `token`, `statement`, and `term`, along with a solver state type `t` that encapsulates terms and identifiers for managing SMT solver interactions like asserting constraints, querying models, and handling state transitions. Child modules extend this functionality by enabling identifier management with namespace isolation, custom statement parsing, term construction with annotations, and incremental script parsing with error recovery. Example uses include building SMT benchmarks, integrating SMT input into verification tools, implementing REPLs, and extending SMT-LIB syntax with domain-specific commands.",
      "description_length": 858,
      "index": 192,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_icnf.Make.Lexer",
      "library": "dolmen.icnf",
      "description": "This module defines the lexical analysis component for parsing iCNF input. It provides two key operations: `descr`, which maps tokens to descriptive strings for error reporting, and `token`, which processes a lexing buffer to generate tokens. It operates on `token` values and `lexbuf` structures from the standard library, specifically handling the low-level parsing of iCNF syntax elements.",
      "description_length": 392,
      "index": 193,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_icnf.Make.Parser",
      "library": "dolmen.icnf",
      "description": "Parses iCNF language input into structured statements using lexer buffers. It provides functions to parse either an entire file into a list of statements or read statements incrementally from a stream. This module is used to process iCNF files or input streams statement by statement, supporting incremental parsing for interactive or streaming use cases.",
      "description_length": 355,
      "index": 194,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_icnf.Statement",
      "library": "dolmen.icnf",
      "description": "This module defines the structure of iCNF statements, including clauses and solve instructions. It provides functions to construct clauses from literal lists and generate solve commands with assumptions, using a specified term type. It is used to build and manipulate iCNF input files for logic solving tasks.",
      "description_length": 309,
      "index": 195,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_icnf.Term",
      "library": "dolmen.icnf",
      "description": "This module represents terms in the iCNF (incremental Conjunctive Normal Form) language, specifically handling atomic propositions and their negations. It provides the `atom` function to construct terms from integers, where positive values denote variables and negative values denote negated variables. This module is used to build logical expressions for input to SAT solvers or formal verification tools.",
      "description_length": 406,
      "index": 196,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_icnf.Make",
      "library": "dolmen.icnf",
      "description": "This module generates a parser for the iCNF format using provided language, term, and statement modules, supporting both lazy and incremental parsing from various input sources. It combines lexical analysis and structured statement parsing, exposing operations to process entire files or stream input line by line. Key data types include `token` and `lexbuf`, with functions like `descr` and `token` for lexing, and statement-level parsing for structured interpretation. Example uses include loading benchmark files or streaming large iCNF content without full in-memory loading.",
      "description_length": 579,
      "index": 197,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_icnf",
      "library": "dolmen.icnf",
      "description": "This module parses and processes the iCNF format for incremental satisfiability problems, defining term and statement structures that support clause addition, assumption handling, and solver interaction. It provides core types like clauses, solve commands, and terms\u2014constructed from literals and integers representing variables or their negations\u2014enabling manipulation of logical constraints and integration with SAT/SMT solvers. The parser submodule handles input from files or streams, supporting lazy and incremental processing of large benchmarks. Together, these components allow tasks like reading and modifying iCNF files, building logical expressions, and executing incremental solving workflows.",
      "description_length": 705,
      "index": 198,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_zf.Make.Lexer",
      "library": "dolmen.zf",
      "description": "This module defines the lexical analysis component for parsing the Zipperposition format. It provides two core operations: `descr`, which maps tokens to descriptive strings for error reporting, and `token`, which processes a lexing buffer to generate tokens. It works directly with `lexbuf` from OCaml's standard `Lexing` module and a custom `token` type defined in the enclosing module. A concrete use case is transforming raw input streams into structured tokens during the initial phase of parsing logical formulas in the Zipperposition format.",
      "description_length": 547,
      "index": 199,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_zf.Make.Parser",
      "library": "dolmen.zf",
      "description": "Parses Zipperposition format input into structured statements using lexer buffers. It provides `file` to parse an entire file into a list of statements and `input` to parse one statement at a time, returning `None` at end-of-file. Designed for reading and processing logical formulas and proofs from files or interactive sources.",
      "description_length": 329,
      "index": 200,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_zf.Id",
      "library": "dolmen.zf",
      "description": "This module represents identifiers within a namespace system, primarily used for terms and types in the Zipperposition format. It provides operations to create identifiers (`mk`) and access the term namespace (`term`). Concrete use cases include managing symbol resolution and ensuring uniqueness of identifiers across different scopes in formal logic expressions.",
      "description_length": 364,
      "index": 201,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_zf.Statement",
      "library": "dolmen.zf",
      "description": "This module defines core constructs for representing statements in the Zipperposition format, including declarations, definitions, goals, assumptions, and lemmas. It operates on types such as identifiers (`id`), terms (`term`), and locations (`location`), organizing them into structured statements (`t`). Concrete use cases include building logical assertions, defining functions, and specifying inductive data types within formal verification tasks.",
      "description_length": 451,
      "index": 202,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_zf.Make",
      "library": "dolmen.zf",
      "description": "This module generates a parser for the Zipperposition format by integrating lexical analysis, incremental parsing, and file resolution. It processes input from files, standard input, or raw strings into structured tokens and statements, supporting on-demand parsing and handling include directives. The lexer module converts character streams into tokens and provides descriptive strings for errors, while the parser module builds structured statements from those tokens, enabling both full-file and line-by-line processing. Example usage includes reading logic problem files, parsing individual expressions from interactive input, or resolving included files during parsing.",
      "description_length": 675,
      "index": 203,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_zf.Term",
      "library": "dolmen.zf",
      "description": "This module provides operations to build and transform terms representing logical and arithmetic expressions, supporting constants, integers, functions, quantifiers, lambda expressions, and logical connectives like implication or equivalence. It centers on an abstract type `t` that encapsulates terms with optional source-location tracking for error reporting, enabling precise diagnostics during term manipulation. These capabilities are tailored for applications in formal verification, symbolic computation, and theorem proving where structured term manipulation and logical reasoning are critical.",
      "description_length": 602,
      "index": 204,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_zf",
      "library": "dolmen.zf",
      "description": "This module handles parsing and manipulation of logical formulas in the Zipperposition input format, providing structured representations of terms and statements with support for construction, traversal, and transformation. It includes identifiers with namespaced management for unique symbol resolution, a parser that processes input from files or strings into structured statements, and utilities for building complex logical expressions with support for quantifiers, lambda expressions, and arithmetic operations. The module enables tasks such as reading and analyzing logic problem files, constructing formal assertions, and performing transformations on logical terms with precise error reporting. Submodules organize identifiers, statements, and parsing components to support formal verification, theorem proving, and symbolic computation workflows.",
      "description_length": 855,
      "index": 205,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_smtlib2_v6_response.Make.Lexer",
      "library": "dolmen_smtlib2_v6_response",
      "description": "This module defines a lexer for parsing SMT-LIB 2.6 input, providing functions to convert character streams into tokens and associate descriptive error messages with those tokens. It operates on `lexbuf` from OCaml's standard `Lexing` module and produces a custom `token` type specific to the SMT-LIB language. It is used to tokenize input for subsequent parsing stages in SMT solver interaction workflows.",
      "description_length": 406,
      "index": 206,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_response.Make.Parser",
      "library": "dolmen_smtlib2_v6_response",
      "description": "Parses SMT-LIB v2.6 input into structured statements using lexer functions. It processes lex buffers with token streams, producing either a complete list of statements from a file or individual statements incrementally. Useful for reading SMT solver responses or parsing SMT scripts interactively.",
      "description_length": 297,
      "index": 207,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_smtlib2_v6_response.Term",
      "library": "dolmen_smtlib2_v6_response",
      "description": "This module represents terms in the SMT-LIB language, supporting operations like constant creation, numeric and string literals, term application, quantification, pattern matching, and term annotations. It works with custom term types that encode SMT expressions, including identifiers, literals, and structured terms with locations. Concrete use cases include constructing and manipulating SMT formulas with let bindings, universal and existential quantifiers, and annotated expressions for solver interaction.",
      "description_length": 511,
      "index": 208,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_response.Id",
      "library": "dolmen_smtlib2_v6_response",
      "description": "This module handles the creation and manipulation of identifiers in the SMT-LIB language, specifically supporting operations to construct identifiers with namespaces and optional indexed suffixes. It works with string-based names and predefined namespace types for sorts, terms, and attributes. Concrete use cases include generating valid SMT-LIB identifiers for terms and types during input parsing or code generation.",
      "description_length": 419,
      "index": 209,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_response.Make",
      "library": "dolmen_smtlib2_v6_response",
      "description": "This module generates a parser for the Smtlib format, supporting both incremental and full parsing of string-based input sources like files and standard input. It produces structured statements according to the SMT-LIB v2.6 specification, handling include directives and recovering from syntax errors. The lexer submodule converts character streams into tokens, associating detailed error messages with parsing steps, while the parser submodule processes these tokens into statements, enabling interactive script parsing or processing solver responses. Example uses include reading SMT scripts from disk, handling input streams with embedded includes, and parsing solver outputs incrementally.",
      "description_length": 693,
      "index": 210,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_response.Statement",
      "library": "dolmen_smtlib2_v6_response",
      "description": "This module defines operations for constructing SMT-LIB 2.6 response statements, including function definitions, recursive function declarations, and response outcomes like SAT, UNSAT, and ERROR. It works with identifiers, terms, locations, and definition lists to build structured SMT solver responses. Concrete use cases include generating function definitions with argument and return types, declaring recursive functions, and producing solver answers with optional models or error messages.",
      "description_length": 494,
      "index": 211,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_response",
      "library": "dolmen_smtlib2_v6_response",
      "description": "This module processes SMT-LIBv2 responses, providing interfaces for identifiers, terms, and statements that align with the SMT-LIBv2 standard. It enables parsing and constructing solver responses such as SAT, UNSAT, or ERROR, along with function definitions and recursive declarations, using identifiers with namespaces and structured terms that include literals, quantifiers, and annotations. The integrated parser submodule handles input from files or streams, supporting includes and error recovery, while the identifier and term submodules facilitate building and manipulating SMT expressions with let bindings, patterns, and locations. Example uses include integrating SMT solvers into OCaml tools, generating SMT scripts, and processing formal verification feedback with precise error handling and term manipulation.",
      "description_length": 822,
      "index": 212,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_tptp",
      "library": "dolmen.tptp",
      "description": "Handles parsing and pretty-printing of TPTP (Thousands of Problems for Theorem Provers) format versions, specifically supporting the latest version and version 6.3.0. Works with abstract syntax trees representing logical formulas and problem statements. Useful for interfacing with automated theorem provers that consume or produce TPTP files.",
      "description_length": 343,
      "index": 213,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Logic.Make.S-Parser",
      "library": "dolmen.class",
      "description": "Parses logic language files into statement lists or individual statements using lexer buffers. It handles token streams generated by a lexer, producing abstract syntax trees for formal proof processing. Use for reading proof scripts from files or evaluating input incrementally in a REPL.",
      "description_length": 288,
      "index": 214,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_class.Logic.Make.S",
      "library": "dolmen.class",
      "description": "This module defines the structure for implementing formal logic languages, including token types and parsing components. It provides functions for locating files, parsing inputs entirely or incrementally, and handling errors during parsing. It is used to process logic statements from files, standard input, or string contents, particularly in proof systems where resiliency to syntax errors and efficient handling of large inputs are required.",
      "description_length": 444,
      "index": 215,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Response.Make.S",
      "library": "dolmen.class",
      "description": "This module defines a language specification for parsing formal proof inputs, including token definitions, a lexer, and a parser. It supports file resolution with directory context, full parsing of inputs into lazy lists of statements, and incremental parsing for interactive or streaming use cases. Concrete uses include processing TPTP files, handling include directives relative to source files, and parsing large or stdin-provided proof scripts with error recovery.",
      "description_length": 469,
      "index": 216,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Response.Make.S-Parser",
      "library": "dolmen.class",
      "description": "Parses formal proof statements from a lexing buffer using a provided tokenization function. It supports parsing entire files into a list of statements or incrementally parsing single statements. This module is used to process input streams in a response language for theorem proving tools.",
      "description_length": 289,
      "index": 217,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Response.Make.S-Lexer",
      "library": "dolmen.class",
      "description": "This module defines a lexer for parsing formal proof languages by converting input from a `lexbuf` into tokens. It provides functions to associate descriptive error messages with tokens and to generate tokens from a lexing buffer. It is used to implement low-level syntax recognition in proof language parsers.",
      "description_length": 310,
      "index": 218,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Logic.Make.S-Lexer",
      "library": "dolmen.class",
      "description": "This module defines tokenization logic for parsing logical expressions, mapping character streams to structured tokens. It operates on `lexbuf` input, producing `token` values enriched with descriptive metadata for error handling. Used to implement low-level syntax recognition in formal proof systems.",
      "description_length": 302,
      "index": 219,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_class.Response.S-module-type-S-Lexer",
      "library": "dolmen.class",
      "description": "This module defines tokenization logic for parsing formal proof languages. It includes functions to generate tokens from a lexing buffer and associate descriptive error messages with tokens. It is used to implement low-level lexical analysis in proof language parsers.",
      "description_length": 268,
      "index": 220,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Logic.S-module-type-S",
      "library": "dolmen.class",
      "description": "This module defines the structure for language implementations used in formal proof systems, specifying core components like token types, lexer and parser modules, and file resolution logic. It provides functions for locating files based on language rules and parsing input sources either fully or incrementally, supporting precise handling of syntax errors during input processing. Concrete use cases include parsing proof scripts from files or standard input, and resolving included files relative to the current source directory.",
      "description_length": 532,
      "index": 221,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Response.S",
      "library": "dolmen.class",
      "description": "This module handles parsing and response generation for formal proof languages, supporting operations like file resolution, full and incremental parsing of input sources, and language-specific processing. It works with data types representing languages (e.g., Smtlib2), files, and parsed statements. Concrete use cases include parsing SMT solver responses from files or standard input, detecting language based on file extension, and lazily processing large statement lists.",
      "description_length": 474,
      "index": 222,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Response.Make",
      "library": "dolmen.class",
      "description": "This module orchestrates language-specific response parsing for formal proof formats, integrating language detection, abstract syntax tree manipulation, and statement stream processing. It coordinates with its submodules to support full or incremental parsing of inputs from files or streams, using lexers and parsers tailored to formats like SMT-LIB v2 and TPTP. Main data types include lex buffers, token streams, and lazy lists of statements, with operations for file resolution, error recovery, and interactive parsing. Examples include validating proof scripts from stdin, parsing large TPTP files with include directives, and streaming proof responses in theorem proving tools.",
      "description_length": 683,
      "index": 223,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Logic.Make",
      "library": "dolmen.class",
      "description": "This module processes formal logic formats like Smtlib2, TPTP, and Dimacs, parsing files either fully or incrementally into structured representations for theorem proving and model checking. It identifies input languages, drives lexing and parsing workflows, and supports error-resilient processing of large logic files. The core functionality works with `lexbuf` streams and `token` values defined in its submodules, enabling precise syntax recognition and detailed error reporting during input evaluation. Use it to load and analyze logic problems from disk, implement REPLs for proof scripts, or validate syntax in formal verification pipelines.",
      "description_length": 648,
      "index": 224,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_class.Response.S-module-type-S",
      "library": "dolmen.class",
      "description": "This module defines the interface for language implementations used in formal proof systems, specifying core components like token types, lexer and parser modules, and functions for locating files and parsing input. It operates on file paths, string contents, and lazy or incremental streams of parsed statements, supporting both full and line-by-line parsing. Concrete use cases include processing proof scripts from files or standard input, handling include directives with relative paths, and recovering from syntax errors during interactive or streaming parsing.",
      "description_length": 566,
      "index": 225,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_class.Logic.S-module-type-S-Parser",
      "library": "dolmen.class",
      "description": "Parses input using a lexer to produce statements from a formal logic language. It processes lex buffers with user-defined token parsers, returning either a list of statements from a complete file or an optional single statement for incremental parsing. Useful for reading logic formulas or proof scripts from files or interactive inputs.",
      "description_length": 337,
      "index": 226,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Response.S-module-type-S-Parser",
      "library": "dolmen.class",
      "description": "Parses input files or streams into lists of statements using a provided lexer function. It handles lexical buffers and token generation, producing structured data for further processing. Useful for reading formal proof scripts or configuration files incrementally or in bulk.",
      "description_length": 275,
      "index": 227,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Logic.S-module-type-S-Lexer",
      "library": "dolmen.class",
      "description": "This module defines token description and parsing functions for a logic language lexer. It processes input using `lexbuf` to generate tokens and provides descriptive error messages via token metadata. Used to tokenize logical expressions and language-specific constructs during formal proof parsing.",
      "description_length": 299,
      "index": 228,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_class.Logic.S",
      "library": "dolmen.class",
      "description": "This module handles parsing and language detection for formal proof formats like SMT-LIB, TPTP, and DIMACS. It provides functions to locate files, parse their contents either lazily or incrementally, and map file extensions or names to corresponding language modules. Concrete use cases include loading and processing proof files in automated theorem proving tools.",
      "description_length": 365,
      "index": 229,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Logic",
      "library": "dolmen.class",
      "description": "This module provides foundational operations for building and manipulating logical expressions in formal proof systems, supporting data types such as terms, formulas, and proof rules in first-order logic. It enables tasks like encoding logical assertions, performing proof transformations, and implementing theorem provers, while integrating language processing tools for parsing and lexing logic inputs. Submodules define language structures, tokenization, and parsing workflows that support formats like Smtlib2, TPTP, and Dimacs, allowing precise syntax recognition and error handling. Use it to load logic problems from files, implement interactive proof script parsers, or validate formal proofs with custom language extensions.",
      "description_length": 733,
      "index": 230,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class.Response",
      "library": "dolmen.class",
      "description": "This module encodes and decodes proof-related data, working with abstract syntax trees and term representations to handle responses from theorem provers and SMT solvers. It coordinates low-level tokenization, parsing, and language-specific processing through submodules that manage lexical analysis, file resolution, error recovery, and lazy statement streams. Key data types include lex buffers, token streams, and parsed statements, supporting operations like parsing SMT-LIB or TPTP files, validating proof scripts from stdin, and handling include directives with relative paths. The module enables both full and incremental parsing, integrating language detection and interactive parsing workflows.",
      "description_length": 702,
      "index": 231,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_class",
      "library": "dolmen.class",
      "description": "This module provides core infrastructure for processing logical expressions and proof data in formal verification systems. It defines key data types such as terms, formulas, proof rules, lex buffers, token streams, and parsed statements, supporting operations like parsing, validation, transformation, and interactive processing of logic inputs. It enables tasks such as loading and validating SMT-LIB or TPTP files, implementing custom proof script parsers, handling include directives with relative paths, and integrating language extensions with precise syntax recognition. Use it to build theorem provers, decode solver responses, or manage incremental parsing workflows with error recovery and lazy evaluation.",
      "description_length": 715,
      "index": 232,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2.Response",
      "library": "dolmen.smtlib2",
      "description": "This module parses and handles responses from SMT solvers according to the SMT-LIB 2.6 standard or the latest available version. It supports operations to interpret solver output, extract result statuses, and retrieve model values or unsat cores. Concrete use cases include integrating with SMT solvers for formal verification tasks and processing SMT query results in automated reasoning pipelines.",
      "description_length": 399,
      "index": 233,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2.Script",
      "library": "dolmen.smtlib2",
      "description": "Handles parsing and evaluation of SMT-LIBv2 script files, supporting version-specific syntax and commands. Works with abstract syntax trees representing SMT-LIBv2 expressions and scripts. Useful for integrating SMT solver interactions directly from OCaml, enabling automated verification tasks with precise version control over SMT-LIB dialects.",
      "description_length": 345,
      "index": 234,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_smtlib2",
      "library": "dolmen.smtlib2",
      "description": "This module processes SMT-LIBv2 scripts and interacts with SMT solvers, handling both script evaluation and response interpretation according to the SMT-LIB 2.6 standard. It provides data types for SMT-LIBv2 abstract syntax trees, solver commands, and response values, along with operations to check satisfiability, extract models, and parse solver outputs. The module supports concrete tasks such as running SMT scripts from OCaml, querying solver results, and retrieving unsat cores or model assignments. Its submodules focus on response parsing and script evaluation, enabling precise control over solver interaction and version-specific SMT-LIB processing.",
      "description_length": 660,
      "index": 235,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_ae.Make.Parser",
      "library": "dolmen.ae",
      "description": "Parses Alt-ergo input files or individual statements from a lex buffer using a provided token function. It operates on `lexbuf` and produces either a list of statements for a complete file or an optional statement for incremental parsing. This module is used to process Alt-ergo's native format, enabling both full-file and step-by-step parsing workflows.",
      "description_length": 355,
      "index": 236,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_ae.Make.Lexer",
      "library": "dolmen.ae",
      "description": "This module defines a lexer for parsing Alt-ergo input by converting character streams into tokens. It includes functions to generate tokens from a lexing buffer and to associate descriptive error messages with each token. It is used to process raw input files or strings into structured tokens for further parsing and analysis.",
      "description_length": 328,
      "index": 237,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_ae.Id",
      "library": "dolmen.ae",
      "description": "This module represents and manipulates identifiers within different namespaces such as variables, terms, sorts, and declarations. It provides functions to construct identifiers with or without tracking information, supporting precise identification of terms and sub-terms in input files. Concrete use cases include generating unique term identifiers and managing type variables or top-level declarations in the Alt-ergo input language.",
      "description_length": 435,
      "index": 238,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_ae.Make",
      "library": "dolmen.ae",
      "description": "This module generates a parser for the native Alt-ergo input format, supporting full and incremental parsing from sources like files, stdin, or strings. It processes input lazily or step-by-step using token streams, recovers from errors by resynchronizing at line boundaries, and resolves included files via a helper function. The main data types include `lexbuf` for input buffering and structured statements produced during parsing. Example usage includes parsing an entire file into a list of statements or incrementally reading one statement at a time from a stream.",
      "description_length": 570,
      "index": 239,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_ae.Statement",
      "library": "dolmen.ae",
      "description": "This module represents statements in the Alt-ergo input language, including function and predicate definitions, algebraic and abstract type declarations, axioms, case splits, theories, rewriting rules, and goal or check-sat statements. It operates on identifiers, terms, and locations to construct structured logical and computational elements. Concrete use cases include defining logic functions, declaring algebraic data types, specifying axioms for verification, and encoding proof goals for automated reasoning.",
      "description_length": 515,
      "index": 240,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_ae.Term",
      "library": "dolmen.ae",
      "description": "This module supports construction and manipulation of logical and arithmetic expressions with first-class terms, enabling propositional operations (negation, conjunction, implication), arithmetic computations (addition, division, exponentiation), and bitvector manipulations (extraction, concatenation). It operates on a polymorphic term type `t` embedded with identifiers, built-in types (integers, reals, bitvectors), and structured data like quantifiers, conditionals, and algebraic data types. Designed for formal verification tasks, it facilitates SMT solving by allowing term annotations (tracking, quantifier triggers) and symbolic reasoning over complex expressions in program analysis or hardware verification workflows.",
      "description_length": 729,
      "index": 241,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_ae",
      "library": "dolmen.ae",
      "description": "This module provides the foundation for parsing and modeling Alt-ergo input files, integrating identifier handling, term construction, statement representation, and full parsing capabilities. Core data types include identifiers for tracking variables and declarations, structured statements for logical and computational definitions, and polymorphic terms supporting logical, arithmetic, and bitvector operations. It enables tasks such as parsing a script into executable statements, building and annotating logical expressions for SMT solving, and managing type declarations and proof goals within formal verification workflows. Key operations span identifier resolution, incremental parsing with error recovery, term manipulation with quantifiers and conditionals, and structured statement construction for axioms, functions, and theories.",
      "description_length": 841,
      "index": 242,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_dimacs.Make.Parser",
      "library": "dolmen.dimacs",
      "description": "Parses DIMACS input files into lists of logical statements using lexer buffers. It defines functions to parse entire files or individual statements incrementally, returning structured data representing the clauses and declarations in the DIMACS format. Useful for reading SAT problem instances from files or streams in a streaming fashion.",
      "description_length": 339,
      "index": 243,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_dimacs.Make.Lexer",
      "library": "dolmen.dimacs",
      "description": "This module defines a lexer for parsing DIMACS input files. It provides functions to generate tokens from a lexing buffer and associate descriptive error messages with each token. It is used to tokenize input streams for further parsing by the generated DIMACS parser.",
      "description_length": 268,
      "index": 244,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_dimacs.Make",
      "library": "dolmen.dimacs",
      "description": "This module generates a DIMACS parser that supports both incremental and full parsing of logical statements from files, strings, or standard input. It processes input via a lexer that produces tokens with error metadata, enabling robust handling of malformed lines and lazy parsing of large files. The parser works with structured clause data, allowing clients to read SAT problem instances and recover from syntax errors by resuming on the next line. Together with its lexer and statement modules, it provides a complete pipeline for DIMACS input processing.",
      "description_length": 559,
      "index": 245,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_dimacs.Statement",
      "library": "dolmen.dimacs",
      "description": "Implements the core operations for constructing Dimacs format statements, including creating CNF headers and clauses. It works with literals represented as terms and organizes them into clauses with optional location tracking. Used to generate valid Dimacs input files for SAT solvers by encoding logical formulas in conjunctive normal form.",
      "description_length": 341,
      "index": 246,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_dimacs.Term",
      "library": "dolmen.dimacs",
      "description": "This module represents logical terms in the DIMACS format, providing operations to construct and manipulate propositional formulas. It works with variables and literals encoded as integers, where positive values denote variables and negative values denote their negation. Concrete use cases include parsing and generating clauses for SAT solvers, and building logical expressions from integer-based literals.",
      "description_length": 408,
      "index": 247,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_dimacs",
      "library": "dolmen.dimacs",
      "description": "This module processes DIMACS input for SAT and SMT problems, parsing logical statements into structured clauses and terms while supporting error recovery and incremental reading from various input sources. It defines literals as signed integers, organizes them into clauses, and constructs CNF formulas for solver interaction, enabling both parsing and generation of DIMACS files. The lexer and parser pipeline handles malformed input with detailed error metadata, and the term module supports building and manipulating propositional expressions. Use cases include reading benchmark files, encoding logical constraints, and interfacing with automated reasoning tools.",
      "description_length": 667,
      "index": 248,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Make.Parser",
      "library": "dolmen_smtlib2_v6_script",
      "description": "Parses SMT-LIB v2.6 input into abstract syntax trees using lexer functions. It processes lexbuf streams to produce lists of statements or individual statements incrementally. Useful for reading SMT solver scripts from files or interactive inputs.",
      "description_length": 246,
      "index": 249,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Make.Lexer",
      "library": "dolmen_smtlib2_v6_script",
      "description": "This module defines the lexical analysis component for parsing SMT-LIBv2 input. It provides functions to convert character streams into structured tokens and associate descriptive error information with those tokens. It operates on `lexbuf` input, producing `token` values enriched with diagnostic metadata, specifically tailored for parsing SMT-LIBv2 scripts.",
      "description_length": 360,
      "index": 250,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Extension",
      "library": "dolmen_smtlib2_v6_script",
      "description": "This module defines extension points for handling custom statements in the Smtlib format. It provides a type for locations, terms, and statements, along with a function to process unrecognized statement forms. It is used to integrate domain-specific extensions into Smtlib parsers and interpreters.",
      "description_length": 298,
      "index": 251,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Term",
      "library": "dolmen_smtlib2_v6_script",
      "description": "This module constructs and manipulates SMT-LIB terms including constants, literals, applications, quantifiers, and annotations. It supports concrete operations like creating integer, real, and string literals, building function applications, defining quantified variables, and attaching attributes to terms. Use cases include parsing and generating SMT-LIB scripts with precise term representations and annotations for theorem proving or constraint solving.",
      "description_length": 457,
      "index": 252,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Print",
      "library": "dolmen_smtlib2_v6_script",
      "description": "Prints SMT-LIB identifiers to a formatter, automatically quoting them when necessary. Works directly with `Dolmen_std.Name.t` identifiers. Useful for generating valid SMT-LIB output from parsed identifier values.",
      "description_length": 212,
      "index": 253,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Make",
      "library": "dolmen_smtlib2_v6_script",
      "description": "This module generates a parser for the SMT-LIB format, supporting both incremental and full parsing of input sources such as files, strings, or standard input. It processes character streams into token streams using a lexer, then parses those tokens into abstract syntax trees representing SMT-LIB statements. Key data types include `lexbuf` for input buffering, `token` for lexical elements with error metadata, and statement lists for structured script representation. You can use it to read and evaluate SMT-LIB scripts interactively, handle syntax errors gracefully by resuming on the next line, or lazily process large input files statement by statement.",
      "description_length": 659,
      "index": 254,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Statement",
      "library": "dolmen_smtlib2_v6_script",
      "description": "This module provides operations for building and executing SMT-LIBv2 scripts to interact with theorem provers, supporting commands to control solver state, define logical theories, and query results. It works with terms, types, and identifiers in a located context, enabling first-order logic assertions, inductive type definitions, and solver-specific queries like model extraction or unsat core generation. Specific use cases include formal verification workflows where SMT solvers are used to check satisfiability, validate logical constraints, or generate proofs for complex systems.",
      "description_length": 587,
      "index": 255,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script.Id",
      "library": "dolmen_smtlib2_v6_script",
      "description": "This module handles the creation and manipulation of identifiers within specific namespaces, such as sorts, terms, and attributes. It provides functions to construct basic and indexed identifiers using string names and namespace tags. Use cases include building well-scoped SMT-LIB identifiers for terms and types with optional indexed suffixes.",
      "description_length": 345,
      "index": 256,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dolmen_smtlib2_v6_script",
      "library": "dolmen_smtlib2_v6_script",
      "description": "This module parses and processes SMT-LIB2 input scripts, supporting the definition and manipulation of identifiers, terms, and statements according to the SMT-LIB2 standard. It enables the construction of SMT solvers and formal verification tools by providing abstract syntax trees for logical expressions and commands, along with a parser that handles input from files or streams and supports error recovery and incremental processing. The `Term` module builds and annotates SMT-LIB expressions like quantifiers and function applications, while the `Print` submodule serializes identifiers and terms back into valid SMT-LIB text. You can use it to parse scripts, manipulate logical formulas with precise term representations, and generate solver commands for constraint solving or theorem proving workflows.",
      "description_length": 808,
      "index": 257,
      "embedding_norm": 1.0
    }
  ],
  "filtering": {
    "total_modules_in_package": 263,
    "meaningful_modules": 258,
    "filtered_empty_modules": 5,
    "retention_rate": 0.9809885931558935
  },
  "statistics": {
    "max_description_length": 1007,
    "min_description_length": 212,
    "avg_description_length": 479.3062015503876,
    "embedding_file_size_mb": 0.9376420974731445
  }
}