{
  "package": "lib_parsing",
  "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
  "embedding_dimension": 1024,
  "total_modules": 9,
  "creation_timestamp": "2025-07-15T23:05:57.358738",
  "modules": [
    {
      "module_path": "Ast_fuzzy",
      "library": "lib_parsing",
      "description": "This module defines a simplified abstract syntax tree (AST) structure for representing and manipulating code fragments with placeholders and ellipses. It includes operations for identifying metavariables and supports tree structures enclosed in various delimiters like parentheses, braces, and angle brackets. Concrete use cases include parsing and matching code patterns in refactoring tools or linters.",
      "description_length": 404,
      "index": 0,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Loc",
      "library": "lib_parsing",
      "description": "This module handles location tracking by combining start and end tokens into a single location range. It supports operations like creating locations from token lists, merging ranges, adjusting start/end tokens, and checking if a location is fake. It is used to manage source code positions for parsing and error reporting, enabling precise location-based operations on tokens and lists of elements with associated locations.",
      "description_length": 424,
      "index": 1,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_ast_fuzzy",
      "library": "lib_parsing",
      "description": "This module constructs and manipulates abstract syntax trees and token lists from a list of tokens using customizable hooks. It provides functions to build trees and tokens, traverse and modify tree structures with visitors, and extract or abstract positional information from trees. Concrete use cases include parsing and analyzing source code with flexible syntactic representations, such as handling incomplete or ambiguous input in fuzzy parsing scenarios.",
      "description_length": 460,
      "index": 2,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_parse_info",
      "library": "lib_parsing",
      "description": "This module converts parse information into OCaml values, supporting adjustable precision for dumping through flags that control inclusion of full, token, and type info. It works with `Parse_info.t` and produces `OCaml.v`, using a `dumper_precision` record to configure detail levels. Concrete use cases include command-line tools that require customizable output of parsed data structures with varying levels of detail.",
      "description_length": 420,
      "index": 3,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Flag_parsing",
      "library": "lib_parsing",
      "description": "This module manages parsing and lexing behavior through mutable flags that control verbosity, error handling, and debugging. It provides functions to configure command-line options for controlling these behaviors and a conditional execution function for sgrep-specific logic. It is used to fine-tune parser diagnostics and error recovery during input processing tasks.",
      "description_length": 368,
      "index": 4,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_info",
      "library": "lib_parsing",
      "description": "This module provides operations for managing token metadata with precise source code location tracking, including origin tracing, positional comparison, and error handling during parsing. It works with token-related data structures like `token_location` and `Parse_info.t` to enable tasks such as combining tokens, generating synthetic positions, and extracting positional boundaries for syntactic analysis. Key use cases include parser implementation, lexical error recovery, and tools requiring granular source code provenance tracking, such as linters or code formatters.",
      "description_length": 574,
      "index": 5,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parsing_helpers",
      "library": "lib_parsing",
      "description": "This module handles tokenization and position adjustment for parsing, providing functions to create token streams, adjust token locations relative to a base position, and generate error messages with precise file and line information. It works with lex buffers, token lists, and token location data structures to support parsing workflows that require accurate source position tracking. Concrete use cases include fixing token positions after reading from temporary files and generating lexers compatible with Yacc-style parsers.",
      "description_length": 529,
      "index": 6,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Skip_code",
      "library": "lib_parsing",
      "description": "This module handles file and directory skipping logic during processing. It provides functions to load skip rules from a file, filter and split file lists based on those rules, and reorder files to prioritize error-free directories. It works directly with file and directory names using the `skip` type to represent different skip conditions. Use this module when selectively processing files while excluding specific directories or files based on predefined rules.",
      "description_length": 465,
      "index": 7,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_stat",
      "library": "lib_parsing",
      "description": "Tracks parsing outcomes for source files, including counts of nodes, errors, timeouts, and problematic lines. Aggregates statistics across multiple files and identifies recurring parsing issues. Useful for analyzing parser performance on large codebases and debugging common syntax errors.",
      "description_length": 289,
      "index": 8,
      "embedding_norm": 1.0
    }
  ],
  "filtering": {
    "total_modules_in_package": 10,
    "meaningful_modules": 9,
    "filtered_empty_modules": 1,
    "retention_rate": 0.9
  },
  "statistics": {
    "max_description_length": 574,
    "min_description_length": 289,
    "avg_description_length": 437.0,
    "embedding_file_size_mb": 0.03311729431152344
  }
}