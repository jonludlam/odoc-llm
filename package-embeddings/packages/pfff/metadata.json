{
  "package": "pfff",
  "embedding_model": "Qwen/Qwen3-Embedding-8B",
  "embedding_dimension": 4096,
  "total_modules": 271,
  "creation_timestamp": "2025-08-18T19:19:19.072178",
  "modules": [
    {
      "module_path": "ANSITerminal",
      "library": "commons_core",
      "description": "This module enables text styling, cursor positioning, and screen manipulation through ANSI escape sequences, supporting dynamic terminal output formatting. It operates on `style` values for foreground/background colors and attributes, as well as terminal output streams for layout control. Typical applications include creating colored logs, progress bars, or interactive text-based interfaces with precise visual alignment.",
      "description_length": 424,
      "index": 0,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Int",
      "library": "commons_core",
      "description": "This module defines a type alias `t` for `int` and provides a `compare` function that orders integers by subtraction. It works directly with integer values. Use this module for consistent integer comparison in sorting or data structure operations.",
      "description_length": 247,
      "index": 1,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Math",
      "library": "commons_core",
      "description": "Contains mathematical constants like `pi` for use in calculations involving geometry, trigonometry, or physics. Directly provides precise values needed for numerical computations. Useful in scenarios requiring standard mathematical values without redefinition.",
      "description_length": 260,
      "index": 2,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parallel",
      "library": "commons_core",
      "description": "This module provides parallel execution of functions across multiple cores or threads. It supports mapping functions over lists in parallel, managing job batches, and handling exceptions with optional backtraces. Concrete use cases include accelerating CPU-bound computations like data transformations, batch processing, and parallel search algorithms.",
      "description_length": 352,
      "index": 3,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Console",
      "library": "commons_core",
      "description": "Handles progress reporting for list operations and long-running tasks. Provides functions to wrap list iterations with progress updates, execute functions with progress display, and return results while showing progress. Useful for command-line tools processing large datasets or performing batch operations where visual feedback is needed.",
      "description_length": 340,
      "index": 4,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Logger",
      "library": "commons_core",
      "description": "This module performs logging operations with a `log` function that takes an optional log level, a message, and an optional output destination. It handles string-based log messages and supports conditional logging based on level and output target. Use it to record application events to standard output, a file, or another destination.",
      "description_length": 334,
      "index": 5,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Common2.IntMap",
      "library": "commons",
      "description": "This module implements a map data structure with integer keys, supporting operations like insertion, lookup, deletion, and iteration. It provides functions for transforming and comparing maps, including `add`, `find`, `remove`, `map`, `fold`, and `iter`. Use it to efficiently manage key-value associations where keys are integers, such as tracking counters, indexing data, or representing sparse arrays.",
      "description_length": 404,
      "index": 6,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Common2.BasicType",
      "library": "commons",
      "description": "Defines common data types used throughout the codebase, including type aliases like `filename` for string. Provides basic type definitions that simplify handling of file paths and similar string-based data. Useful for ensuring consistency in functions that read from or write to files.",
      "description_length": 285,
      "index": 7,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Common2.IntIntMap",
      "library": "commons",
      "description": "This module implements a map with integer pairs as keys and arbitrary values, supporting standard operations like insertion, lookup, iteration, and transformation. It provides functions for adding or removing key-value pairs, checking membership, applying functions over keys and values, and comparing or folding over the map's contents. Use cases include tracking 2D grid positions with associated data, managing sparse matrices, or handling coordinate-based configurations.",
      "description_length": 475,
      "index": 8,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Common2.ArithFloatInfix",
      "library": "commons",
      "description": "This module defines standard arithmetic operations for both floating-point and integer types, including addition, subtraction, multiplication, and division. It also provides in-place increment operations for floating-point references. These functions are useful for numerical computations where precise type handling and in-place updates are required.",
      "description_length": 351,
      "index": 9,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Common2.Infix",
      "library": "commons",
      "description": "This module defines three infix operators for function application and string matching. It supports operations like applying a function to a value using `|>`, checking if a string matches a regular expression pattern with `=~`, and testing a string against a compiled regular expression using `==~`. These operators streamline data processing tasks such as parsing input, filtering strings, and chaining transformations.",
      "description_length": 420,
      "index": 10,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Common2.StringSet",
      "library": "commons",
      "description": "This module implements a sorted set structure for strings, supporting operations like union, intersection, difference, and comparison, along with filtering, partitioning, and splitting based on ordered traversal. It provides utilities for converting to and from lists, counting elements, and selecting arbitrary members, ideal for tasks requiring ordered string collections with efficient membership checks or set algebra. Use cases include managing unique identifiers, processing sorted string sequences, and performing data aggregation with set-based logic.",
      "description_length": 559,
      "index": 11,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Map_",
      "library": "commons",
      "description": "This module implements purely applicative association tables using balanced binary trees, supporting efficient insertion, lookup, and traversal operations. It works with ordered key types and associated values, enabling operations like `add`, `find`, `remove`, and `fold` with logarithmic time complexity. Concrete use cases include maintaining symbol tables during compilation, tracking configuration settings with ordered keys, and efficiently aggregating or transforming key-value data in functional pipelines.",
      "description_length": 513,
      "index": 12,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Ocaml",
      "library": "commons",
      "description": "This module supports structured data handling through type registration, value construction, and deconstruction operations, working with primitives, tuples, sums, lists, options, and references. It enables polymorphic transformations via `map_of_*` functions and side-effect-driven traversal with `v_*` functions, primarily targeting custom algebraic types like `either` and `either3`. Use cases include converting OCaml values to typed representations, applying uniform processing to variant or boxed data, and inspecting complex data structures through visitor patterns.",
      "description_length": 572,
      "index": 13,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Set_",
      "library": "commons",
      "description": "This module supports operations for creating, querying, and transforming sorted collections of unique elements, using a balanced binary tree implementation. It provides functions for membership checks, ordered insertion and deletion, set algebra (union, intersection), and structural manipulations like splitting or extracting min/max values, all while maintaining elements in sorted order according to a given comparison function. Typical use cases include efficiently managing dynamic collections with frequent lookups, maintaining ordered indices, or performing set-theoretic operations on data requiring strict element ordering.",
      "description_length": 632,
      "index": 14,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dumper",
      "library": "commons",
      "description": "Converts OCaml values of any type into their string representations, primarily for debugging or logging. Uses the `Sexplib` or similar serialization internally to generate human-readable output. Useful when inspecting complex or nested data structures during development.",
      "description_length": 271,
      "index": 15,
      "embedding_norm": 1.0
    },
    {
      "module_path": "OUnit",
      "library": "commons",
      "description": "This library provides assertion functions like `assert_equal` and `assert_raises` to validate values and exceptions, alongside utilities to construct and manipulate hierarchical test suites using types such as `TestCase`, `TestList`, and `TestLabel`. It supports organizing tests with filtering, labeling, and decoration operations, while enabling customizable execution through event handlers or text-based runners to capture results. Use cases include building structured unit tests, modifying test workflows programmatically, and integrating with custom reporting systems.",
      "description_length": 575,
      "index": 16,
      "embedding_norm": 1.0
    },
    {
      "module_path": "File_type",
      "library": "commons",
      "description": "This module defines a variant type to classify files into categories like source code, binary, text, media, and archives. It provides functions to determine a file's type from its name, check if it's textual, or identify specific formats like JSON or WebPL. Use cases include filtering source files, validating media types, or handling archives in build systems.",
      "description_length": 362,
      "index": 17,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Outline",
      "library": "pfff-h_files-format",
      "description": "Handles parsing and writing hierarchical outline data from files using customizable regex patterns. Operates on `outline` trees composed of `outline_node` elements, each containing title and stars fields. Used to read and write structured outline files, such as those in Org mode, with functions to check root nodes and define root-level content.",
      "description_length": 346,
      "index": 18,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Simple_format",
      "library": "pfff-h_files-format",
      "description": "This module processes text files by filtering out comment lines and parsing structured content. It uses a regular expression to identify comments, reads and filters lines from a file, and parses title-colon-value formats into structured data. Concrete use cases include extracting configuration entries and processing annotated text files.",
      "description_length": 339,
      "index": 19,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Source_tree",
      "library": "pfff-h_files-format",
      "description": "Handles tree structure transformations by mapping directories to subsystems. Provides functions to load and apply hierarchical reorganizations, and determine subsystems from directories. Useful for refactoring directory structures in projects while maintaining logical subsystem associations.",
      "description_length": 292,
      "index": 20,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Meta_parse_info",
      "library": "pfff-h_program-lang",
      "description": "This module converts parse information into a structured format using customizable precision settings. It operates on `Parse_info.t` data, allowing selective inclusion of full, token, or type information via the `dumper_precision` type. Use it to serialize parse details for debugging, logging, or analysis with varying levels of detail.",
      "description_length": 337,
      "index": 21,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Database_code",
      "library": "pfff-h_program-lang",
      "description": "This module manages a database of code entities, providing operations to load, save, and merge databases, as well as query and adjust entity metadata. It works with structured data including entity records containing identifiers, names, file positions, and properties, along with directories and files tracked in a hierarchical format. Use cases include building and maintaining a structured index of code elements for features like auto-completion, cross-referencing, and usage analysis.",
      "description_length": 488,
      "index": 22,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Pretty_print_code",
      "library": "pfff-h_program-lang",
      "description": "This module manages code and structured data formatting through stateful operations that track indentation levels, margins, and line positions using a mutable environment type. It processes lists, strings, and nested structures with utilities for conditional spacing, block nesting, and separator insertion while supporting backtracking during format decisions. Key applications include generating human-readable code representations, pretty-printing function arguments and lists with dynamic indentation, and handling whitespace-sensitive text construction.",
      "description_length": 558,
      "index": 23,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Tags_file",
      "library": "pfff-h_program-lang",
      "description": "This module defines a data structure for managing tags in a `TAGS` file, including operations to generate both standard and vi-compatible tag files, create individual tags, and add method tags in unambiguous contexts. It works with lists of files paired with tag lists, where each tag includes metadata like name, line number, byte offset, and entity kind. Concrete use cases include generating navigation files for code editors and enriching source code with structured tag information.",
      "description_length": 487,
      "index": 24,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Test_program_lang",
      "library": "pfff-h_program-lang",
      "description": "Analyzes program layer statistics from a given filename, producing structured output. Extracts and reports metrics related to program structure and complexity. Useful for static analysis tools and code quality assessment.",
      "description_length": 221,
      "index": 25,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_program_lang",
      "library": "pfff-h_program-lang",
      "description": "This module defines a single unit test value that can be executed with OUnit. It works with the OUnit testing framework's test type to construct and run test cases. A concrete use case is verifying the correctness of functions in other modules through assertions and test suites.",
      "description_length": 279,
      "index": 26,
      "embedding_norm": 1.0
    },
    {
      "module_path": "R2c",
      "library": "pfff-h_program-lang",
      "description": "Converts error codes to JSON format or string representations. Works with error lists and string identifiers. Useful for logging or returning structured error responses in web services.",
      "description_length": 185,
      "index": 27,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_ast_fuzzy",
      "library": "pfff-h_program-lang",
      "description": "This module constructs and manipulates abstract syntax trees and token lists from a list of tokens using customizable hooks. It provides functions to build trees, extract tokens from trees, and define visitors to traverse tree structures. Concrete use cases include parsing and analyzing source code with flexible syntactic representations.",
      "description_length": 340,
      "index": 28,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Layer_coverage",
      "library": "pfff-h_program-lang",
      "description": "This module converts line coverage data into visualization layers and provides command-line actions for processing. It transforms `Coverage_code.lines_coverage` into red-green or heatmap layers using `gen_red_green_layer` and `gen_heatmap_layer`. Use cases include generating visual representations of code coverage from test runs and integrating with command-line tools for coverage analysis.",
      "description_length": 393,
      "index": 29,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Entity_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines types and conversions for representing code entities and their properties. It includes operations to convert entity kinds to and from strings, and structures to describe properties such as dynamic calls, global usage, and class kind. It is used to model and analyze source code elements with specific metadata in static analysis tools.",
      "description_length": 355,
      "index": 30,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Refactoring_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines data types to represent specific code refactoring operations, such as adding or removing interfaces, splitting members, and modifying type hints, along with positional information for applying changes. It provides the `load` function to read a list of refactorings from a file, enabling batch processing of code transformations. Use cases include automating codebase migrations, enforcing coding standards, and implementing large-scale type annotations.",
      "description_length": 473,
      "index": 31,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Error_code",
      "library": "pfff-h_program-lang",
      "description": "This module provides operations for creating, annotating, filtering, and comparing errors with support for severity levels, location tracking, and path adjustments. It works with error data structures, error lists, and file-based inputs to enable use cases like command-line configurable error reporting, validation of errors against expected file outputs, and exception handling in workflows requiring precise error control.",
      "description_length": 425,
      "index": 32,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast_generic_common",
      "library": "pfff-h_program-lang",
      "description": "This module converts arithmetic operators and increment/decrement expressions from a generic AST representation into OCaml values. It handles specific data types like `arithmetic_operator`, `incr_decr`, and tuples of `incr_decr` with `prefix_postfix`. Use cases include translating abstract syntax trees into executable OCaml code or analyzing expression structures in a compiler or linter.",
      "description_length": 390,
      "index": 33,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_info",
      "library": "pfff-h_program-lang",
      "description": "This module provides utilities for tracking and manipulating source code positions, token metadata, and parsing state during lexing and parsing. It operates on token lists, lex buffers, and file change data to manage character offsets, line/column conversions, and token origin tracking, with support for error reporting and position adjustments during transformations. Key use cases include correlating parsed tokens with their original source locations, handling macro-expanded or synthetic tokens, and converting between absolute character positions and human-readable file coordinates for diagnostics.",
      "description_length": 605,
      "index": 34,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Prolog_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines a set of fact types representing program analysis data, such as entity locations, kinds, types, and relationships like inheritance or mixin usage. It includes functions to convert facts and entity kinds to string representations and to parse entities from strings. It is used to encode and manipulate structured analysis output, suitable for storage or exchange in a Prolog-like format.",
      "description_length": 406,
      "index": 35,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_generic",
      "library": "pfff-h_program-lang",
      "description": "This module defines core AST types and utilities for managing identifiers, symbols, and resolved names, supporting operations like entity creation, parameter/field manipulation, and gensym-based symbol generation. It provides transformation functions to convert statements into structured items, extract variable names, and normalize definitions into expressions, primarily working with AST nodes such as statements, expressions, and resolved name references. These capabilities enable semantic analysis, code restructuring, and symbol resolution in language processing tasks.",
      "description_length": 576,
      "index": 36,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Layer_code",
      "library": "pfff-h_program-lang",
      "description": "This module manages layered data visualizations by defining structures for organizing files, metadata, and color-coded properties. It supports operations to build, filter, load, and save layers, along with generating JSON representations and statistical summaries. Concrete use cases include visualizing code analysis results with color mappings for properties like red-green status or heat maps.",
      "description_length": 396,
      "index": 37,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Highlight_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines a comprehensive set of syntax highlighting categories and associated rendering rules for code visualization. It includes functions to map categories and usage information to visual styles such as foreground/background colors, font weight, and underlining. It is used to apply precise, context-aware syntax highlighting in code editors or viewers based on the semantic role of each token.",
      "description_length": 407,
      "index": 38,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Big_grep",
      "library": "pfff-h_program-lang",
      "description": "This module provides functions to build and query an index of entities within a large string, supporting both case-sensitive and case-insensitive searches. It uses a hash table to map positions in the string to entities, enabling efficient lookups. Concrete use cases include searching for the top N matching entities in a document or dataset based on substring queries.",
      "description_length": 370,
      "index": 39,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Skip_code",
      "library": "pfff-h_program-lang",
      "description": "This module processes skip lists to filter, reorder, and classify files based on directory and file skip rules. It operates on lists of `skip` values, which represent directories, files, or special skip markers, and applies them to file paths during traversal or processing. Concrete use cases include excluding specific files or directories from analysis, prioritizing or deferring error-prone files, and validating file inclusion based on predefined skip rules.",
      "description_length": 463,
      "index": 40,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Info_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines an `info_txt` type as an alias for `Outline.outline` and provides the `load` function to read and parse a file into this structure. It works with file paths and outline data, typically used to extract structured information from text files. A concrete use case is loading documentation or structured metadata stored in plain text format.",
      "description_length": 357,
      "index": 41,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Archi_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines a comprehensive set of source architecture categories and source kinds, providing direct mappings from architectural roles to string representations and maintaining a fixed list of all architecture types. It includes a function to detect duplicate directory names, ensuring unique directory structure organization. Concrete use cases include categorizing source files by architectural role, validating directory layouts, and supporting tooling that depends on source structure analysis.",
      "description_length": 506,
      "index": 42,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_fuzzy",
      "library": "pfff-h_program-lang",
      "description": "This module represents a simplified abstract syntax tree (AST) structure for fuzzy parsing, supporting operations to model parenthetical, bracketed, and braced expressions, along with metavariables and ellipses. It works with token and tree data types, where trees are composed of nested structures and wrapped strings. Concrete use cases include building and matching incomplete or approximate code snippets during code search or transformation tasks.",
      "description_length": 452,
      "index": 43,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_unparser",
      "library": "pfff-h_program-lang",
      "description": "This module processes lists of token-kind and parse-info pairs, transforming them into a structured format for unparsing. It handles operations like filtering out specific elements, converting transformed tokens to strings, and manipulating elements based on their type, such as removing or adding tokens. Use cases include generating source code from modified parse trees and cleaning up token sequences by removing unwanted or redundant elements.",
      "description_length": 448,
      "index": 44,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Comment_code",
      "library": "pfff-h_program-lang",
      "description": "Handles placement of comments relative to code elements by determining the appropriate position for comments before or after specific nodes in the abstract syntax tree. Works with `Parse_info.t` nodes and generic lists of elements. Used during code formatting or transformation to ensure comments are preserved in the correct location.",
      "description_length": 335,
      "index": 45,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Scope_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines a set of scope identifiers used to represent different variable binding contexts in the language. It provides functions to convert these scope values to string representations and to OCaml values. These operations are used during code generation and analysis to track variable visibility and lifetime.",
      "description_length": 321,
      "index": 46,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Overlay_code",
      "library": "pfff-h_program-lang",
      "description": "This module manages file path mappings between original and overlay directories using hash tables and file lists. It provides functions to create, load, save, and validate overlays, as well as adapt layers and databases to use overlay paths. Concrete use cases include redirecting file accesses during testing, managing configuration overlays, and adapting build artifacts to different directory structures.",
      "description_length": 407,
      "index": 47,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Flag_parsing",
      "library": "pfff-h_program-lang",
      "description": "This module manages parsing and lexing behavior through mutable flags that control verbosity, error handling, and debugging. It provides functions to configure command-line options for tools that require fine-grained control over parsing processes. The module is used in tools like code analyzers or linters to enable or disable features such as error recovery, lexical error reporting, and debug output during parsing.",
      "description_length": 419,
      "index": 48,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Archi_code_lexer",
      "library": "pfff-h_program-lang",
      "description": "This module implements a lexer for parsing source code, primarily handling tokenization and categorization of input. It operates on `Lexing.lexbuf` input buffers and returns categorized tokens as `source_archi` values. It is used to analyze and break down source code into structured components during compilation or interpretation.",
      "description_length": 332,
      "index": 49,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Archi_code_parse",
      "library": "pfff-h_program-lang",
      "description": "Parses architectural descriptions from files, converting them into structured `source_archi` representations. It processes input files using a specified root directory to resolve relative paths. This module is used to load and initialize architecture models from disk for further analysis or transformation.",
      "description_length": 307,
      "index": 50,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Coverage_code",
      "library": "pfff-h_program-lang",
      "description": "This module handles serialization and storage of test and line coverage data. It works with association lists mapping filenames to coverage scores or detailed line coverage information. Functions convert coverage data to and from JSON, save it to files, or load it, enabling persistent tracking of code coverage metrics across test runs.",
      "description_length": 337,
      "index": 51,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast_fuzzy",
      "library": "pfff-h_program-lang",
      "description": "Converts a `trees` value from the `Ast_fuzzy` module into an `Ocaml.v` value, representing OCaml abstract syntax trees in a structured form. Works with `Ast_fuzzy.trees` and `Ocaml.v` data types. Useful for transforming parsed OCaml code into a format suitable for further analysis or manipulation.",
      "description_length": 298,
      "index": 52,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Datalog_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines a set of fact types representing pointer analysis and program analysis relations, such as variable-to-heap assignments, field accesses, and function calls. It provides functions to convert facts to string representations, output facts in bddbddb format for analysis, and generate explanations from bddbddb output files. These operations directly support static analysis tasks like points-to analysis, call graph construction, and field-sensitive data flow tracking.",
      "description_length": 485,
      "index": 53,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Layer_parse_errors",
      "library": "pfff-h_program-lang",
      "description": "This module generates visualization layers from parsing statistics. It provides functions to create red-green and heatmap layers based on a directory root and a list of parsing stats. These layers are used to represent codebase parsing success or failure visually in a heatmap or diff-like format.",
      "description_length": 297,
      "index": 54,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Pleac",
      "library": "pfff-h_program-lang",
      "description": "This module processes code excerpts and skeletons to generate structured source files. It parses data files into sections of code, and skeleton files to map output structure, using comment styles to format generated files. It supports generating either one file or directory per section, with customizable hooks for formatting output.",
      "description_length": 334,
      "index": 55,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Highlight_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "Processes OCaml programs to apply syntax highlighting by visiting each node in the abstract syntax tree and corresponding tokens. It uses a customizable tag hook to associate syntax categories with specific formatting actions, based on parsing information and user preferences. This module is used to generate highlighted output for OCaml source code in tools like documentation generators or code viewers.",
      "description_length": 406,
      "index": 56,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ml_to_generic",
      "library": "pfff-lang_ml-analyze",
      "description": "This module provides polymorphic transformation functions to convert OCaml AST elements into a generic AST representation, handling constructs like expressions, patterns, types, and modules through direct mappings and wrappers for collections. It operates on OCaml's `Ast_ml` structures and produces nodes in the `G` generic AST format, ensuring uniform translation across language features such as match cases, literals, and for-direction constructs. The conversions are designed for use in code analysis or transformation tools requiring intermediate generic representations of OCaml programs.",
      "description_length": 595,
      "index": 57,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_analyze_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "This module defines a single unit test value compatible with the OUnit testing framework. It works with the `OUnit.test` type, which represents test cases or test suites. The primary use case is to provide an entry point for executing a specific set of tests when integrated with an OUnit-based test runner.",
      "description_length": 307,
      "index": 58,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_analyze_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "This module defines a function `actions` that returns a list of command-line operations for analyzing test files. It works with file paths and command-line arguments to process and evaluate test cases. Concrete use cases include running test analysis tools from the command line and generating reports based on test execution results.",
      "description_length": 334,
      "index": 59,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "This module defines core data structures for representing OCaml abstract syntax trees, including types, expressions, patterns, and program items. It supports operations for constructing and deconstructing language elements like function calls, type applications, record expressions, and control structures. Concrete use cases include parsing, type checking, and code transformation tools that operate on OCaml source code.",
      "description_length": 422,
      "index": 60,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Coverage_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "Converts coverage data keyed by filenames into a human-readable format using only the base names of files, given a directory path to resolve relative paths. Works with `lines_coverage` data structures that map file paths to line coverage information. Useful for displaying coverage reports in a cleaner format without full file paths.",
      "description_length": 334,
      "index": 61,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_ml_build",
      "library": "pfff-lang_ml-analyze",
      "description": "The module facilitates conversion of structured syntax elements\u2014such as parenthesized lists, optional values, and type declarations\u2014into abstract syntax tree (AST) representations, while translating nodes between the `Cst_ml` concrete syntax tree and module `A`'s AST constructs. It operates on syntactic elements like expressions, patterns, let-bindings, and type definitions, enabling downstream analysis and transformations during parsing or code processing workflows.",
      "description_length": 471,
      "index": 62,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Database_light_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "Computes a database structure from a list of file paths, applying lightweight processing to extract and organize data. Works with file system paths and a custom database type that stores structured information derived from the input files. Useful for building indexed data representations from log files or configuration sources without heavy parsing overhead.",
      "description_length": 360,
      "index": 63,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Tags_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "Reads tag definitions from a list of files or directories, recursively processing each file to extract tags. Works with file paths and tag data structures representing parsed tag information. Useful for building tag databases or analyzing codebases by extracting symbolic tags from source files.",
      "description_length": 295,
      "index": 64,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "Builds a directed graph representing dependencies between OCaml source files. It takes a directory path and a list of file names, parsing each file to extract module dependencies. Useful for analyzing codebases to determine compilation order or visualize module relationships.",
      "description_length": 276,
      "index": 65,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Module_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "Handles module name extraction from file paths and AST nodes. Converts source file names into module names and retrieves the top module name from a code graph node. Used during compilation and analysis to resolve module identifiers from physical files and abstract syntax trees.",
      "description_length": 278,
      "index": 66,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_fuzzy",
      "library": "pfff-lang_FUZZY",
      "description": "This module parses source code files and string patterns into abstract syntax trees using fuzzy parsing techniques. It supports multiple programming languages through the `Lang_fuzzy.t` parameter and produces ASTs along with optional token information. It is used for analyzing code with relaxed syntax constraints, such as handling incomplete or malformed input.",
      "description_length": 363,
      "index": 67,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lang_fuzzy",
      "library": "pfff-lang_FUZZY",
      "description": "This module defines a sum type representing programming languages such as PHP, ML, Java, Skip, JavaScript, and C++. It provides functions to convert strings and filenames into the corresponding language type, enabling language detection based on input patterns. These operations are used for identifying source code file types in tools like linters, parsers, or code analyzers.",
      "description_length": 377,
      "index": 68,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_cst_php",
      "library": "pfff-lang_php",
      "description": "This module converts various PHP concrete syntax tree (CST) elements into OCaml values. It provides functions to transform CST nodes such as expressions, statements, types, and modifiers into a generic OCaml representation. These conversions are used to analyze or manipulate PHP source code through its CST in OCaml-based tooling.",
      "description_length": 331,
      "index": 69,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Visitor_php",
      "library": "pfff-lang_php",
      "description": "This module defines a visitor pattern for traversing and analyzing PHP concrete syntax trees. It provides typed entry points for visiting specific node kinds such as expressions, statements, class definitions, function parameters, and XHP constructs, each with custom callback behavior. It is used to implement concrete syntax tree transformations, static analysis passes, and code generation tools tailored to PHP source code.",
      "description_length": 427,
      "index": 70,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_php",
      "library": "pfff-lang_php",
      "description": "Adjusts a list of PHP tokens to correct common parsing issues, ensuring proper syntax representation. Works specifically with `Parser_php.token` lists. Useful after initial lexing to prepare tokens for accurate parsing in PHP code analysis tools.",
      "description_length": 246,
      "index": 71,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Flag_parsing_php",
      "library": "pfff-lang_php",
      "description": "This module manages parsing behavior and error reporting for PHP code, providing direct control over features like short open tags, case sensitivity, and XHP extensions. It exposes flags and functions to configure lexing, parsing, and pretty-printing, such as `strict_lexer` and `pp_default`. Concrete use cases include enabling Facebook-specific language extensions, customizing error verbosity, and configuring XHP parsing behavior via command-line flags.",
      "description_length": 457,
      "index": 72,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_php",
      "library": "pfff-lang_php",
      "description": "This module defines a comprehensive set of lexical tokens representing PHP syntax elements, including keywords, operators, literals, and structural symbols. It provides parsing functions `main` and `sgrep_spatch_pattern` that transform lexed input into structured PHP CST (concrete syntax tree) representations. It is used for parsing PHP source code into an AST for static analysis, code transformation, or pattern matching tasks.",
      "description_length": 431,
      "index": 73,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Token_helpers_php",
      "library": "pfff-lang_php",
      "description": "This module provides functions to inspect and manipulate PHP tokens, including checking token types like EOF or comments, extracting token kinds and source information, and transforming token metadata. It works directly with `Parser_php.token` and related types such as `Parse_info.token_kind` and `Cst_php.info`. Concrete use cases include filtering comments during code analysis, retrieving token positions for error reporting, and modifying token information during AST transformations.",
      "description_length": 489,
      "index": 74,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_php",
      "library": "pfff-lang_php",
      "description": "This module parses PHP code into concrete syntax trees (CSTs) and provides functions to extract program structures, expressions, and tokens from files or strings. It supports precise parsing with optional preprocessing, fast parsing, and utilities for debugging and testing, such as creating temporary PHP files or parsing expressions in isolation. Use cases include static analysis tools, linters, and code transformation pipelines targeting PHP.",
      "description_length": 447,
      "index": 75,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_php",
      "library": "pfff-lang_php",
      "description": "This module provides operations for managing lexing states, emitting tokens, and handling context-sensitive lexing in PHP, such as distinguishing identifiers from language extensions or XHP tags. It works with `Lexing.lexbuf` input buffers, lexer state variables, and PHP-specific token/info types, supporting constructs like heredoc/nowdoc strings, XHP tags, and nested comments. Key use cases include parsing PHP scripts with embedded variables, state transitions in double-quoted strings, and correctly tokenizing multi-line XHP text or comments.",
      "description_length": 549,
      "index": 76,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Map_php",
      "library": "pfff-lang_php",
      "description": "This module defines a visitor pattern for transforming PHP concrete syntax trees (CSTs), providing functions to map over expressions, statements, class definitions, names, and tokens. It works with recursive data structures representing PHP source code elements, enabling precise modifications during AST traversal. Concrete use cases include code refactoring, linting, and transformation tools that require structured editing of PHP syntax nodes.",
      "description_length": 447,
      "index": 77,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_php_mly_helper",
      "library": "pfff-lang_php",
      "description": "This module provides functions for manipulating and validating PHP abstract syntax trees, particularly handling parameter lists, expression construction, and error reporting for ambiguous XHP syntax. It operates on data types like `Cst_php.toplevel`, `Cst_php.parameter`, and `Cst_php.expr`, along with token and parse info structures. Concrete use cases include flattening top-level statements, constructing valid function parameters, and enforcing correct usage of XHP syntax through failure functions.",
      "description_length": 504,
      "index": 78,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lib_parsing_php",
      "library": "pfff-lang_php",
      "description": "This module handles parsing and analysis of PHP source files, providing functions to identify PHP files, extract structured data from parsed PHP code, and retrieve specific elements like function calls, variables, constants, and return statements. It operates on data types such as filenames, token lists, and various AST node types defined in `Cst_php`. Concrete use cases include static code analysis, code navigation tools, and extracting metadata from PHP programs for refactoring or documentation purposes.",
      "description_length": 511,
      "index": 79,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Pp_php",
      "library": "pfff-lang_php",
      "description": "Adapts PHP tokens by adjusting them to match the original source structure, using a tokenizer function and the original filename. It processes a list of PHP tokens to ensure accurate representation of the source code. Useful for tools that require precise token alignment with the original file, such as code formatters or linters.",
      "description_length": 331,
      "index": 80,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Cst_php",
      "library": "pfff-lang_php",
      "description": "This module provides operations for parsing and transforming PHP syntax structures, focusing on handling nested expressions, function arguments, and type annotations through helper functions that extract or restructure wrapped syntax elements. It works with identifiers, names, type hints, and entities like functions and classes, enabling conversions between identifier types and the extraction of source code strings or token metadata. These capabilities support use cases such as static analysis tools, code refactoring utilities, and precise manipulation of PHP syntax trees during parsing or transformation tasks.",
      "description_length": 618,
      "index": 81,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Scope_php",
      "library": "pfff-lang_php",
      "description": "Handles PHP scope representations with a conversion function to string. Works with the `phpscope` type, which wraps a code scope structure. Useful for debugging or logging PHP scope information in a readable format.",
      "description_length": 215,
      "index": 82,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_parsing_php",
      "library": "pfff-lang_php",
      "description": "This module defines a single test case for parsing PHP unit tests using OUnit. It includes functions to set up and run tests for PHP-specific parsing logic. A concrete use case is validating the correctness of PHP code parsing in a larger testing framework.",
      "description_length": 257,
      "index": 83,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Unparse_php",
      "library": "pfff-lang_php",
      "description": "Converts PHP abstract syntax trees into string representations. It handles various PHP AST nodes such as programs, expressions, and generic AST elements. Useful for generating PHP code from parsed structures, printing transformed code, or debugging AST manipulations.",
      "description_length": 267,
      "index": 84,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_php",
      "library": "pfff-lang_php",
      "description": "This module tests PHP parsing, tokenization, and AST traversal functionalities. It operates on PHP source files, validating correct syntax tree generation and node visiting. Use cases include verifying parser accuracy on specific files and checking tokenization consistency.",
      "description_length": 274,
      "index": 85,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Annotation_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module parses JavaScript programs to extract specific annotations like `ProvidesModule`, `ProvidesLegacy`, and `RunWhenReady` from comments. It works with JavaScript ASTs and raw strings, producing lists of typed annotations paired with source location metadata. Concrete use cases include analyzing module dependencies and execution timing in legacy JavaScript codebases.",
      "description_length": 377,
      "index": 86,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Map_ast_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module provides functions for transforming and traversing JavaScript AST nodes using a visitor pattern. It includes utilities for mapping over optional values and scopes, and defines a default traversal strategy that can be extended. Concrete use cases include rewriting specific expressions, analyzing variable scopes, or modifying syntax tree structures during static analysis or code transformation tasks.",
      "description_length": 413,
      "index": 87,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Ast_js_build",
      "library": "pfff-lang_js-analyze",
      "description": "Converts concrete syntax trees (CSTs) into abstract syntax trees (ASTs) for JavaScript programs and expressions. It processes CST nodes like statements, expressions, and declarations into their corresponding AST representations. Useful during parsing and transformation stages in JavaScript compilers or linters.",
      "description_length": 312,
      "index": 88,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Utils_js",
      "library": "pfff-lang_js-analyze",
      "description": "Converts JavaScript AST nodes to their string representation and loads files by executing a function within the context of the file's contents. Works with JavaScript AST structures and file paths. Useful for debugging or extracting source code from parsed JavaScript files.",
      "description_length": 273,
      "index": 89,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Graph_code_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module constructs a dependency graph from JavaScript source files, analyzing variable declarations and expressions to determine entity kinds. It processes directories and file lists into a structured graph representation, handling AST nodes and qualified names. Concrete use cases include static analysis for code navigation, variable scoping, and program structure visualization.",
      "description_length": 385,
      "index": 90,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Js_to_generic",
      "library": "pfff-lang_js-analyze",
      "description": "Converts JavaScript AST nodes to their generic AST equivalents. It handles whole programs and individual AST nodes, ensuring types align between the JavaScript-specific and generic AST representations. Useful when building tools that analyze or transform JavaScript code using a unified AST interface.",
      "description_length": 301,
      "index": 91,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Tags_js",
      "library": "pfff-lang_js-analyze",
      "description": "Converts a list of file or directory paths into a list of filenames paired with their associated tags by parsing each file's contents. Works with lists of strings representing file paths and returns tuples of filenames and tag lists. Useful for extracting structured metadata from source code files during analysis or indexing tasks.",
      "description_length": 333,
      "index": 92,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module facilitates the conversion of JavaScript abstract syntax tree nodes into generic OCaml values, enabling serialization of structures like expressions, statements, and program elements. It operates on data types such as properties, module directives, and top-level constructs, supporting use cases that require interoperability between JavaScript and OCaml environments, such as static analysis, code transformation, or cross-language data marshaling.",
      "description_length": 461,
      "index": 93,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Database_light_js",
      "library": "pfff-lang_js-analyze",
      "description": "Computes a database structure from a list of file paths, applying lightweight processing to extract and organize data. Works with file system paths and constructs an in-memory database representation. Useful for initializing a simplified database model from source files in a specified directory.",
      "description_length": 296,
      "index": 94,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Module_path_js",
      "library": "pfff-lang_js-analyze",
      "description": "Resolves a file path relative to a given root and current working directory, returning the absolute path if valid. Works with string-based directory and file paths. Useful for locating files in a project directory structure when given relative or partial paths.",
      "description_length": 261,
      "index": 95,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Stdlib_js",
      "library": "pfff-lang_js-analyze",
      "description": "Handles file path resolution and extraction of standard library files. Works with filenames and directory names. Extracts specified source files into a target directory, ensuring correct path handling for the standard library.",
      "description_length": 226,
      "index": 96,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Module_pre_es6",
      "library": "pfff-lang_js-analyze",
      "description": "This module defines a hierarchy of shape types to represent JavaScript module structures, including objects, arrays, functions, and classes, with operations to construct and manipulate these shapes. It provides specific functions like `new_object` to create object shapes from property maps, `new_class` to generate class shapes with prototypes, and `string_of_shape` to serialize shapes into strings for debugging. These capabilities support analysis and transformation of JavaScript codebases, particularly for tools handling module dependencies and type inference.",
      "description_length": 567,
      "index": 97,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_ast_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module implements a visitor pattern for traversing and analyzing JavaScript abstract syntax trees. It provides hooks to customize processing of expressions, statements, top-level nodes, object properties, function parameters, and token information. Concrete use cases include static analysis tools, linters, and code transformation passes that need to inspect or modify JavaScript AST nodes programmatically.",
      "description_length": 413,
      "index": 98,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_analyze_js",
      "library": "pfff-lang_js-analyze",
      "description": "Transforms JavaScript AST nodes by abstracting position information and extracts parse info from nodes. Works with `Ast_js.any` and `Parse_info.t` types. Useful for analyzing JavaScript code structure while preserving source location data for error reporting or refactoring tools.",
      "description_length": 280,
      "index": 99,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module defines core data structures for representing JavaScript abstract syntax trees, including expressions, statements, and program elements. It supports operations for parsing and analyzing JavaScript code, such as handling variables, control flow, and module directives like imports and exports. Concrete use cases include building linters, code transformers, and static analysis tools that operate on JavaScript source code.",
      "description_length": 434,
      "index": 100,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Class_pre_es6",
      "library": "pfff-lang_js-analyze",
      "description": "Extracts entity kinds and names from a JavaScript AST, mapping tokens to their associated entities. Works with parsed JavaScript programs and symbol tables. Useful for static analysis tools needing to resolve variable or function declarations in pre-ES6 code.",
      "description_length": 259,
      "index": 101,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Comment_js",
      "library": "pfff-lang_js-analyze",
      "description": "Removes leading and trailing comment markers from a string. Works with standard comment syntax like `(* ... *)` or `// ...`. Useful for processing OCaml or C-style comments in source code analysis tools.",
      "description_length": 203,
      "index": 102,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Transpile_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module handles the transformation of JavaScript concrete syntax trees (CST) into abstract syntax trees (AST), with specific support for translating XHP HTML expressions, pattern matching, variable declarations, and for-of loop constructs. It operates on data types such as `Cst_js.expr`, `Cst_js.pattern`, `Cst_js.name`, and `Cst_js.stmt`, converting them into their corresponding AST variants. Concrete use cases include parsing and transforming JavaScript source code that includes JSX (via XHP), destructuring assignments, and ES6 for-of loops into a form suitable for further analysis or code generation.",
      "description_length": 613,
      "index": 103,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Highlight_js",
      "library": "pfff-lang_js-analyze",
      "description": "Processes a JavaScript program to apply syntax highlighting by traversing the concrete syntax tree and tokens. Uses a provided tag hook to associate categories with tokens based on the parsed structure and user preferences. Useful for rendering highlighted JavaScript code in editors or documentation tools.",
      "description_length": 307,
      "index": 104,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_analyze_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module defines command-line actions for analyzing JavaScript test files. It provides a function `actions` that returns a list of available command-line operations. These actions are specifically used to trigger analyses like parsing, linting, or generating reports for JavaScript test codebases.",
      "description_length": 300,
      "index": 105,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_analyze_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module defines a single unit test suite using OUnit, focused on testing JavaScript-related functionality. It includes test cases that validate the behavior of JavaScript interoperability features, such as value conversions and function calls. A concrete use case is verifying that OCaml bindings to JavaScript APIs behave correctly in a Node.js or browser environment.",
      "description_length": 373,
      "index": 106,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast",
      "library": "pfff-lang_GENERIC",
      "description": "Converts a generic AST node into an OCaml value representation. Works with `Ast_generic.any` and `Ocaml.v` types. Useful for serializing AST elements into values for analysis or transformation tasks.",
      "description_length": 199,
      "index": 107,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast",
      "library": "pfff-lang_GENERIC",
      "description": "This module defines core abstract syntax tree (AST) types used for representing program structures in a generic form, including programs, expressions, statements, identifiers, and names. It provides direct type aliases for working with these AST elements, enabling precise manipulation and traversal of parsed code. Concrete use cases include building and analyzing code transformations, linters, or static analysis tools that require direct AST access.",
      "description_length": 453,
      "index": 108,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_ast",
      "library": "pfff-lang_GENERIC",
      "description": "This module defines a visitor pattern for traversing and analyzing abstract syntax trees (ASTs) with specific entry points for expressions, statements, types, patterns, and other language constructs. It provides a structured way to hook into AST node processing through continuation-based callbacks that receive both the node and a visitor output handle. Concrete use cases include static analysis tools, linters, and code transformation passes that need to inspect or modify AST nodes during traversal.",
      "description_length": 503,
      "index": 109,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_ast",
      "library": "pfff-lang_GENERIC",
      "description": "Converts abstract syntax tree nodes to position information and strips positional details from nodes. Works with AST elements and their associated source code metadata. Useful for analyzing or transforming code while managing source location data.",
      "description_length": 247,
      "index": 110,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Find_source",
      "library": "pfff-lang_GENERIC",
      "description": "This module identifies source files in specified directories or file lists for a given language. It provides functions to collect filenames from a root directory, process a mix of directories and files, and apply a finder function to paths. Concrete use cases include scanning project directories for source files during build setup or analysis tool configuration.",
      "description_length": 364,
      "index": 111,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_generic",
      "library": "pfff-lang_GENERIC",
      "description": "This module defines a function `actions` that returns a list of command-line parsing actions for test execution. It works with common data types like strings and lists to handle input arguments. Concrete use cases include setting up test runners that parse command-line flags to control test behavior, such as selecting specific test cases or enabling verbose output.",
      "description_length": 367,
      "index": 112,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lang",
      "library": "pfff-lang_GENERIC",
      "description": "This module defines a type for programming languages and provides functions to convert strings and filenames to language tags. It supports filtering source files by language from a list of paths. For example, it can identify Python files from a directory list or determine the language of a file based on its extension.",
      "description_length": 319,
      "index": 113,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_generic",
      "library": "pfff-lang_GENERIC",
      "description": "This module provides functions to parse source code into abstract syntax trees (ASTs) for different programming languages. It supports parsing entire programs from files and parsing individual code patterns from strings, using a specified language. The parsed output conforms to a generic AST structure, enabling cross-language analysis and transformation tasks.",
      "description_length": 362,
      "index": 114,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Map_ast",
      "library": "pfff-lang_GENERIC",
      "description": "This module defines a visitor pattern for transforming AST nodes in a concrete syntax tree, specifically handling expressions, statements, and token information. It provides a structured way to traverse and modify AST elements using customizable visitor functions. Use cases include implementing code refactoring tools, linters, or AST-based transformations where selective node processing is required.",
      "description_length": 402,
      "index": 115,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "SetPt",
      "library": "commons_ocollection",
      "description": "This module implements a binary trie structure optimized for integer sets using bitwise operations to enable efficient membership checks, dynamic modifications, and set algebra operations like union and difference. It specializes in handling sparse integer key distributions through Patricia tree mechanics, leveraging prefix matching and bitmasking for compact representation. The functionality supports use cases such as network routing table implementations, sparse bitmap management, and combinatorial optimization problems requiring fast set operations.",
      "description_length": 558,
      "index": 116,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Osetb",
      "library": "commons_ocollection",
      "description": "Represents an empty set value for polymorphic set implementations. Used as a starting point for building sets through union or add operations. Essential for initializing set-based computations where elements are incrementally inserted.",
      "description_length": 235,
      "index": 117,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Oset",
      "library": "commons_ocollection",
      "description": "This module defines infix operators for set-like operations such as membership, union, intersection, difference, subset checks, and equality comparisons. It works with objects that support set-like methods like `mem`, `union`, `inter`, `minus`, `is_subset_of`, and `is_equal`. Use this module to manipulate custom set-like structures with a concise operator syntax, such as combining or comparing sets of values.",
      "description_length": 412,
      "index": 118,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ograph_extended",
      "library": "commons_ocollection",
      "description": "This module provides depth-first search traversal functions for mutable graph structures, allowing callbacks on nodes and path tracking. It supports visual representation of graphs through customizable printing functions that generate output files compatible with graph visualization tools. Concrete use cases include analyzing control flow graphs, generating visual depictions of abstract syntax trees, and debugging graph transformations.",
      "description_length": 440,
      "index": 119,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Seti",
      "library": "commons_ocollection",
      "description": "This module implements a list-based set structure for integers, offering operations to add, remove, and check element membership while maintaining exact representations through invariant-preserving transformations. It supports interval-based manipulations via range-packing and decomposition functions, alongside standard set operations like union, intersection, and difference, all optimized for list-backed integer sets. The toolchain is particularly suited for scenarios requiring precise integer range management, such as resource allocation, interval tracking, or symbolic computation with sparse numeric domains.",
      "description_length": 618,
      "index": 120,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ofullcommon",
      "library": "commons_ocollection",
      "description": "This library provides utilities for structured data manipulation, including list transformations, string processing, and operations on custom set and hash types, alongside system-level functionalities like file handling, path normalization, and process control. It supports debugging through logging, profiling with score tracking, and testing via assertions, while offering advanced data structure manipulations for trees, graphs, and matrices, enabling applications in data processing pipelines, system scripting, and performance analysis.",
      "description_length": 541,
      "index": 121,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ograph_simple",
      "library": "commons_ocollection",
      "description": "Converts a mutable graph structure into a string representation using custom formatting functions for keys and nodes, then writes the result to a file. Works with any graph containing keys, nodes, and edges. Useful for serializing graph data to disk for debugging or external processing.",
      "description_length": 287,
      "index": 122,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Highlight_nw",
      "library": "pfff-lang_nw-analyze",
      "description": "Traverses a parsed program and token stream, applying a tagging function to each token based on its syntactic category. Uses highlighter preferences to determine which elements to mark and how to classify them. Useful for syntax highlighting in editors or code visualization tools where tokens need contextual categorization during traversal.",
      "description_length": 342,
      "index": 123,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dataflow.VarSet",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module manages immutable sets of strings ordered by a comparison function, providing standard set operations like union, intersection, and difference alongside ordered element retrieval (e.g., min/max), filtering, and transformation. It supports conversions to and from lists and sequences, with sequence-based construction and modification, and includes safe and unsafe variants for handling edge cases like missing elements. These features are particularly valuable for variable set analysis requiring precise ordered processing and robust element management.",
      "description_length": 566,
      "index": 124,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dataflow.VarMap",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module provides ordered maps with string keys, enabling operations like insertion, deletion, merging, and ordered traversal while supporting transformations that preserve key ordering. It includes utilities for querying boundary elements (min/max), mapping values with or without key context, and splitting or combining maps based on ordered key ranges. These features are ideal for managing symbol tables, aggregating ordered data streams, or implementing algorithms requiring sorted associative storage with string-indexed values",
      "description_length": 536,
      "index": 125,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Normalize_ast",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "Transforms abstract syntax trees by applying normalization rules to ensure consistent structure and representation. Works directly with `Ast.any` types, standardizing constructs like expressions, statements, and declarations. Useful for preparing ASTs before analysis or code generation, ensuring equivalent forms are treated uniformly.",
      "description_length": 336,
      "index": 126,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Controlflow",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module manages control flow graphs using nodes and edges, where nodes represent program points with optional parse information and edges indicate direct transitions. It provides operations to locate specific nodes, identify entry and exit points, and visualize the graph. Concrete use cases include analyzing structured code flow in ASTs, such as tracking execution paths through statements or functions.",
      "description_length": 409,
      "index": 127,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Controlflow_build",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module constructs control flow graphs from function definitions and statement lists, producing `Controlflow.flow` structures that represent program execution paths. It handles AST-generic parameters and statements, generating detailed error information with location metadata when construction fails. Use cases include static analysis tools that require CFGs for detecting unreachable code, analyzing loops, or performing data flow analysis.",
      "description_length": 446,
      "index": 128,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_analyze_generic",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module defines a function `actions` that returns a list of command-line operations for analyzing test files. It works with file paths and command-line arguments to process and evaluate test cases. Concrete use cases include running test analysis tools from the command line, such as checking test coverage or validating test structures.",
      "description_length": 341,
      "index": 129,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dataflow",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module implements dataflow analysis primitives using variable maps and sets to track value propagation through control flow graphs. It provides fixed-point computation for iterative dataflow algorithms, union and difference operations on variable environments, and node indexing utilities for mapping variables to control flow nodes. These capabilities enable precise liveness analysis, reaching definitions tracking, and other dataflow analyses on program structures represented via the associated flow and nodei types.",
      "description_length": 525,
      "index": 130,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dataflow_liveness",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "Performs liveness analysis on control flow graphs to determine which variables are live at each point in the program. It uses a fixpoint algorithm to propagate liveness information through the flow graph, updating a mapping that tracks live variables at each node. This module is used during register allocation and dead code elimination to optimize program execution.",
      "description_length": 368,
      "index": 131,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Controlflow_visitor",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module processes control flow graphs by extracting expressions from nodes and applying transformations across nodes and expressions. It works with control flow nodes, abstract syntax tree expressions, and visitor functions that operate on these structures. Concrete use cases include analyzing or modifying code during static analysis, such as gathering all expressions in a control flow path or applying a transformation to each node and its associated expressions.",
      "description_length": 471,
      "index": 132,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dataflow_reaching",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "Implements a fixpoint algorithm for dataflow analysis, computing reaching definitions across control flow graphs. Operates on node-indexed sets to track definition propagation through program points. Useful for compiler optimization tasks like dead code elimination and variable liveness analysis.",
      "description_length": 297,
      "index": 133,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lrvalue",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module analyzes expressions to extract identifiers used as lvalues or rvalues. It provides functions to retrieve lists of variable names and their associated metadata from expressions. Useful for static analysis tasks like identifying variable assignments and uses in a compiler or linter.",
      "description_length": 294,
      "index": 134,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Highlight_java",
      "library": "pfff-lang_java-analyze",
      "description": "Processes Java source code to apply syntax highlighting by traversing the abstract syntax tree and token list. It uses a provided tag hook to mark up specific elements according to their category, such as keywords or comments. This module is used during the highlighting phase to generate formatted output for Java files.",
      "description_length": 321,
      "index": 135,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Unit_analyze_java",
      "library": "pfff-lang_java-analyze",
      "description": "This module defines a single unit test case using OUnit, specifically targeting Java-related functionality. It works with Java objects and methods through OCaml's Java interoperability features. A concrete use case is testing the correctness of Java method invocations from OCaml code.",
      "description_length": 285,
      "index": 136,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Java_to_generic",
      "library": "pfff-lang_java-analyze",
      "description": "Converts Java abstract syntax trees into a generic representation. Transforms Java programs and arbitrary Java AST nodes into their generic equivalents. Useful for analyzing Java code with tools that operate on the generic AST format.",
      "description_length": 234,
      "index": 137,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Graph_code_java",
      "library": "pfff-lang_java-analyze",
      "description": "Builds a graph representation of Java code by parsing definitions and dependencies from a list of filenames. It processes Java source files to extract structural information such as class hierarchies and method calls. Useful for static analysis tasks like code navigation, dependency visualization, and call graph construction.",
      "description_length": 327,
      "index": 138,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Package_java",
      "library": "pfff-lang_java-analyze",
      "description": "Performs a lookup in the graph for a fully qualified Java class name, given as a list of package components and a class name. It returns an optional node representing the class if found. Useful for resolving class references in Java codebases during analysis or transformation tasks.",
      "description_length": 283,
      "index": 139,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Builtins_java",
      "library": "pfff-lang_java-analyze",
      "description": "Extracts files from a list of source directories into a destination directory. Works with directory and file path data types to perform bulk file copying operations. Useful for consolidating source files from multiple directories into a single output location.",
      "description_length": 260,
      "index": 140,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_analyze_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "This module provides a function to highlight C++ code in a specified file and a function to retrieve command-line actions for testing. It works with file paths and command-line interface data structures. Use it to analyze and visualize C++ syntax in test scenarios or integrate test actions into a CLI tool.",
      "description_length": 307,
      "index": 141,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Highlight_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "Processes C++ top-level syntax elements and associated tokens, applying a tagging function to annotate them based on a given highlighting configuration. It operates on parsed C++ top-level constructs paired with their lexical tokens, enabling semantic-aware highlighting for code presentation or analysis tools. Useful for implementing syntax highlighters or static analysis passes that require precise token-level categorization.",
      "description_length": 430,
      "index": 142,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Builtins_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "Renames standard header files in the specified directory to avoid conflicts. Works with file paths and directories. Useful during C++ project refactoring to ensure header names are unique.",
      "description_length": 188,
      "index": 143,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Check_variables_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "Performs semantic analysis on C++ programs by checking variable declarations and usage. It ensures variables are properly declared before use and annotates the AST with type information. Useful for implementing static analysis tools or compilers targeting C++.",
      "description_length": 260,
      "index": 144,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Database_light_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "This module computes a database structure from a list of file paths, using a specified algorithm to process and store the data. It operates on `Common.path` lists and produces a `Database_code.database` value, which represents the structured result of parsing and analyzing the input files. A typical use case involves generating a searchable database of code entities from a set of source files for static analysis or navigation tools.",
      "description_length": 436,
      "index": 145,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Flag_analyze_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "Controls whether the C++ checker outputs debug information during analysis. Works with boolean values through a mutable reference. Useful for enabling or disabling verbose logging in C++ code analysis workflows.",
      "description_length": 211,
      "index": 146,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Python_to_generic",
      "library": "pfff-lang_python-analyze",
      "description": "Converts Python abstract syntax trees into a generic AST representation. It transforms Python-specific AST nodes, such as those representing functions, classes, and expressions, into their generic equivalents. This module is used when analyzing or processing Python code within a language-agnostic toolchain.",
      "description_length": 308,
      "index": 147,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Highlight_python",
      "library": "pfff-lang_python-analyze",
      "description": "Processes Python programs to apply syntax highlighting by traversing the AST and token list. It uses a provided tag hook to mark up specific categories of tokens based on the given highlighting preferences. Useful for generating highlighted source code output in tools like IDEs or documentation generators.",
      "description_length": 307,
      "index": 148,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Resolve_python",
      "library": "pfff-lang_python-analyze",
      "description": "Resolves name bindings in a Python abstract syntax tree by analyzing variable declarations and usages. It processes `Ast_python.program` structures to ensure each identifier is correctly associated with its definition. This is essential for tasks like static analysis or preparing code for further transformations.",
      "description_length": 314,
      "index": 149,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parse_nw",
      "library": "pfff-lang_nw",
      "description": "This module provides functions to parse network configuration files into abstract syntax trees and token lists. It handles operations like full parsing with `parse`, fuzzy parsing with `parse_fuzzy`, and token extraction via `tokens`, working directly with file paths as input. Concrete use cases include analyzing network policies, extracting configuration elements, and supporting tools like linters or transformers for network infrastructure code.",
      "description_length": 450,
      "index": 150,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Token_helpers_nw",
      "library": "pfff-lang_nw",
      "description": "This module provides functions to inspect and transform tokens from the `Lexer_nw` module. It includes checks for end-of-file and comment tokens, extracts token kinds and parsing info, and allows visiting and modifying token metadata. These operations are used during lexical analysis to process and refine token streams before parsing.",
      "description_length": 336,
      "index": 151,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_nw",
      "library": "pfff-lang_nw",
      "description": "This module processes network-related test files by tokenizing their contents and defining command-line actions for test execution. It operates on filenames and token streams, specifically handling network protocol test cases. Use this module to parse and run tests from files containing network protocol data.",
      "description_length": 310,
      "index": 152,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Ast_nw",
      "library": "pfff-lang_nw",
      "description": "This module processes abstract syntax trees representing network configurations, providing functions to analyze and transform routing and firewall rules. It works with data types such as `program` and nested structures representing network policies. Concrete use cases include validating network reachability constraints and optimizing rule sets for security compliance.",
      "description_length": 370,
      "index": 153,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lexer_nw",
      "library": "pfff-lang_nw",
      "description": "This module defines a lexer for parsing structured text documents, handling tokens like words, numbers, symbols, comments, and verbatim blocks. It supports multiple parsing modes for handling contexts such as LaTeX, noweb chunks, and verbatim sections, with functions to transition between modes. It is used to tokenize input streams for further processing in document analysis or compilation pipelines.",
      "description_length": 403,
      "index": 154,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Figures",
      "library": "pfff-h_visualization",
      "description": "This module defines geometric data structures for working with 2D points and rectangles, both in floating-point and integer pixel formats. It provides operations to calculate dimensions and area, validate rectangle coordinates, check point containment, compute rectangle intersections, and generate random points within pixel rectangles. Concrete use cases include layout validation in graphical interfaces, collision detection in rendering systems, and random point generation for pixel-based image processing.",
      "description_length": 511,
      "index": 155,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Simple_color",
      "library": "pfff-h_visualization",
      "description": "The module facilitates color representation conversions between RGB float, RGB integer, X11 color strings, and integer color codes, while providing predefined color sets (basic, degraded, grayscale) and rainbow arrays. It enables creating colors by name, extracting RGB components, generating Emacs-compatible strings, and selecting random colors from predefined lists, which is particularly useful",
      "description_length": 398,
      "index": 156,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Plot_jgraph",
      "library": "pfff-h_visualization",
      "description": "Generates 2D plots from matrix data using jgraph, with customizable axis labels and row/column headers. Accepts a matrix of floating-point values and outputs visualizations for numerical analysis or data presentation. Useful for plotting heatmaps, function surfaces, or experimental results.",
      "description_length": 291,
      "index": 157,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Treemap",
      "library": "pfff-h_visualization",
      "description": "This module offers operations to build and render hierarchical visualizations using directory/file trees, supporting layout algorithms like Squarified and Classic to partition space into rectangles. It processes tree structures with customizable size, color, and label mappings, and includes utilities for sorting, filtering, and generating example datasets. The module is used to create interactive visualizations of filesystem hierarchies and to initialize test trees for command-line interface demonstrations.",
      "description_length": 512,
      "index": 158,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lib_parsing_lisp",
      "library": "pfff-lang_lisp",
      "description": "Traverses directories and files to collect source file paths. Works with lists of file and directory paths, filtering to return only file paths. Useful for gathering all source files in a project directory for processing or analysis.",
      "description_length": 233,
      "index": 159,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_lisp",
      "library": "pfff-lang_lisp",
      "description": "This module parses Lisp programs into abstract syntax trees and token lists. It provides functions to extract the AST and tokens from a file, along with parsing statistics. Use it to analyze Lisp code structure or build tools that process Lisp source files directly.",
      "description_length": 266,
      "index": 160,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_lisp",
      "library": "pfff-lang_lisp",
      "description": "This module tests the parsing of Lisp source files into tokens and provides a command-line interface for running these tests. It operates on Lisp syntax files and evaluates the correctness of tokenization and parsing logic. A concrete use case is validating that Lisp code is correctly transformed into an abstract syntax tree during the parsing process.",
      "description_length": 354,
      "index": 161,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_lisp",
      "library": "pfff-lang_lisp",
      "description": "This module defines a token type for parsing Lisp-like syntax, including elements like numbers, identifiers, strings, and delimiters with associated source information. It provides functions to inspect tokens, such as checking if a token is EOF or a comment, extracting source info, and transforming token metadata. It is used during lexical analysis to process and track source code elements in a structured way.",
      "description_length": 413,
      "index": 162,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_lisp",
      "library": "pfff-lang_lisp",
      "description": "This module defines a Lisp-like abstract syntax tree with support for parsing and manipulating S-expressions, atoms, and special forms. It works with recursive data structures representing programs as lists of S-expressions, each containing tokens, parentheses, and wrapped special syntax nodes. Concrete use cases include building and transforming Lisp-style code representations for interpreters or linters.",
      "description_length": 409,
      "index": 163,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_lisp",
      "library": "pfff-lang_lisp",
      "description": "This module processes input text into structured tokens for parsing Lisp-like syntax. It includes functions to extract tokens, handle string literals, and report errors during lexical analysis. Concrete use cases include reading source code files, identifying language keywords, and preparing input for the parser module.",
      "description_length": 321,
      "index": 164,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_opti",
      "library": "pfff-graph_code",
      "description": "This module provides optimized graph representations and operations for efficient node and edge manipulation. It works with a custom graph type that maps nodes to indices, tracks children and usage relationships, and supports fast lookups and transformations. Concrete use cases include optimizing graph-based algorithms where performance is critical, such as dependency resolution or hierarchical structure manipulation.",
      "description_length": 421,
      "index": 165,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Graph_code",
      "library": "pfff-graph_code",
      "description": "This module provides functions for constructing, modifying, and analyzing directed graphs with rich metadata attached to nodes and edges, such as file paths, privacy attributes, and hierarchical relationships. It supports operations like graph traversal, dependency resolution, structural statistics, and transformations guided by whitelists or file-based configurations. These capabilities are used in code analysis tasks requiring dependency visualization, hierarchical organization, and structural refinements based on external rules or symbolic mappings.",
      "description_length": 558,
      "index": 166,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_class_analysis",
      "library": "pfff-graph_code",
      "description": "Analyzes class hierarchies and method dispatch in object-oriented code represented as a graph. It builds a structure mapping class names to nodes, identifies top-level methods, and determines candidate methods for protection or dispatch. Useful for analyzing inheritance chains and method resolution in OCaml or similar languages.",
      "description_length": 330,
      "index": 167,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dependencies_matrix_code",
      "library": "pfff-graph_code",
      "description": "This module centers on dependency analysis and graph manipulation through matrix operations, offering functions to expand or focus nodes, score triangular matrix cells, identify inactive rows or columns, and convert traversal paths into string representations. It operates on dependency matrices, node index mappings, and hierarchical graph structures, enabling use cases like software dependency visualization, traversal path optimization, and computing distances between entities in complex systems.",
      "description_length": 501,
      "index": 168,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_tags",
      "library": "pfff-graph_code",
      "description": "Extracts definitions and associated tags from a graph code structure, producing a list of filename and tag list pairs. Processes `Graph_code.graph` data to identify and collect tagged elements. Useful for generating tag files or analyzing code dependencies based on graph representations.",
      "description_length": 288,
      "index": 169,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Layer_graph_code",
      "library": "pfff-graph_code",
      "description": "Generates heatmap and statistics visualization layers for graph data. It processes a graph structure along with node rankings or directory-based statistics, writing the results to image files. Used to visualize node importance and directory activity in a layered graph format.",
      "description_length": 276,
      "index": 170,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_export",
      "library": "pfff-graph_code",
      "description": "Converts a graph structure into a JSON representation. Works with `Graph_code.graph` and `Json_type.t` types. Useful for exporting graph data to be consumed by external systems or APIs that require JSON format.",
      "description_length": 210,
      "index": 171,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_database",
      "library": "pfff-graph_code",
      "description": "Converts a graph representation into a database structure, mapping nodes and edges to corresponding database entries. It operates on graph data structures defined in `Graph_code` and persists them into a `Database_code` format. This function is used to store dependency graphs of codebases in a queryable database for analysis tools.",
      "description_length": 333,
      "index": 172,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_graph_code",
      "library": "pfff-graph_code",
      "description": "This module defines a single function `unittest` that constructs an OUnit test from a string parser producing a graph. It operates on string inputs and graph data structures, specifically integrating graph construction with unit testing frameworks. A typical use case involves testing graph generation from string representations, such as verifying correct parsing of DOT files into internal graph models.",
      "description_length": 405,
      "index": 173,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_prolog",
      "library": "pfff-graph_code",
      "description": "This module defines a context type for tracking usage edges in a graph, supporting operations like adding edges during Prolog code generation. It works with graph nodes and token locations to build Prolog facts from a control flow graph. Concrete use cases include generating Prolog representations of function calls and assignments for analysis tools.",
      "description_length": 352,
      "index": 174,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_helpers",
      "library": "pfff-graph_code",
      "description": "Propagates user information from functions, globals, and types to prototype, extern, and typedef declarations in a graph representation. Operates on a `Graph_code.graph` structure to update node metadata with usage details. Useful for analyzing code dependencies and ensuring correct declaration ordering in generated code.",
      "description_length": 323,
      "index": 175,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dependencies_matrix_build",
      "library": "pfff-graph_code",
      "description": "This module builds and manipulates dependency matrices to optimize the ordering of nodes in a graph based on dependency constraints. It provides functions to construct the matrix from a graph and configuration, analyze dependency orders, and apply sorting and partitioning strategies to reduce dependencies. Key operations include hill climbing for iterative optimization, matrix partitioning, and sorting rows and columns based on dependency counts, specifically supporting use cases like module or package ordering in build systems.",
      "description_length": 534,
      "index": 176,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_graph_code",
      "library": "pfff-graph_code",
      "description": "This module defines a function `actions` that returns a list of command-line actions for testing graph-related code. It works with graph data structures and command-line interface types defined in the `Common` module. Concrete use cases include registering test commands to verify graph algorithms like traversal, shortest path, or cycle detection from the command line.",
      "description_length": 370,
      "index": 177,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Graph_code_checker",
      "library": "pfff-graph_code",
      "description": "Performs semantic analysis on a graph representation of code to detect errors. It processes nodes and edges in the graph structure to identify issues such as unreachable code, cycles, or invalid transitions. Useful for validating control flow graphs in compilers or static analysis tools.",
      "description_length": 288,
      "index": 178,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Visitor_c.Ast_cpp",
      "library": "pfff-lang_c",
      "description": "This module defines visitor methods for traversing and processing various operator nodes in a C++ abstract syntax tree (AST). It includes handlers for assignment, fixity, unary, and binary operations, enabling analysis or transformation of expression-level constructs. Concrete use cases include implementing linters, code analyzers, or transformation passes over C++ source code.",
      "description_length": 380,
      "index": 179,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Ast_c",
      "library": "pfff-lang_c",
      "description": "This module defines core data structures for representing C abstract syntax trees, including types, statements, function definitions, and top-level constructs. It provides operations for working with annotated names, type constructors, control flow statements, and preprocessing directives. Use cases include parsing, analyzing, and transforming C code, such as processing function bodies, inspecting struct layouts, or evaluating macro definitions.",
      "description_length": 449,
      "index": 180,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_c",
      "library": "pfff-lang_c",
      "description": "This module provides functions for creating and combining visitors that traverse C++ AST nodes, specifically handling expressions and token information. It works with visitor input and output types that allow custom processing of C++ expressions and source tokens. Concrete use cases include building linters, code transformers, or static analyzers that process C++ syntax trees.",
      "description_length": 379,
      "index": 181,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Ast_c_build",
      "library": "pfff-lang_c",
      "description": "Converts parsed C++ syntax trees into corresponding C abstract syntax trees. Processes program and any nodes from the C++ concrete syntax tree, mapping them to their C equivalents. Useful for translating C++ code to C during compilation or analysis tasks.",
      "description_length": 255,
      "index": 182,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Meta_ast_c",
      "library": "pfff-lang_c",
      "description": "This module converts C AST nodes into OCaml values, supporting serialization of entire programs, arbitrary AST elements, and type expressions. It operates on data types defined in the `Ast_c` module, including `program`, `any`, and `type_`. Useful for generating OCaml representations of C code structures for analysis or transformation tools.",
      "description_length": 343,
      "index": 183,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_c",
      "library": "pfff-lang_c",
      "description": "This module defines a function `actions` that returns a list of command-line actions for parsing and executing tests. It works with the `Common.cmdline_actions` type, which represents named operations and their argument handlers. It is used to register test-related commands like running or listing tests when initializing a command-line interface.",
      "description_length": 348,
      "index": 184,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_c",
      "library": "pfff-lang_c",
      "description": "This module identifies source files in specified directories or file lists and extracts parse information from abstract syntax trees. It processes file paths to locate C source files and converts syntax tree nodes into structured parse data. Useful for analyzing C codebases by mapping file locations to syntactic elements.",
      "description_length": 323,
      "index": 185,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_c",
      "library": "pfff-lang_c",
      "description": "This module provides functions for parsing C source code into abstract syntax trees (ASTs) and token lists, including full program parsing and parsing of arbitrary C code fragments from strings. It operates on file paths and string inputs, producing structured AST representations and parsing statistics. Concrete use cases include analyzing C codebases, extracting program structures for analysis, and processing C snippets embedded in other data.",
      "description_length": 448,
      "index": 186,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Cst_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module handles low-level syntactic constructs and transformations for C++ code parsing. It provides utilities to destructure and manipulate AST nodes, such as unwrapping parenthesized expressions, extracting identifiers, and managing type qualifiers. Concrete use cases include parsing C++ declarations, resolving qualified names, and transforming token sequences during preprocessing.",
      "description_length": 390,
      "index": 187,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module identifies C++ source files in specified directories or file lists and extracts parsing information from C++ abstract syntax trees. It processes file paths to locate source files and converts syntax tree nodes into structured parsing data. Useful for analyzing C++ codebases by mapping source locations and extracting syntactic details directly from parsed trees.",
      "description_length": 375,
      "index": 188,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_typedef",
      "library": "pfff-lang_cpp",
      "description": "Processes C++ token streams to identify and extract typedef declarations. Works with lists of grouped C++ tokens and nested token lists. Used to analyze C++ codebases for type definitions, helping tools like linters or refactoring utilities detect custom types.",
      "description_length": 261,
      "index": 189,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parsing_hacks",
      "library": "pfff-lang_cpp",
      "description": "Replaces macro-related tokens in a C++ token stream based on predefined macro definitions and the target language dialect. Works with C++ token lists, macro definition hashtables, and language flags to adjust token sequences for correct parsing. Useful for preprocessing C++ code before analysis or transformation passes.",
      "description_length": 321,
      "index": 190,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_views_context",
      "library": "pfff-lang_cpp",
      "description": "This module manages context tags for C++ token views, specifically setting context tags for multi-grouped tokens and identifying strings that resemble typedefs. It operates on lists of `multi_grouped` token structures and string values. Use cases include configuring token contexts during parsing and determining if a token sequence matches a typedef pattern.",
      "description_length": 359,
      "index": 191,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Token_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module defines types to classify C++ preprocessing and language-specific constructs, such as directives, macros, templates, and qualifiers. It provides precise categorization for comment-like elements encountered during C++ parsing. These types help distinguish between different syntactic roles in C++ code, supporting accurate syntax highlighting, analysis, and transformation tools.",
      "description_length": 390,
      "index": 192,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module tests C++ parsing functionalities by tokenizing, parsing, and dumping abstract syntax trees from input files. It handles C++ source code using filename inputs and supports language variant flags during parsing. Use cases include validating parser correctness, inspecting token generation, and analyzing AST structure for specific C++ dialects.",
      "description_length": 355,
      "index": 193,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_views_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module provides operations to construct, deconstruct, and traverse hierarchical token groupings representing C++ syntactic structures like parentheses, braces, and preprocessor conditionals, enabling precise manipulation of nested code elements. It also supports flattening these structured groupings into linear sequences of extended tokens or typed values, facilitating tasks like code analysis, transformation, and serialization through recursive structural decomposition. The flattened representations preserve contextual information while simplifying access to nested content.",
      "description_length": 586,
      "index": 194,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_recovery_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module provides a function `find_next_synchro` that identifies the next synchronization point in a list of C++ tokens by comparing the tokens that have been passed with those remaining. It operates on lists of `Parser_cpp.token` values, analyzing token sequences to recover parsing state after errors. A typical use case involves resynchronizing a parser after a syntax error by locating a known good token boundary in C++ source code.",
      "description_length": 440,
      "index": 195,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parser_cpp_mly_helper",
      "library": "pfff-lang_cpp",
      "description": "This module provides utilities for parsing and transforming C++ declarations and types, including managing type qualifiers, storage classes, inline specifiers, and converting legacy C-style syntax into structured AST components. It constructs function definitions for constructors and destructors by processing parameter lists and exception specifications, operating on AST elements such as declarations, compound statements, and expressions to enforce syntactic and semantic consistency during parsing. Key use cases involve handling complex type declarations, parameter list normalization, and bridging outdated C syntax with modern C++ AST representations.",
      "description_length": 659,
      "index": 196,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Unit_parsing_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module defines a single test value `unittest` that represents a test case for parsing C++ unit declarations. It works with `OUnit.test` structures to validate the correctness of the parsing logic. A concrete use case is verifying that C++ code containing class or function declarations is parsed into the expected abstract syntax tree format.",
      "description_length": 347,
      "index": 197,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Visitor_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module defines a visitor pattern for traversing and transforming C++ abstract syntax trees. It provides a structured way to handle various C++ constructs such as expressions, statements, types, and top-level declarations through customizable callbacks. Concrete use cases include implementing linters, code transformers, or static analysis tools that need to process C++ source code in a structured and extensible manner.",
      "description_length": 426,
      "index": 198,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module processes C++ token streams to perform specific syntactic transformations and analyses. It identifies and modifies templates, constructors, and qualifiers, and reclassifies tokens in contexts like class definitions and typedefs. Concrete use cases include preprocessing code for analysis tools, refactoring support, and custom code generation tasks.",
      "description_length": 361,
      "index": 199,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_cst_cpp",
      "library": "pfff-lang_cpp",
      "description": "Converts C++ concrete syntax trees into OCaml values, specifically transforming `Cst_cpp.program` and `Cst_cpp.any` types into `Ocaml.v` representations. Uses optional precision settings to control dumping detail. Useful for serializing C++ ASTs for analysis or storage.",
      "description_length": 270,
      "index": 200,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Type_cpp",
      "library": "pfff-lang_cpp",
      "description": "Determines whether a given C++ type is a function type or a method type. Works directly with C++ type declarations parsed into the `Cst_cpp.fullType` structure. Useful for analyzing C++ codebases to identify function signatures and member functions during static analysis or refactoring tasks.",
      "description_length": 293,
      "index": 201,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_pp",
      "library": "pfff-lang_cpp",
      "description": "This module processes preprocessor directives and macro-related structures in C++ token streams. It identifies patterns such as `#ifdef` blocks, macro definitions with parentheses, and string macros, while filtering out comments and preprocessor noise. Concrete use cases include analyzing conditional compilation regions and extracting macro initialization patterns.",
      "description_length": 367,
      "index": 202,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module processes C++ source code by breaking it into tokens, handling keywords, characters, strings, comments, and preprocessing directives. It uses lexing buffers to track input state and generates tokens consumed by a parser, with support for error reporting and radix conversion. Concrete use cases include parsing C++ code in a compiler or static analysis tool, extracting lexical elements like identifiers or literals, and managing nested comments or string literals.",
      "description_length": 477,
      "index": 203,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module parses C++ source files into concrete syntax trees and token lists, supporting both standard and fuzzy parsing modes. It handles preprocessing directives, macro definitions, and language-specific variations via explicit configuration. Direct use cases include static analysis of C++ codebases and extracting macro definitions for further processing.",
      "description_length": 361,
      "index": 204,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Pp_token",
      "library": "pfff-lang_cpp",
      "description": "This module processes C++ preprocessor tokens to extract and apply macro definitions. It defines a `define_body` type to represent macro parameters and token sequences, and provides functions to extract macros from token lists and apply macro expansions to parenthesized token groups. It is used to implement macro substitution during C++ code parsing.",
      "description_length": 352,
      "index": 205,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_helpers_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module supports lexical analysis and parsing of C++ code through predicate functions that classify token types such as keywords, operators, comments, and structural elements. It enables inspection of `Parser_cpp.token` values to extract metadata like line numbers and syntactic roles, facilitating tasks such as token filtering, source location tracking, and transformation of token information during compilation workflows.",
      "description_length": 429,
      "index": 206,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_define",
      "library": "pfff-lang_cpp",
      "description": "Replaces or adjusts specific tokens in a list of C++ preprocessor tokens to handle define-related parsing issues. Works directly with lists of `Parser_cpp.token` values, modifying them to correct or normalize define directives. Useful when preprocessing C++ code to ensure correct macro expansion and parsing of defined symbols.",
      "description_length": 328,
      "index": 207,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_lib",
      "library": "pfff-lang_cpp",
      "description": "This module provides functions for manipulating and transforming C++ tokens and comments during parsing, including setting comment kinds, updating token contexts, and modifying token values. It works directly with C++ tokens, extended token views, and parsing contexts to handle low-level adjustments in the AST. Concrete use cases include preprocessing namespace declarations, macro expansions, and comment association in C++ code.",
      "description_length": 432,
      "index": 208,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module defines a token type representing lexical elements of C++ code, including keywords, operators, literals, and preprocessor directives, along with parsing functions that convert token streams into structured C++ syntax trees. It includes entry points for parsing entire programs, top-level declarations, and pattern-matching expressions used in code analysis tools. The module is used to implement precise syntactic analysis of C++ source files, particularly for static analysis, refactoring, and code transformation tasks.",
      "description_length": 533,
      "index": 209,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Flag_parsing_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module handles parsing and configuration of command-line flags specific to C/C++ preprocessing and compilation. It defines language-specific macros, controls verbosity and debugging outputs, and configures preprocessing behaviors like `#ifdef` handling and `#if 0` pass-through. Concrete use cases include enabling C++ parsing, filtering preprocessor messages, and debugging the abstract syntax tree during compilation.",
      "description_length": 424,
      "index": 210,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graphe",
      "library": "pfff-commons-graph",
      "description": "This module offers a comprehensive suite of graph operations for directed structures, enabling vertex and edge manipulation, pathfinding, and structural analysis like transitive closure and strongly connected component detection. It operates on generic graphs with nodes parameterized over arbitrary types, utilizing auxiliary hash tables for node-to-integer or node-to-string mappings to support algorithms such as SCC condensation, depth calculation, and Graphviz-based visualization. Typical applications include dependency resolution, hierarchical layout generation, and analysis of complex network topologies through traversal and transformation utilities.",
      "description_length": 661,
      "index": 211,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Cst_ml",
      "library": "pfff-lang_ml",
      "description": "This module defines core syntactic constructs for representing OCaml code as a concrete syntax tree, including types, expressions, patterns, and top-level items. It works with token-based representations of identifiers, type declarations, module expressions, and program structures, alongside helper functions to extract names, module paths, and associated tokens. It is used for parsing, analyzing, and transforming OCaml source code while preserving lexical information.",
      "description_length": 472,
      "index": 212,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_ml",
      "library": "pfff-lang_ml",
      "description": "This module provides functions to locate and filter source files, including `.ml`, `.mli`, and `.cmt` files, from directories or file lists. It operates on file paths and parses OCaml source structures into token information. Concrete use cases include scanning projects for compilable files and extracting parseable content from OCaml codebases.",
      "description_length": 346,
      "index": 213,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lexer_ml",
      "library": "pfff-lang_ml",
      "description": "This module implements lexical analysis for parsing source code, converting character streams into tokens and handling string literals, comments, and keywords. It operates on `Lexing.lexbuf` input buffers and uses hash tables to map keywords to token types, producing `Parser_ml.token` values. Concrete uses include scanning identifiers, processing quoted strings with escaping, and tracking source location metadata via `Parse_info.t`.",
      "description_length": 436,
      "index": 214,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_ml",
      "library": "pfff-lang_ml",
      "description": "This module defines a visitor pattern for traversing and transforming OCaml abstract syntax trees. It provides entry points for customizing behavior at various AST node types, including expressions, patterns, types, and top-level structures. Concrete use cases include implementing linters, code transformers, or analysis tools that need to inspect or modify OCaml source code at a fine-grained level.",
      "description_length": 401,
      "index": 215,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_parsing_ml",
      "library": "pfff-lang_ml",
      "description": "This module defines a single unit test case using OUnit, providing a structured way to write and run tests for OCaml code. It works with the `OUnit.test` type, which represents individual test cases or test suites. A concrete use case is verifying the correctness of a function's output by defining expected input-output pairs within the `unittest` value.",
      "description_length": 355,
      "index": 216,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_ml",
      "library": "pfff-lang_ml",
      "description": "Parses OCaml source files into tokens and abstract syntax trees, supporting concrete syntax analysis and code transformation tasks. Works with filenames, token streams, and OCaml AST structures. Used for testing parser behavior on real-world OCaml codebases and validating syntax extensions.",
      "description_length": 291,
      "index": 217,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_helpers_ml",
      "library": "pfff-lang_ml",
      "description": "This module provides functions to inspect and transform tokens, including checking token types like EOF or comments, extracting token kinds and source information, and modifying token metadata. It operates on `Parser_ml.token` and `Parse_info.t` data structures. Concrete use cases include token analysis during parsing, source location tracking, and preprocessing transformations.",
      "description_length": 381,
      "index": 218,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_cst_ml",
      "library": "pfff-lang_ml",
      "description": "Converts a concrete syntax tree (CST) node from the `Cst_ml` module into an `Ocaml.v` value, which represents OCaml abstract syntax tree (AST) elements. Works directly with `Cst_ml.any` and `Ocaml.v` types, enabling translation of parsed syntax into a form suitable for further processing or analysis. Useful in scenarios like transforming parsed OCaml code into a structured representation for linters, refactoring tools, or code generators.",
      "description_length": 442,
      "index": 219,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parse_ml",
      "library": "pfff-lang_ml",
      "description": "Parses OCaml source files into concrete syntax trees and token lists. It provides functions to extract the program structure, individual tokens, or both, along with parsing statistics. Useful for static analysis tools, linters, or code transformation utilities that need direct access to OCaml's concrete syntax.",
      "description_length": 312,
      "index": 220,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_ml",
      "library": "pfff-lang_ml",
      "description": "This module defines a comprehensive set of lexical tokens and parsing functions for OCaml source code. It includes detailed token types for literals, identifiers, keywords, operators, and structural symbols, each annotated with parsing metadata. The module provides entry points for parsing OCaml interfaces and implementations into concrete syntax trees, supporting precise syntactic analysis of OCaml programs.",
      "description_length": 412,
      "index": 221,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_cst_js",
      "library": "pfff-lang_js",
      "description": "Converts JavaScript concrete syntax trees (CSTs) into OCaml values for serialization or analysis. Works with `Cst_js.program` and `Cst_js.any` types, handling entire programs or arbitrary CST nodes. Useful for generating OCaml representations of parsed JavaScript code, such as for testing, transformation, or embedding in OCaml tooling.",
      "description_length": 337,
      "index": 222,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parse_js",
      "library": "pfff-lang_js",
      "description": "This module parses JavaScript code into concrete syntax trees and token lists. It provides functions to parse files or strings into programs, tokens, or generic AST nodes, with detailed parsing statistics. Use it to analyze JavaScript source code, extract program structures, or process raw tokens from a given input.",
      "description_length": 317,
      "index": 223,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_js",
      "library": "pfff-lang_js",
      "description": "This module defines a token type representing JavaScript lexical elements, including identifiers, literals, operators, and punctuation, along with parsing functions that convert input streams into abstract syntax trees. It processes JavaScript source code using a lexer to generate tokens and constructs module items or parse patterns for analysis tools. It is used to support static analysis, code transformation, and pattern matching in JavaScript codebases.",
      "description_length": 460,
      "index": 224,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Flag_parsing_js",
      "library": "pfff-lang_js",
      "description": "This module defines mutable flags controlling JavaScript parsing behavior. It includes `jsx`, which enables JSX syntax support, and `debug_asi`, which activates debugging for automatic semicolon insertion. These flags configure low-level parser behavior during JavaScript code analysis.",
      "description_length": 286,
      "index": 225,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Token_helpers_js",
      "library": "pfff-lang_js",
      "description": "This module provides functions to inspect and transform JavaScript tokens, including checking token types like EOF or comments, extracting positional information, and modifying token metadata. It operates directly on `Parser_js.token` and `Parse_info.t` types, enabling precise source location tracking and token property manipulation. Concrete use cases include syntax highlighting, linting rules based on token position, and preprocessing JavaScript code during analysis or transformation pipelines.",
      "description_length": 501,
      "index": 226,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_js",
      "library": "pfff-lang_js",
      "description": "This module tests JavaScript parsing and tokenization functionality. It provides functions to run parsing tests on JavaScript files or directories, validate token generation from a file, and retrieve command-line test actions. It works with file paths, filenames, and command-line interface structures. Use cases include verifying correct parsing of JavaScript syntax trees and ensuring accurate lexical token generation during testing workflows.",
      "description_length": 446,
      "index": 227,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_parsing_js",
      "library": "pfff-lang_js",
      "description": "This module defines a single test value `unittest` that represents a test suite for parsing units in JavaScript. It works with JavaScript string inputs and verifies correct conversion into internal unit representations. Use this to validate unit parsing logic in JavaScript contexts.",
      "description_length": 283,
      "index": 228,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Cst_js",
      "library": "pfff-lang_js",
      "description": "This module defines core syntactic constructs for representing JavaScript code as a concrete syntax tree, including expressions, statements, and program structures. It provides data types like `expr`, `stmt`, and `any` to model JavaScript code elements with precise location information via `tok` and wrappers. Functions like `unwrap`, `unparen`, and `info_of_name` extract components from wrapped values, while utilities like `al_info` and `fakeInfoAttach` manipulate token metadata for analysis and transformation tasks.",
      "description_length": 522,
      "index": 229,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parsing_hacks_js",
      "library": "pfff-lang_js",
      "description": "This module adjusts JavaScript token streams to handle edge cases in parsing. It provides `fix_tokens` and `fix_tokens_ASI`, which modify lists of `Parser_js.token` values to correct syntax issues, particularly around automatic semicolon insertion (ASI). These functions are used when preprocessing JavaScript code to ensure valid parsing, such as in code formatters or linters.",
      "description_length": 378,
      "index": 230,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Visitor_js",
      "library": "pfff-lang_js",
      "description": "This module implements a visitor pattern for traversing and analyzing JavaScript concrete syntax trees. It provides customizable entry points (`kexpr`, `kstmt`, `kprop`, `kinfo`) to process expressions, statements, object properties, and tokens during tree traversal. It is used to extract structured data, perform static analysis, or transform JavaScript source code by visiting specific AST nodes.",
      "description_length": 399,
      "index": 231,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_js",
      "library": "pfff-lang_js",
      "description": "This module identifies JavaScript script files by extension, extracts source files from directories or file lists, and tokenizes abstract syntax tree nodes from parsed JavaScript code. It operates on file paths, directories, and JavaScript AST elements. Concrete use cases include filtering JavaScript files in a project directory, preparing files for analysis, and extracting lexical tokens from parsed code for transformation or inspection tasks.",
      "description_length": 448,
      "index": 232,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_js",
      "library": "pfff-lang_js",
      "description": "This module provides tokenization of input streams, state management via mode stacks for contextual lexing (e.g., code, XHP tags, backquotes), and utilities for string escape processing, regular expression parsing, and nested comment handling. It operates on `Lexing.lexbuf` buffers, uses `Buffer.t` for content accumulation, and leverages a keyword hash table to map identifiers to tokens. Designed for JavaScript-like languages, it supports complex use cases such as parsing embedded XHP tags, multi-stage string interpolation, and stateful lexing of nested syntactic constructs.",
      "description_length": 581,
      "index": 233,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Token_helpers_python",
      "library": "pfff-lang_python",
      "description": "This module provides functions to analyze and transform Python tokens based on their properties. It includes checks for end-of-file, comment, and special tokens, along with utilities to extract and modify token metadata. These operations support tasks like token filtering, metadata enrichment, and preprocessing during Python code parsing.",
      "description_length": 340,
      "index": 234,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lib_parsing_python",
      "library": "pfff-lang_python",
      "description": "Traverses directories and files to collect Python source files. Works with file paths and directory structures. Useful for analyzing or processing Python codebases by gathering relevant files for further operations.",
      "description_length": 215,
      "index": 235,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parser_python",
      "library": "pfff-lang_python",
      "description": "This module defines a token type for parsing Python source code, including lexical elements like keywords, operators, literals, and structural markers such as INDENT and DEDENT. It provides entry points for parsing full programs and sgrep-style patterns, consuming lexbuf input and producing AST nodes. Concrete use cases include building Python code analyzers, linters, and transformation tools that require precise syntactic parsing.",
      "description_length": 435,
      "index": 236,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Ast_python",
      "library": "pfff-lang_python",
      "description": "This module defines a complete abstract syntax tree (AST) for Python, including expressions, statements, types, and program structures. It supports analysis and transformation of Python code by providing detailed node types such as resolved names, expression contexts, and pattern matching via `expr` and `pattern` types. Concrete use cases include static analysis tools, linters, and code refactoring systems that require precise modeling of Python source code.",
      "description_length": 462,
      "index": 237,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_python",
      "library": "pfff-lang_python",
      "description": "This module provides functions to parse Python source files into abstract syntax trees (ASTs) and token lists, supporting direct analysis of Python code structure. It handles operations like full program parsing, token extraction, and string-based AST generation, working with data types such as `Ast_python.program`, `Parser_python.token`, and `Parse_info.parsing_stat`. Concrete use cases include static code analysis, linting, and transformation of Python source files.",
      "description_length": 472,
      "index": 238,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_python",
      "library": "pfff-lang_python",
      "description": "This module provides functions for tokenizing Python source code, including handling string literals (single/double-quoted, long strings, f-strings) with escape sequences and embedded expressions, managing lexer state transitions (mode stacks), and tracking positional context (offsets, newline handling). It operates on `Lexing.lexbuf` input, maintains parsing context through a state object, and generates tokens for a parser. Key use cases include accurately lexing Python's complex string syntax, handling indentation-sensitive parsing via newline tracking, and supporting context-dependent lexing modes.",
      "description_length": 608,
      "index": 239,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Meta_ast_python",
      "library": "pfff-lang_python",
      "description": "Converts Python abstract syntax trees into OCaml values. It provides functions to transform `Ast_python.program` and `Ast_python.any` types into the `Ocaml.v` type. This module is used when embedding Python ASTs into OCaml code, such as during code generation or analysis tools.",
      "description_length": 278,
      "index": 240,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Unit_parsing_python",
      "library": "pfff-lang_python",
      "description": "This module defines a single test value `unittest` of type `OUnit.test`, representing a test case or suite for use with the OUnit testing framework. It is used to structure and execute unit tests written in OCaml, typically by grouping test cases into suites and running them through the OUnit test runner. A concrete use case is defining a collection of assertions to validate the correctness of parsing logic in a Python-related module.",
      "description_length": 438,
      "index": 241,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Test_parsing_python",
      "library": "pfff-lang_python",
      "description": "This module processes Python source files to generate and validate token sequences, ensuring correct lexical analysis. It works with filenames and command-line actions to execute parsing tests and verify output against expected results. Concrete use cases include testing the lexer's ability to handle Python syntax edge cases and verifying tokenization accuracy for specific code examples.",
      "description_length": 390,
      "index": 242,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast_skip",
      "library": "pfff-lang_skip",
      "description": "Converts values of type `Ast_skip.any` to the `Ocaml.v` representation, enabling serialization or further processing of abstract syntax tree nodes that may contain skipped or placeholder elements. Works directly with the `Ast_skip.any` variant and the `Ocaml.v` type for output. Useful when handling incomplete or partially processed OCaml ASTs, particularly during parsing or transformation stages where certain nodes are temporarily skipped.",
      "description_length": 443,
      "index": 243,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lib_parsing_skip",
      "library": "pfff-lang_skip",
      "description": "This module identifies source files in given directories or file lists and extracts parse information from abstract syntax tree nodes. It processes file paths to locate relevant source files and converts AST elements into structured parse data. Useful for analyzing codebases by mapping AST nodes to their source locations.",
      "description_length": 323,
      "index": 244,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_skip",
      "library": "pfff-lang_skip",
      "description": "Parses Skip source files into abstract syntax trees and token streams. Processes input files to extract program structures and lexical tokens, supporting syntax analysis and code transformation tasks. Useful for building compilers, linters, or static analysis tools targeting the Skip language.",
      "description_length": 294,
      "index": 245,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Test_parsing_skip",
      "library": "pfff-lang_skip",
      "description": "This module processes test files to extract and validate token sequences, ensuring correct parsing behavior for skipped content. It operates on filenames and command-line arguments, producing parsed token outputs and actionable commands. Use it to verify how the parser handles ignored or skipped sections in source files during testing workflows.",
      "description_length": 347,
      "index": 246,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parser_skip",
      "library": "pfff-lang_skip",
      "description": "This module defines a lexer and parser for a programming language, handling token types like keywords, identifiers, literals, and operators. It processes input using a lexbuf and generates an abstract syntax tree (AST) for the parsed program. Concrete use cases include parsing source code files or strings into structured AST nodes for further analysis or compilation.",
      "description_length": 369,
      "index": 247,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_skip",
      "library": "pfff-lang_skip",
      "description": "This module defines data types for abstract syntax tree nodes, including tokens, names, and a unit-based program structure. It provides functions to extract string values and token information from name tuples. Useful for parsing and analyzing source code where token metadata is required alongside identifiers.",
      "description_length": 311,
      "index": 248,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_parsing_skip",
      "library": "pfff-lang_skip",
      "description": "This module defines a single unit test that verifies the behavior of a parser when it skips specific input elements. It uses the OUnit testing framework to assert that the parser correctly ignores designated tokens or whitespace during parsing. The test targets concrete parser logic, ensuring that skipped content does not affect the final parsed result.",
      "description_length": 355,
      "index": 249,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_helpers_skip",
      "library": "pfff-lang_skip",
      "description": "This module provides functions to inspect and transform tokens, including checking if a token is an end-of-file or a comment, extracting token kinds and parse info, and applying transformations to token parse info. It operates on `Parser_skip.token` and `Parse_info.t` data types. Concrete use cases include filtering out comments and EOF tokens during preprocessing and adjusting token positions or metadata in a parser pipeline.",
      "description_length": 430,
      "index": 250,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_skip",
      "library": "pfff-lang_skip",
      "description": "This module defines a visitor pattern for traversing and transforming AST nodes in a syntax tree, specifically using the `Ast_skip.tok` type. It provides `default_visitor` as a base implementation and `mk_visitor` to create customized visitors. Concrete use cases include implementing custom logic for skipping or modifying specific nodes during AST traversal.",
      "description_length": 360,
      "index": 251,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_skip",
      "library": "pfff-lang_skip",
      "description": "This module handles lexical analysis tasks for parsing, including token extraction, string and comment processing, and error reporting. It operates on `Lexing.lexbuf` input buffers and produces tokens, strings, and position information. Concrete use cases include reading individual tokens, parsing quoted strings with escaping, and skipping over comments during source code analysis.",
      "description_length": 384,
      "index": 252,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_c",
      "library": "pfff-lang_c-analyze",
      "description": "Builds a graph representation of code dependencies from a directory and file list, supporting Prolog context analysis. Uses hooks to define nodes and edges during graph construction, with optional verbose output. Stores Datalog facts for analysis or export, enabling integration with logic programming tools.",
      "description_length": 308,
      "index": 253,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_analyze_c",
      "library": "pfff-lang_c-analyze",
      "description": "This module defines a function `actions` that returns a list of command-line actions for testing and analyzing C code. It works with the `Common.cmdline_actions` type, which represents executable operations and their associated metadata. Concrete use cases include registering actions like parsing, type checking, or transforming C source files through command-line interfaces.",
      "description_length": 377,
      "index": 254,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Ast_cil",
      "library": "pfff-lang_c-analyze",
      "description": "This module defines core data structures for representing abstract syntax trees in a C-like intermediate language, including variables, lvalues, rvalues, and instructions. It supports operations for constructing and manipulating AST nodes such as assignments, function calls, memory allocations, and array accesses. Concrete use cases include building and transforming program representations during compilation or static analysis tasks.",
      "description_length": 437,
      "index": 255,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Datalog_c",
      "library": "pfff-lang_c-analyze",
      "description": "This module translates C code into Datalog facts by analyzing expressions and instructions to generate corresponding logical assertions. It processes C expressions and top-level definitions to extract facts about variable scopes, types, and control flow, using a mutable environment to track local and global state. Concrete use cases include building analysis tools for program verification and generating logical representations of C functions for static analysis.",
      "description_length": 466,
      "index": 256,
      "embedding_norm": 1.0
    },
    {
      "module_path": "C_to_generic",
      "library": "pfff-lang_c-analyze",
      "description": "Converts C Abstract Syntax Trees (ASTs) into a generic AST representation. It provides functions to transform C-specific AST nodes, such as programs and any node types, into their generic counterparts. This facilitates analysis or processing of C code using tools built for the generic AST format.",
      "description_length": 297,
      "index": 257,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast_cil",
      "library": "pfff-lang_c-analyze",
      "description": "Converts CIL (C Intermediate Language) instructions into OCaml values, specifically translating `Ast_cil.instr` into `Ocaml.v`. This function is used to map low-level CIL operations to higher-level OCaml representations. A typical use case involves processing CIL code during analysis or transformation tasks, such as extracting instruction data for further processing in an OCaml-based toolchain.",
      "description_length": 397,
      "index": 258,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast_java",
      "library": "pfff-lang_java",
      "description": "Converts Java AST nodes into OCaml values, enabling analysis or transformation of Java code within OCaml tools. Works with `Ast_java.any`, a variant representing any Java AST node, and produces `Ocaml.v` values. Useful for building code analyzers, linters, or transformation pipelines that process Java source code.",
      "description_length": 315,
      "index": 259,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_java",
      "library": "pfff-lang_java",
      "description": "This module provides functions to locate Java source files within directories and extract token sequences from abstract syntax trees. It operates on file paths and Java AST nodes, producing lists of filenames and token streams. Useful for analyzing Java codebases by converting source files into processable tokens for further inspection or transformation.",
      "description_length": 356,
      "index": 260,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Token_helpers_java",
      "library": "pfff-lang_java",
      "description": "This module provides functions to inspect and manipulate Java tokens, including checking token types like EOF or comments, extracting token kinds and source information, and transforming token metadata. It operates on `Parser_java.token` and `Parse_info.t` data structures. Use cases include preprocessing Java source code, filtering comments, and analyzing token positions for static analysis or code transformation tools.",
      "description_length": 423,
      "index": 261,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_java",
      "library": "pfff-lang_java",
      "description": "This module defines core data structures for representing Java abstract syntax trees, including types, identifiers, modifiers, and compilation units. It provides functions to manipulate and extract information from AST nodes, such as unwrapping values, checking modifier properties, and generating placeholder tokens. Concrete use cases include parsing Java source files into structured ASTs, analyzing class and method declarations, and supporting transformations or inspections during static analysis or code generation tasks.",
      "description_length": 528,
      "index": 262,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_parsing_java",
      "library": "pfff-lang_java",
      "description": "This module defines a single test case for parsing Java unit files, focusing on validating the structure and content of parsed Java code. It uses OUnit to assert expected outcomes against actual results from the parsing process. The test ensures correctness in handling Java syntax and file formatting during unit parsing.",
      "description_length": 322,
      "index": 263,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_java",
      "library": "pfff-lang_java",
      "description": "This module defines command-line actions for parsing Java test cases. It provides the `actions` function, which returns a list of available command-line operations specific to Java parsing tests. The primary use case is to support command-line interaction for running and managing Java test parsing workflows.",
      "description_length": 309,
      "index": 264,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parser_java",
      "library": "pfff-lang_java",
      "description": "This module defines a comprehensive set of lexical tokens for parsing Java source code, including identifiers, literals, operators, and keywords. It provides entry points for parsing Java programs and patterns used in code matching tools. The module works directly with Lexing.lexbuf input buffers and constructs abstract syntax trees defined in the Ast_java module.",
      "description_length": 366,
      "index": 265,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_java",
      "library": "pfff-lang_java",
      "description": "This module provides functions to parse Java source files and strings into abstract syntax trees (ASTs) and token lists. It supports operations like full program parsing, token extraction, and parsing of arbitrary Java fragments from strings. Concrete use cases include analyzing Java codebases, extracting code structure for refactoring tools, and tokenizing Java input for syntax highlighting or transformation pipelines.",
      "description_length": 423,
      "index": 266,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lexer_java",
      "library": "pfff-lang_java",
      "description": "This module provides functions for lexing Java source code, converting raw input into tokens and handling lexical errors. It processes `Lexing.lexbuf` input, identifies keywords using a hash table, and returns structured tokens with associated parse information. Concrete use cases include parsing Java identifiers, handling comments, and recognizing primitive types during compilation.",
      "description_length": 386,
      "index": 267,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_java",
      "library": "pfff-lang_java",
      "description": "Replaces specific token sequences in Java source code to handle parsing edge cases, such as correcting malformed constructs or disambiguating syntax. Works directly on lists of Java tokens produced by the Java parser. Useful when preprocessing code before analysis or transformation passes that require consistent token structures.",
      "description_length": 331,
      "index": 268,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Visitor_java",
      "library": "pfff-lang_java",
      "description": "This module defines a visitor pattern for traversing and transforming Java abstract syntax trees. It provides functions to create and apply visitors that operate on specific AST nodes like expressions, statements, types, and declarations. Concrete use cases include code analysis, refactoring, and transformation tasks tailored to Java source structures.",
      "description_length": 354,
      "index": 269,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Config_pfff",
      "library": "pfff-config",
      "description": "This module defines configuration parameters such as version, file paths, and logging settings. It uses basic data types like strings and references to manage global or shared state across different components. Concrete use cases include setting up default directories, tracking application version, and enabling optional logging output.",
      "description_length": 337,
      "index": 270,
      "embedding_norm": 0.9999999403953552
    }
  ],
  "filtering": {
    "total_modules_in_package": 328,
    "meaningful_modules": 271,
    "filtered_empty_modules": 57,
    "retention_rate": 0.8262195121951219
  },
  "statistics": {
    "max_description_length": 661,
    "min_description_length": 185,
    "avg_description_length": 382.05535055350555,
    "embedding_file_size_mb": 3.927454948425293
  }
}