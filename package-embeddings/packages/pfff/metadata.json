{
  "package": "pfff",
  "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
  "embedding_dimension": 1024,
  "total_modules": 270,
  "creation_timestamp": "2025-07-16T00:01:18.343124",
  "modules": [
    {
      "module_path": "Graphe",
      "library": "pfff-commons-graph",
      "description": "The module enables graph manipulation through operations like vertex and edge modification, traversal, and structural transformations such as condensation and mirroring. It works with directed graphs where nodes can be of any type (`'a`) and edges have directional relationships, alongside auxiliary structures like hash tables for node metadata. These capabilities support tasks like dependency resolution, hierarchical layout analysis, and visualization-driven debugging of complex networks.",
      "description_length": 493,
      "index": 0,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Config_pfff",
      "library": "pfff-config",
      "description": "This module defines configuration parameters such as version, file paths, and logging settings. It uses basic data types like strings and references to manage global state and options. Concrete use cases include setting up default paths, enabling logging, and referencing standard library modules.",
      "description_length": 297,
      "index": 1,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Visitor_c.Ast_cpp",
      "library": "pfff-lang_c",
      "description": "This module defines visitor methods for traversing and processing C++ abstract syntax trees. It includes functions for handling assignment, unary, binary, and fixity operators during AST traversal. These operations are used to analyze or transform C++ code structures such as expressions and statements.",
      "description_length": 303,
      "index": 2,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_c",
      "library": "pfff-lang_c",
      "description": "Parses C source files into abstract syntax trees and token lists, providing functions to extract typed ASTs from filenames or strings. Works with C ASTs, tokens, and parsing info. Used to analyze C code structure, extract program elements, or build tools like linters and transformers.",
      "description_length": 285,
      "index": 3,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lib_parsing_c",
      "library": "pfff-lang_c",
      "description": "This module identifies C source files in given directories or file lists and extracts parsing information from abstract syntax trees. It processes file paths to locate compilable C files and converts AST nodes into structured parse information. Used for analyzing C codebases and generating metadata for tools like linters or refactoring utilities.",
      "description_length": 348,
      "index": 4,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_c",
      "library": "pfff-lang_c",
      "description": "This module defines core data structures for representing C abstract syntax trees, including types, statements, function definitions, and top-level constructs. It provides operations for working with annotated names, type constructors, control flow statements, and preprocessing directives. Use cases include parsing and analyzing C code, manipulating AST nodes, and implementing C-specific transformations or linters.",
      "description_length": 418,
      "index": 5,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast_c",
      "library": "pfff-lang_c",
      "description": "This module converts C AST nodes into OCaml values, supporting serialization or further processing. It handles C programs, arbitrary AST nodes, and C types through dedicated conversion functions. Useful for generating OCaml representations of C code, such as for analysis tools or code generators.",
      "description_length": 297,
      "index": 6,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_c",
      "library": "pfff-lang_c",
      "description": "The module defines a function `actions` that returns a list of command-line actions for parsing and processing test inputs. It works with string-based command-line arguments and associated execution functions. This module is used to register test cases that parse and evaluate expressions from input files or standard input.",
      "description_length": 324,
      "index": 7,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_c",
      "library": "pfff-lang_c",
      "description": "This module implements the visitor pattern for traversing and transforming C++ abstract syntax trees, allowing users to define custom behavior through callback functions. It provides the `mk_visitor` function to construct visitors that process expressions and token information, operating primarily over the AST structures defined in the `Ast_cpp` submodule. The visitor supports detailed traversal control with specific methods for handling assignment, unary, binary, and fixity operators, enabling precise code analysis or transformation tasks. Example uses include rewriting expressions, collecting variable usage, or enforcing coding standards during AST processing.",
      "description_length": 670,
      "index": 8,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Ast_c_build",
      "library": "pfff-lang_c",
      "description": "Converts parsed C++ syntax trees into their corresponding C abstract syntax representations. It processes `Cst_cpp.program` and `Cst_cpp.any` values, mapping them to equivalent structures in `Ast_c`. This module is used during translation from C++ to C to restructure syntax before code generation.",
      "description_length": 298,
      "index": 9,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_parsing_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module defines a single test value `unittest` of type `OUnit.test`, representing a test case or test suite for use with the OUnit testing framework. It is used to structure and execute unit tests for OCaml code, typically by grouping test cases into suites and running them with OUnit's test runner. The module focuses on providing a concrete test definition that can be integrated into a larger test harness.",
      "description_length": 414,
      "index": 10,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_pp",
      "library": "pfff-lang_cpp",
      "description": "This module processes preprocessor directives and macro definitions in C++ code by analyzing grouped token structures. It identifies conditional compilation blocks, macro initializations, and parenthesized expressions, focusing on patterns like `#ifdef`, `#define`, and string macros. Concrete use cases include parsing header files to extract function declarations under conditional compilation or analyzing macro definitions with complex parenthetical groupings.",
      "description_length": 464,
      "index": 11,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module manipulates C++ token sequences to detect and modify specific syntactic patterns. It identifies constructors, template usages, qualifiers, and object instantiations, often reclassifying tokens to improve parsing accuracy. Designed for preprocessing C++ code, it helps disambiguate complex declarations and template syntax before deeper analysis.",
      "description_length": 357,
      "index": 12,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Flag_parsing_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module handles parsing and configuration of command-line flags specific to C/C++ preprocessing and compilation. It defines language-specific macros, controls verbosity and debugging outputs, and configures preprocessing behaviors such as error handling and conditional compilation. Concrete use cases include enabling C++ debug output, filtering preprocessor messages, and setting macro definitions from command-line arguments.",
      "description_length": 432,
      "index": 13,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module defines a token type representing lexical elements of C++ code, including keywords, operators, literals, and preprocessor directives, along with functions to parse C++ source into concrete syntax trees. It provides entry points for parsing entire programs, top-level declarations, and sgrep/spatch patterns from token streams. Concrete use cases include C++ code analysis, transformation tools, and static inspection utilities that require structured access to parsed C++ constructs.",
      "description_length": 495,
      "index": 14,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module processes C++ source code input using lexing functions to tokenize and extract information. It handles operations like keyword recognition, string and character literal parsing, comment processing, and error handling. Concrete use cases include building a lexer for a C++ parser, analyzing source code structure, and extracting tokens with associated metadata.",
      "description_length": 372,
      "index": 15,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Test_parsing_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module tests C++ parsing functionality by processing source files and validating tokenization, parsing, and AST dumping. It handles operations like tokenizing input files, parsing C++ code into abstract syntax trees, and serializing the results for verification. Use cases include validating the correctness of the C++ parser implementation and debugging parsing issues through structured output.",
      "description_length": 401,
      "index": 16,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Pp_token",
      "library": "pfff-lang_cpp",
      "description": "This module processes C++ preprocessor tokens to extract macro definitions and apply them to token streams. It handles data structures like `define_body`, which represents macro definitions with either function-like parameters or object-like expansions. Key operations include `extract_macros` for collecting macros from tokens and `apply_macro_defs` for expanding macros in parenthesized groups.",
      "description_length": 396,
      "index": 17,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module parses C++ source files into concrete syntax trees and token lists, handling preprocessing directives and macro definitions. It provides functions to extract macros, parse programs into various intermediate representations, and supports fuzzy parsing for error-tolerant analysis. Use cases include static analysis tools, code transformation pipelines, and macro expansion utilities.",
      "description_length": 394,
      "index": 18,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Type_cpp",
      "library": "pfff-lang_cpp",
      "description": "Determines whether a given C++ type is a function type or a method type. Works directly with C++ type declarations parsed into the `Cst_cpp.fullType` structure. Useful for analyzing C++ code to distinguish between function pointers and member function pointers in type definitions.",
      "description_length": 281,
      "index": 19,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_views_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module provides operations for constructing, traversing, and restructuring hierarchical token trees with contextual metadata, as well as converting nested syntactic groupings into flattened sequences or OCaml values. It operates on extended token representations organized by parentheses, braces, and preprocessor directives, enabling use cases like parsing complex C++ constructs (e.g., conditional compilation blocks or nested expressions) and transforming hierarchical token data into linear formats for analysis or code generation.",
      "description_length": 540,
      "index": 20,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_helpers_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module provides predicate functions to classify C++ tokens into categories such as whitespace, comments, preprocessor directives, keywords, and operators, along with utility functions that check token properties, extract positional information, and transform metadata. These operations on `Parser_cpp.token` values enable precise source code analysis, refactoring, and preprocessing tasks by supporting detailed token discrimination and metadata manipulation.",
      "description_length": 464,
      "index": 21,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Cst_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module handles low-level syntactic constructs and transformations in C++ code parsing. It provides utilities to deconstruct and manipulate parsed elements like names, types, and expressions, with functions to extract tokens, remove syntactic noise (like commas and parentheses), and construct identifiers. Concrete use cases include processing C++ declarations, resolving qualified names, and preparing parsed code for further semantic analysis.",
      "description_length": 450,
      "index": 22,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parsing_recovery_cpp",
      "library": "pfff-lang_cpp",
      "description": "`find_next_synchro` takes two lists of C++ parser tokens and identifies the next synchronization point during parsing, returning the synchronized prefix and the remaining tokens. It works by analyzing the `next` tokens in the lookahead and the `already_passed` tokens from prior parsing. This function is used during error recovery in C++ parsing to resynchronize the parser after encountering syntax errors.",
      "description_length": 408,
      "index": 23,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parsing_hacks_lib",
      "library": "pfff-lang_cpp",
      "description": "This module provides functions for manipulating and transforming C++ tokens and comments during parsing, including setting comment kinds, updating token contexts, and modifying token representations. It works directly with C++ tokens, token views, and parsing contexts to handle low-level adjustments needed during preprocessing and parsing stages. It is used for tasks like adjusting macro-related tokens, filtering declarations, and managing namespace-like constructs using regular expressions.",
      "description_length": 496,
      "index": 24,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_cpp",
      "library": "pfff-lang_cpp",
      "description": "Traverses directories and files to collect C++ source file paths. Converts C++ concrete syntax trees into lists of parse information elements. Useful for analyzing C++ codebases by extracting structured data from source files.",
      "description_length": 226,
      "index": 25,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_views_context",
      "library": "pfff-lang_cpp",
      "description": "This module manages context tags for C++ token views by setting multi-grouped context information and identifying potential typedef patterns. It operates on lists of `multi_grouped` structures and string identifiers. Concrete use cases include configuring context-sensitive parsing rules and detecting typedef-like syntax in C++ code during analysis.",
      "description_length": 350,
      "index": 26,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_cst_cpp",
      "library": "pfff-lang_cpp",
      "description": "Converts C++ concrete syntax trees into OCaml values, supporting precise serialization of programs and arbitrary C++ AST nodes. Works with `Cst_cpp.program` and `Cst_cpp.any` types, enabling inspection and transformation of parsed C++ code. Useful for building analysis tools or code generators that operate on C++ source input.",
      "description_length": 328,
      "index": 27,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_define",
      "library": "pfff-lang_cpp",
      "description": "Processes a list of C++ preprocessor tokens to correctly handle and transform define directives, ensuring proper tokenization and structure. It specifically adjusts the token stream to account for macro definitions and their expansions. This function is essential for accurately parsing C++ code that uses complex macro definitions, enabling subsequent analysis or transformation steps.",
      "description_length": 386,
      "index": 28,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_cpp_mly_helper",
      "library": "pfff-lang_cpp",
      "description": "This module includes operations for constructing and modifying C++ declaration records, handling type specifiers, qualifiers, and storage classes, as well as building constructor and destructor definitions from token sequences and parameter lists. It works with AST components like `decl`, `fullType`, `expression`, and `compound`, alongside syntactic elements such as tokens and parameter structures. These functions are used during parsing to transform raw syntax into structured representations, normalize declarations, and manage syntactic edge cases in C++ code.",
      "description_length": 567,
      "index": 29,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_typedef",
      "library": "pfff-lang_cpp",
      "description": "Processes C++ token streams to identify and filter typedef declarations. Works with lists of grouped C++ tokens and extended token information. Extracts typedefs from preprocessed code for analysis or transformation tasks.",
      "description_length": 222,
      "index": 30,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_cpp",
      "library": "pfff-lang_cpp",
      "description": "This module defines a visitor pattern for traversing and transforming C++ abstract syntax trees. It provides a structured way to handle various C++ language constructs such as expressions, statements, types, and top-level declarations through customizable callbacks. The visitor is used to implement analysis, transformation, or code generation tasks over parsed C++ code.",
      "description_length": 372,
      "index": 31,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks",
      "library": "pfff-lang_cpp",
      "description": "Replaces macro-related tokens in a C++ token stream based on provided macro definitions and language flags. Works with C++ tokens, macro definition tables, and language dialects. Useful for preprocessing C++ code by expanding macros during parsing.",
      "description_length": 248,
      "index": 32,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Flag_analyze_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "Controls whether debugging output is enabled for the C++ analysis checker. It provides a mutable reference to a boolean that can be set to enable or disable debug logging during analysis. This flag is used to toggle detailed diagnostic messages when analyzing C++ code.",
      "description_length": 269,
      "index": 33,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Database_light_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "Implements lightweight database initialization by computing and returning a structured database from a list of file paths. Processes each path to extract and organize data into a predefined database schema. Useful for quickly building in-memory databases from static file collections without external dependencies.",
      "description_length": 314,
      "index": 34,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_analyze_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "This module provides a function to highlight C++ source code in a specified file and outputs the result. It also defines command-line actions for interacting with C++ analysis tools. Concrete use cases include syntax highlighting of C++ files and integrating with command-line interfaces for code analysis workflows.",
      "description_length": 316,
      "index": 35,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Builtins_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "Renames standard header files in the specified directory to avoid conflicts. Works with file paths and directories. Useful during C++ project refactoring or integration to manage header file names systematically.",
      "description_length": 212,
      "index": 36,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Check_variables_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "Performs semantic analysis on C++ programs by checking variable declarations and usage. Annotates the program structure with type information and resolves variable scopes. Useful for implementing static analysis tools or preprocessors that require variable tracking in C++ code.",
      "description_length": 278,
      "index": 37,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Highlight_cpp",
      "library": "pfff-lang_cpp-analyze",
      "description": "Processes C++ top-level syntax elements and associated tokens, applying a tagging function to annotate them based on a given highlighting configuration. It works with C++ concrete syntax trees and token lists, using a visitor pattern to traverse and classify top-level declarations and constructs. This module is used to implement syntax highlighting for C++ code by mapping syntactic categories to visual styles or output formats.",
      "description_length": 431,
      "index": 38,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Map_ast",
      "library": "pfff-lang_GENERIC",
      "description": "This module defines a visitor pattern for transforming AST nodes in a structured way, specifically handling expressions, statements, and token information. It allows custom transformation functions to be applied recursively across AST elements while maintaining control flow through visitor methods. Concrete use cases include AST rewriting, static analysis passes, and code transformation tools.",
      "description_length": 396,
      "index": 39,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lib_ast",
      "library": "pfff-lang_GENERIC",
      "description": "Converts abstract syntax tree nodes to position information and strips positional details from nodes, preserving structural integrity. Operates on `Ast.any` values, which represent heterogeneous AST elements. Useful for analyzing or transforming code structures while managing source location metadata.",
      "description_length": 302,
      "index": 40,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_generic",
      "library": "pfff-lang_GENERIC",
      "description": "This module provides functions to parse source code into abstract syntax trees (ASTs) for a specified programming language, supporting both full programs and individual patterns. It operates on file paths and string inputs, producing structured AST representations used for analysis or transformation tasks. Concrete use cases include parsing code files for static analysis, extracting AST nodes from code snippets, and building language-specific tooling like linters or refactoring tools.",
      "description_length": 489,
      "index": 41,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast",
      "library": "pfff-lang_GENERIC",
      "description": "This module defines core data types representing abstract syntax trees, including programs, expressions, statements, identifiers, and names. It provides operations for constructing, traversing, and manipulating AST nodes, enabling analysis and transformation of parsed code. Concrete use cases include implementing linters, code formatters, and static analysis tools that require structured representation of source code.",
      "description_length": 421,
      "index": 42,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast",
      "library": "pfff-lang_GENERIC",
      "description": "Converts a generic AST node into an OCaml value representation. Works with `Ast_generic.any` and `Ocaml.v` types. Useful for serializing AST elements into a format suitable for further processing or storage.",
      "description_length": 207,
      "index": 43,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Find_source",
      "library": "pfff-lang_GENERIC",
      "description": "This module identifies source files in specified directories or paths for a given programming language. It provides functions to list files from a root directory, process a mix of directories and files, and apply a finder function to locate source files. Concrete use cases include scanning project directories for source code files during analysis or build processes.",
      "description_length": 368,
      "index": 44,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Test_parsing_generic",
      "library": "pfff-lang_GENERIC",
      "description": "This module defines a function `actions` that returns a list of command-line parsing actions for test execution. It works with common data types like strings and lists to specify test targets and options. Use it to configure test discovery and execution behavior via command-line arguments.",
      "description_length": 290,
      "index": 45,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_ast",
      "library": "pfff-lang_GENERIC",
      "description": "This module defines a visitor pattern for traversing and transforming abstract syntax trees (ASTs) in OCaml. It provides continuation-based functions for visiting specific AST nodes like expressions, statements, types, patterns, and identifiers, enabling custom processing at each node. It is used to implement analyses, transformations, or pretty-printing of code by hooking into specific AST elements during traversal.",
      "description_length": 420,
      "index": 46,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lang",
      "library": "pfff-lang_GENERIC",
      "description": "This module defines a type representing programming languages and provides functions to convert strings and filenames to language tags. It supports filtering source files by language from a list of paths. Useful for tools that process codebases with multiple language support, such as linters or analyzers.",
      "description_length": 306,
      "index": 47,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lang_fuzzy",
      "library": "pfff-lang_FUZZY",
      "description": "This module defines a sum type representing programming languages such as PHP, ML, Java, JavaScript, and C++. It provides functions to convert strings or filenames into corresponding language variants, enabling language detection based on input patterns. Use cases include determining source code language from file extensions or string identifiers.",
      "description_length": 349,
      "index": 48,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parse_fuzzy",
      "library": "pfff-lang_FUZZY",
      "description": "This module provides functions to parse source files and string patterns into abstract syntax trees using fuzzy parsing techniques. It operates on file paths and string inputs, returning parsed trees along with optional token information. It is used for handling incomplete or imprecise code snippets, supporting tasks like code search and transformation where exact syntax is not required.",
      "description_length": 390,
      "index": 49,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_skip",
      "library": "pfff-lang_skip",
      "description": "This module defines a token type representing lexical elements of a programming language, including identifiers, literals, keywords, and operators, along with their associated source position metadata. It provides a `main` function that processes a lexing buffer using a token-producing function to parse and construct an abstract syntax tree (AST) for a program. Concrete use cases include building parsers for domain-specific languages or implementing compilers that require precise tracking of token positions and types during lexical analysis.",
      "description_length": 547,
      "index": 50,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_skip",
      "library": "pfff-lang_skip",
      "description": "This module defines data types for abstract syntax tree nodes, including tokens, names, and a unit-based program structure. It provides functions to extract string values and token information from name tuples. Concrete use cases include representing and manipulating parsed code elements in a syntax tree.",
      "description_length": 306,
      "index": 51,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_skip",
      "library": "pfff-lang_skip",
      "description": "This module identifies source files in directories or file lists and extracts parse information from abstract syntax tree nodes. It processes file paths to locate relevant source files and converts AST elements into structured parse information. Useful for analyzing codebases by mapping AST nodes to their source locations.",
      "description_length": 324,
      "index": 52,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_skip",
      "library": "pfff-lang_skip",
      "description": "Parses Skip source files into abstract syntax trees and token streams. Processes input files to extract program structures and lexical tokens separately. Useful for static analysis tools needing ASTs or token-based processing.",
      "description_length": 226,
      "index": 53,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_parsing_skip",
      "library": "pfff-lang_skip",
      "description": "This module defines a single test value `unittest` that executes a suite of test cases using the OUnit testing framework. It is used to run predefined unit tests for verifying the correctness of parsing and skipping logic in related modules. The test suite likely includes assertions on input-output behavior of parsing functions, ensuring expected values are returned or errors handled correctly.",
      "description_length": 397,
      "index": 54,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Token_helpers_skip",
      "library": "pfff-lang_skip",
      "description": "This module provides functions to inspect and transform tokens, including checking if a token is an end-of-file or a comment, extracting token kind and parse info, and applying a transformation to a token's parse info. It operates on `Parser_skip.token` and `Parse_info.t` types. Concrete use cases include filtering out comments, identifying token categories during parsing, and modifying token metadata in a syntax tree.",
      "description_length": 422,
      "index": 55,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast_skip",
      "library": "pfff-lang_skip",
      "description": "Converts values of type `Ast_skip.any` to the `Ocaml.v` representation, primarily used for serializing or inspecting skipped AST nodes. Works with the `Ast_skip.any` variant type and the `Ocaml.v` type for output. Useful in scenarios where skipped elements of an abstract syntax tree need to be represented in a structured value form.",
      "description_length": 334,
      "index": 56,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_skip",
      "library": "pfff-lang_skip",
      "description": "Processes OCaml AST nodes with customizable traversal behavior, allowing selective skipping of subtrees during traversal. Works with `Ast_skip.tok` tokens and visitor input/output structures to control navigation through abstract syntax trees. Useful for analyzing or transforming specific parts of OCaml code while bypassing irrelevant sections efficiently.",
      "description_length": 358,
      "index": 57,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_skip",
      "library": "pfff-lang_skip",
      "description": "This module implements lexical analysis operations for parsing source code, handling tasks like token extraction, string and comment processing, and error reporting. It works directly with lexing buffers, string buffers, and custom token and parse info types. Concrete use cases include reading individual tokens, capturing string literals, and skipping or handling comments during parsing.",
      "description_length": 390,
      "index": 58,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_skip",
      "library": "pfff-lang_skip",
      "description": "This module processes test files by parsing their tokens and provides a function to retrieve command-line actions. It operates on filenames and token streams, enabling execution of parsing tasks and integration with command-line interfaces. Use it to analyze test file contents or handle parsing directives during test execution.",
      "description_length": 329,
      "index": 59,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_python",
      "library": "pfff-lang_python",
      "description": "This module tokenizes Python code by converting `lexbuf` input into `Parser_python.token` values, using a stack-based state machine to manage modes like f-strings and triple-quoted strings. It processes string literals (single, double, triple-quoted) with escaping logic, tracks line counts, and maintains lexer state transitions for nested contexts. Useful for parsing Python syntax in tools like interpreters or linters, especially for handling complex string interpolation and multi-line content.",
      "description_length": 499,
      "index": 60,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_python",
      "library": "pfff-lang_python",
      "description": "This module defines a token type representing lexical elements of Python code, including keywords, operators, literals, and structural tokens like indentation. It provides parsing functions `main` and `sgrep_spatch_pattern` that convert a stream of tokens into abstract syntax trees (ASTs) for Python programs or partial code patterns. Concrete use cases include building Python code analyzers, linters, or transformation tools that require precise parsing of Python source files and fragments.",
      "description_length": 494,
      "index": 61,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Token_helpers_python",
      "library": "pfff-lang_python",
      "description": "This module provides functions to analyze and transform Python tokens based on their properties. It includes checks for end-of-file, comment, and special tokens, and allows transforming token metadata using a visitor pattern. These operations are used during parsing to handle token-specific logic and adjustments.",
      "description_length": 314,
      "index": 62,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_python",
      "library": "pfff-lang_python",
      "description": "Traverses directories and files to collect Python source file paths. Works with string lists representing input paths and returns lists of filenames. Useful for batch processing Python codebases, such as in linters or code analyzers.",
      "description_length": 233,
      "index": 63,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_python",
      "library": "pfff-lang_python",
      "description": "This module provides functions to parse Python source files into abstract syntax trees (ASTs) and token lists, supporting precise code analysis and transformation tasks. It handles file-level parsing with detailed error reporting and allows parsing of string inputs into AST elements like programs or generic Python constructs. Concrete use cases include static code analysis, linter development, and programmatic code manipulation based on Python source input.",
      "description_length": 461,
      "index": 64,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Ast_python",
      "library": "pfff-lang_python",
      "description": "This module defines the abstract syntax tree (AST) for Python, including core data types like expressions, statements, and programs. It provides operations to represent and manipulate Python code constructs such as function definitions, control flow, and variable references, with support for resolution tracking and type annotations. Concrete use cases include parsing Python source code, analyzing or transforming code during static analysis, and generating Python code from the AST.",
      "description_length": 485,
      "index": 65,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Test_parsing_python",
      "library": "pfff-lang_python",
      "description": "This module processes Python source files to extract and validate tokens, supporting tasks like syntax analysis and code transformation. It works with filenames and command-line actions, primarily handling string-based data and token streams. A concrete use case includes testing lexical parsing accuracy on Python codebases to ensure correct tokenization before further processing.",
      "description_length": 382,
      "index": 66,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Meta_ast_python",
      "library": "pfff-lang_python",
      "description": "Converts Python abstract syntax trees into OCaml values. It provides functions to transform `Ast_python.program` and `Ast_python.any` types into the `Ocaml.v` type. This module is used for serializing Python ASTs into OCaml representations, enabling further processing or analysis within OCaml tools.",
      "description_length": 300,
      "index": 67,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_parsing_python",
      "library": "pfff-lang_python",
      "description": "This module defines a single test value `unittest` of type `OUnit.test`, representing a test case or test suite for use with the OUnit testing framework. It is used to structure and execute unit tests written in OCaml, typically by combining test cases into a larger test hierarchy. The module is useful for organizing and running automated tests in OCaml projects.",
      "description_length": 365,
      "index": 68,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Highlight_nw",
      "library": "pfff-lang_nw-analyze",
      "description": "Traverses a parsed program and token stream, applying a tagging function to each token based on its syntactic category and the given highlighting preferences. Works with abstract syntax trees and lexer tokens from the Nw dialect. Useful for implementing syntax highlighting in editors or code display tools.",
      "description_length": 307,
      "index": 69,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_helpers_php",
      "library": "pfff-lang_php",
      "description": "This module provides functions to inspect and manipulate PHP tokens, including checking token types like EOF or comments, extracting token kinds and source information, and transforming token metadata. It works directly with `Parser_php.token` and related types such as `Parse_info.token_kind` and `Cst_php.info`. Concrete use cases include filtering comment tokens, retrieving line numbers for diagnostics, and rewriting token content during AST transformations.",
      "description_length": 463,
      "index": 70,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Flag_parsing_php",
      "library": "pfff-lang_php",
      "description": "This module manages parsing behavior and pretty-printing options for PHP code. It provides configuration flags for lexer and parser settings, such as strict mode, short open tags, and XHP extensions, along with functions to control error display and metavariable detection. Concrete use cases include adjusting parsing rules based on PHP dialects and configuring pretty-printer output during command-line processing.",
      "description_length": 416,
      "index": 71,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_php",
      "library": "pfff-lang_php",
      "description": "Adjusts a list of PHP tokens to correct common parsing issues, ensuring proper syntax representation. Works directly with `Parser_php.token` lists, modifying token sequences for accurate parsing. Useful in preprocessing PHP code before analysis or transformation tasks.",
      "description_length": 269,
      "index": 72,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Map_php",
      "library": "pfff-lang_php",
      "description": "This module defines a visitor pattern for transforming PHP concrete syntax trees (CSTs) with specific functions to map expressions, statements, names, class definitions, and tokens. It works with the `Cst_php` module's data types, including `expr`, `stmt`, `name`, `class_def`, and `tok`. Concrete use cases include rewriting PHP code during analysis or transformation passes, such as variable renaming, code simplification, or AST normalization.",
      "description_length": 446,
      "index": 73,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Scope_php",
      "library": "pfff-lang_php",
      "description": "Handles PHP scope representations with a conversion function to string. Works with the `phpscope` type, which wraps a code scope structure. Useful for debugging or logging PHP scope information in a human-readable format.",
      "description_length": 221,
      "index": 74,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_php_mly_helper",
      "library": "pfff-lang_php",
      "description": "This module provides helper functions for parsing PHP code, specifically handling statement lists, parameter validation, and expression construction. It works with PHP concrete syntax trees (`Cst_php`), tokens, and parsing information (`Parse_info`). Functions like `squash_stmt_list` normalize top-level statements, `mk_param` and `mk_var` construct parameter and variable expressions, and validation functions enforce correct parameter list structure during parsing.",
      "description_length": 468,
      "index": 75,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lexer_php",
      "library": "pfff-lang_php",
      "description": "This module implements a state-driven lexer for PHP and XHP syntax, providing operations to manage lexing modes (e.g., embedded XHP tags, nowdocs), generate context-aware tokens (e.g., resolving ambiguous identifiers, handling string interpolations), and process structured literals (heredoc, backquotes, variable offsets). It operates on `Lexing.lexbuf` input buffers, producing `Parser_php.token` values while maintaining mutable state for mode transitions and contextual analysis. Specific use cases include parsing PHP scripts with mixed HTML/XHP content, resolving operator precedence in expressions, and correctly tokenizing complex string literals with embedded variables.",
      "description_length": 679,
      "index": 76,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_php",
      "library": "pfff-lang_php",
      "description": "This module provides functions for parsing and analyzing PHP source code, including identifying PHP files, extracting syntax elements like function calls, variables, and return statements, and processing program structure. It operates on data types such as `Cst_php.any`, `Cst_php.program`, and `Common.filename`, with utilities to retrieve lexical and structural information from PHP code. Concrete use cases include static code analysis, code navigation tools, and extracting metadata from PHP projects.",
      "description_length": 505,
      "index": 77,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_php",
      "library": "pfff-lang_php",
      "description": "This module defines a visitor pattern for traversing and analyzing PHP concrete syntax trees. It provides typed entry points for visiting specific node kinds like expressions, statements, class definitions, function parameters, and XHP elements. Concrete use cases include static analysis tools, linters, and code transformation utilities that need to inspect or modify PHP syntax structures programmatically.",
      "description_length": 409,
      "index": 78,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_php",
      "library": "pfff-lang_php",
      "description": "This module parses PHP code into concrete syntax trees (CSTs) and provides functions to extract tokens, expressions, and programs from files or strings. It supports precise parsing with error recovery, fast parsing, and creation of temporary PHP files for evaluation. Concrete use cases include static analysis tools, code transformation pipelines, and extracting structured data from PHP source files.",
      "description_length": 402,
      "index": 79,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Cst_php",
      "library": "pfff-lang_php",
      "description": "This module enables parsing, transforming, and extracting structured information from PHP concrete syntax trees (CSTs), operating on data structures like identifiers, type hints, function and class nodes, and comma-separated lists. It provides utilities for projecting token details, mapping over nested syntax elements, and converting between identifier representations, which are critical for tasks like source code analysis, codebase refactoring tools, and generating precise syntactic reports. The focus on token-level fidelity and structured traversal makes it particularly suited for applications requiring exact source code reconstruction or detailed AST manipulation.",
      "description_length": 675,
      "index": 80,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Test_parsing_php",
      "library": "pfff-lang_php",
      "description": "This module tests PHP parsing, tokenization, and AST traversal functionalities. It operates on PHP source files, validating correct syntax tree construction and node visiting. Use cases include verifying parser accuracy on specific files and checking token generation consistency.",
      "description_length": 280,
      "index": 81,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_parsing_php",
      "library": "pfff-lang_php",
      "description": "This module defines a single test value that represents a unit test for PHP parsing functionality. It works with the OUnit testing framework to execute and validate test cases. A concrete use case is verifying the correctness of PHP code parsing logic within a larger testing suite.",
      "description_length": 282,
      "index": 82,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unparse_php",
      "library": "pfff-lang_php",
      "description": "Converts PHP concrete syntax trees into string representations. It handles programs, expressions, and arbitrary syntax nodes, preserving structure and content. Useful for generating readable PHP code from parsed ASTs, debugging syntax transformations, or exporting code analysis results.",
      "description_length": 287,
      "index": 83,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_php",
      "library": "pfff-lang_php",
      "description": "This module defines a comprehensive set of lexical tokens representing PHP source code elements, including keywords, operators, literals, and structural symbols. It provides parsing functions that consume lexed input and produce concrete syntax trees (CSTs) for PHP code, supporting both full programs and partial patterns used in code analysis tools. Concrete use cases include building PHP linters, static analyzers, and transformation tools that require precise syntactic representation of PHP code.",
      "description_length": 502,
      "index": 84,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_cst_php",
      "library": "pfff-lang_php",
      "description": "This module converts various PHP concrete syntax tree (CST) elements into OCaml values. It provides functions to transform program structures, expressions, tokens, operators, types, and modifiers from PHP CST representations into OCaml's value type. Use this module when serializing or analyzing PHP code constructs in OCaml-based tooling.",
      "description_length": 339,
      "index": 85,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Pp_php",
      "library": "pfff-lang_php",
      "description": "Adapts tokens from a PHP file by adjusting them to match the original source structure, using a provided tokenizer and the original filename. It works with lists of `Parser_php.token` values, which represent parsed PHP syntax elements. This function is useful when transforming or analyzing PHP code while preserving formatting and structure fidelity.",
      "description_length": 351,
      "index": 86,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Plot_jgraph",
      "library": "pfff-h_visualization",
      "description": "Generates 2D plots from matrix data using jgraph, with customizable axis labels and row/column headers. Operates on matrices of floating-point values and string lists for labeling. Useful for visualizing numerical data grids, such as heatmaps or function evaluations over a grid.",
      "description_length": 279,
      "index": 87,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Treemap",
      "library": "pfff-h_visualization",
      "description": "This module offers operations for generating and visualizing hierarchical data using layout algorithms like Squarified and Classic, transforming directory/file trees into proportional area-based visualizations. It handles screen dimension adaptation, color/size attribute extraction, and supports CLI interactions via predefined commands for inspecting or manipulating treemap instances. Use cases include filesystem hierarchy visualization, disk usage analysis, and interactive exploration of nested data structures through customizable rendering pipelines.",
      "description_length": 558,
      "index": 88,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Figures",
      "library": "pfff-h_visualization",
      "description": "This module defines geometric data structures for working with 2D points and rectangles, both in floating-point and integer pixel formats. It provides operations to calculate dimensions, area, and intersections of rectangles, validate their coordinates, and check if a point lies within a rectangle. Concrete use cases include layout calculations in graphical applications and collision detection in rendering systems.",
      "description_length": 418,
      "index": 89,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Dataflow.VarMap",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module implements ordered associative maps with string keys and arbitrary values, offering creation, modification, and safe querying via operations like `add`, `find`, and `update` with optional return types. It emphasizes ordered traversal, transformations with higher-order functions (`fold`, `filter`), and key-based splitting or range queries, making it suitable for managing hierarchical environments, ordered data analysis, or scenarios requiring precise control over key ordering and conditional transformations.",
      "description_length": 524,
      "index": 90,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dataflow.NodeiSet",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module offers operations for constructing and transforming integer sets through union, intersection, ordered iteration, and comparison-based queries like subset checks and element retrieval (min/max). It supports ordered traversal and sequence integration, enabling use cases like maintaining sorted collections or performing incremental set updates from sequences. The functions emphasize ordered traversal and efficient membership checks, suitable for scenarios requiring ordered data aggregation or comparison-driven processing.",
      "description_length": 536,
      "index": 91,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dataflow.VarSet",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module implements an ordered set abstraction for string elements, offering standard set operations like union, intersection, difference, and comparison, alongside functional transformations via `map`, `fold`, and `filter`. It supports ordered traversal, element selection (min/max), and sequence integration through `add_seq` and `of_seq`, preserving ordered structure during conversions. Designed for scenarios requiring precise variable set manipulation, such as dataflow analysis or dependency tracking, it enables efficient membership queries and ordered collection transformations.",
      "description_length": 591,
      "index": 92,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dataflow",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module orchestrates dataflow analysis by integrating core operations for tracking variables, node indices, and flow graphs with ordered data structures. It combines fixpoint computation and variable mapping manipulation with ordered maps and sets for string and integer keys, enabling precise analysis and transformation of program structures. Use cases include liveness analysis, control flow reconstruction, and forward/backward optimization passes that rely on ordered traversal, set aggregation, and key-based conditional updates. The module supports tasks like merging variable environments, tracking node reachability, and incrementally updating sets of program elements from sequences.",
      "description_length": 697,
      "index": 93,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Controlflow_visitor",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module processes control flow graphs by extracting expressions from nodes and applying transformations across nodes and expressions. It works with control flow nodes, abstract syntax tree expressions, and visitor functions that operate on these structures. Concrete use cases include analyzing or modifying code during static analysis, such as gathering all expressions in a control flow path or applying a transformation to each node and its expressions.",
      "description_length": 460,
      "index": 94,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Controlflow",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module manages control flow graphs using nodes and edges, providing operations to locate specific nodes such as entry and exit points, and to display the graph structure. It works with data types like `node`, `edge`, and `flow`, which represent elements and connections within the graph. Concrete use cases include analyzing program structure by mapping statements to graph nodes and traversing control flow for static analysis or visualization.",
      "description_length": 450,
      "index": 95,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lrvalue",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module analyzes expressions to extract identifiers used as lvalues or rvalues. It provides two functions: `lvalues_of_expr` returns variables assigned in an expression, while `rvalues_of_expr` returns variables read in an expression. Each function processes an abstract syntax tree (AST) expression and returns a list of identifier-name and metadata pairs.",
      "description_length": 361,
      "index": 96,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Normalize_ast",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "Transforms abstract syntax trees by applying normalization rules to ensure consistent structure and representation. Operates on the `Ast.any` type, which encompasses all syntactic constructs in the language. Useful for preparing ASTs for comparison, optimization, or further analysis by eliminating superficial differences.",
      "description_length": 323,
      "index": 97,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_analyze_generic",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module defines a function `actions` that returns a list of command-line operations for analyzing test files. It works with file paths and command-line arguments to perform analysis tasks. Concrete use cases include parsing and processing test files to generate reports or extract metadata.",
      "description_length": 294,
      "index": 98,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Controlflow_build",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "This module constructs control flow graphs from function definitions and statement lists, producing `Controlflow.flow` structures that represent program control flow. It handles AST-generic parameters and statements, and provides detailed error reporting with location information for malformed input. Concrete use cases include analyzing function control flow during static analysis or code transformation tasks.",
      "description_length": 413,
      "index": 99,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dataflow_liveness",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "Performs liveness analysis on control flow graphs using a dataflow framework. It computes which variables are live at each point in the program by propagating liveness information backward through the flow graph. This module is used to determine which variables need to be preserved across function calls or register allocations in a compiler.",
      "description_length": 343,
      "index": 100,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dataflow_reaching",
      "library": "pfff-lang_GENERIC-analyze",
      "description": "Implements a dataflow analysis for computing reaching definitions. It calculates a fixed-point mapping of nodes to sets of reaching definitions using a given flow function. This module is used to determine which variable definitions may reach a given program point in static analysis.",
      "description_length": 284,
      "index": 101,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dependencies_matrix_code",
      "library": "pfff-graph_code",
      "description": "This module provides operations for analyzing and transforming dependency matrices through scoring, filtering, and restructuring based on node relationships. It works with trees, node lists, and array-based matrices to support tasks like computing distances between nodes, identifying parent-child hierarchies, and visualizing dependency patterns in graphs. Specific use cases include refining matrix focus by filtering rows/columns, scoring nodes via ordering, and traversing dependency chains to extract terminal nodes or parent relationships.",
      "description_length": 545,
      "index": 102,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_tags",
      "library": "pfff-graph_code",
      "description": "Extracts definitions and associated tags from a graph code structure, producing a list of filename and tag list pairs. Processes `Graph_code.graph` data to identify and collect tagged elements. Useful for generating tag files or analyzing code dependencies based on graph representations.",
      "description_length": 288,
      "index": 103,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_database",
      "library": "pfff-graph_code",
      "description": "Converts a graph representation into a database structure, mapping nodes and edges to database records. It operates on graph data structures and directory paths, producing a database for persistent storage or querying. Useful for transforming in-memory graph models into a format suitable for database operations.",
      "description_length": 313,
      "index": 104,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Layer_graph_code",
      "library": "pfff-graph_code",
      "description": "Generates heatmap and statistics visualization layers for graph data. It processes a graph structure and node rankings to create a heatmap file, and writes statistics to a file based on a given root directory. Uses standard graph and hash table data structures, and supports command-line interaction through predefined actions.",
      "description_length": 327,
      "index": 105,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dependencies_matrix_build",
      "library": "pfff-graph_code",
      "description": "This module builds and manipulates dependency matrices to optimize the ordering of nodes in a graph based on dependency relationships. It provides functions to construct a dependency matrix from a graph and configuration, analyze row and column counts, and apply sorting and partitioning strategies to minimize dependencies. Concrete use cases include reordering modules in a build system to reduce compilation dependencies and optimizing evaluation order in directed acyclic graphs.",
      "description_length": 483,
      "index": 106,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Graph_code_checker",
      "library": "pfff-graph_code",
      "description": "Performs semantic analysis on a graph representation of code to detect errors. It processes nodes and edges in the graph structure to identify issues such as unreachable code, incorrect node connections, or invalid transitions. Useful for validating control flow graphs in compilers or static analysis tools.",
      "description_length": 308,
      "index": 107,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_class_analysis",
      "library": "pfff-graph_code",
      "description": "Analyzes class hierarchies and method dispatch in a graph representation of code. It constructs a class hierarchy graph, identifies top-level methods per class, and determines potential method dispatch targets. This module is used to detect method overriding and resolve dynamic dispatch in object-oriented code analysis.",
      "description_length": 321,
      "index": 108,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Graph_code_prolog",
      "library": "pfff-graph_code",
      "description": "This module defines a context type for tracking usage edges in a graph during Prolog code generation. It provides functions to hook usage edges based on context and to build a list of Prolog facts from a graph structure. It is used to generate Prolog representations of code graphs, particularly for analyzing or transforming code structures like function calls and assignments.",
      "description_length": 378,
      "index": 109,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_export",
      "library": "pfff-graph_code",
      "description": "Converts a graph data structure into a JSON representation. Works with `Graph_code.graph` and `Json_type.t` types. Useful for exporting graph data to be consumed by external systems or APIs that require JSON format.",
      "description_length": 215,
      "index": 110,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code",
      "library": "pfff-graph_code",
      "description": "This module provides foundational operations for constructing, analyzing, and transforming code graphs, with support for hierarchical organization and file-based mappings. It works with nodes containing metadata like file paths and privacy flags, edges tracking read/write relationships, and auxiliary structures for statistics, whitelists, and symbolic representations. Key use cases include static code analysis, dependency visualization, SCC-based ordering for compilation, and refactoring tools that require precise graph adjustments constrained by whitelists.",
      "description_length": 564,
      "index": 111,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_graph_code",
      "library": "pfff-graph_code",
      "description": "This module defines a function `actions` that returns a list of command-line actions for testing graph-related code. It works with graph data structures and command-line interface components to enable testing of graph algorithms and operations. Concrete use cases include running tests for graph traversal, node manipulation, and edge validation directly from the command line.",
      "description_length": 377,
      "index": 112,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_graph_code",
      "library": "pfff-graph_code",
      "description": "This module defines a single function `unittest` that constructs an OUnit test from a graph-building function. It operates on string inputs to generate graph structures, specifically for testing graph-related functionality. A concrete use case is verifying that a parser correctly converts string representations into expected graph structures in unit tests.",
      "description_length": 358,
      "index": 113,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_helpers",
      "library": "pfff-graph_code",
      "description": "Propagates user information from functions, globals, and types to prototype, extern, and typedef declarations in a graph representation. It operates on a `Graph_code.graph` structure, which models code elements and their relationships. This function is used during code analysis to ensure correct linkage between usage sites and their corresponding declarations in C-like languages.",
      "description_length": 382,
      "index": 114,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Graph_code_opti",
      "library": "pfff-graph_code",
      "description": "This module provides optimized graph representations and operations for efficient node and edge manipulation. It works with a custom graph type that maps nodes to indices, supporting fast lookups and traversal via adjacency lists. Concrete use cases include finding all children of a node, checking node existence, and transforming subgraphs by grouping children under a new node.",
      "description_length": 380,
      "index": 115,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Java_to_generic",
      "library": "pfff-lang_java-analyze",
      "description": "Converts Java abstract syntax trees into a generic AST representation. It handles Java-specific program and any-type nodes, transforming them into their generic counterparts. Useful for cross-language analysis tools that require a unified AST format.",
      "description_length": 250,
      "index": 116,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Highlight_java",
      "library": "pfff-lang_java-analyze",
      "description": "Processes Java source code to apply syntax highlighting by traversing the abstract syntax tree and token list. It uses a provided tag hook to mark up specific elements according to their category, such as keywords or comments. This module is used during the code rendering phase to generate highlighted output for tools like web-based code viewers or documentation generators.",
      "description_length": 376,
      "index": 117,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Package_java",
      "library": "pfff-lang_java-analyze",
      "description": "Performs a lookup in the graph for a fully qualified Java class name, given as a list of package components and a class name. It returns an optional node representing the class if found. Useful for resolving class references in Java codebases during analysis or refactoring tasks.",
      "description_length": 280,
      "index": 118,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Builtins_java",
      "library": "pfff-lang_java-analyze",
      "description": "Extracts files from a list of source directories into a destination directory. Works with directory and file path strings to copy specified files. Useful for bundling Java source files into a single output directory during build processes.",
      "description_length": 239,
      "index": 119,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_analyze_java",
      "library": "pfff-lang_java-analyze",
      "description": "Runs Java unit tests and analyzes their output to verify correctness and code coverage. Works with Java bytecode and test result files to extract metrics like method coverage and exception handling. Useful for integrating test analysis into CI pipelines or assessing test quality in Java projects.",
      "description_length": 297,
      "index": 120,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_java",
      "library": "pfff-lang_java-analyze",
      "description": "Builds a code graph from a list of filenames, using a specified path and verbosity option. It processes source files to construct a graph representation of the code structure. Useful for analyzing dependencies or generating visual representations of project architecture.",
      "description_length": 271,
      "index": 121,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_helpers_js",
      "library": "pfff-lang_js",
      "description": "This module provides functions to inspect and manipulate tokens, including checking token types (EOF, comment), extracting token kind and position information, and transforming token metadata. It operates directly on `Parser_js.token` and `Parse_info.t` types. Concrete use cases include token analysis during parsing, error reporting with precise locations, and preprocessing tokens for further processing stages.",
      "description_length": 414,
      "index": 122,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_js",
      "library": "pfff-lang_js",
      "description": "This module defines a token type representing JavaScript lexical elements, including identifiers, literals, operators, and punctuation, along with parsing functions that process lexed input into structured syntax trees. It includes entry points for parsing entire modules, individual module items, and sgrep/spatch patterns for code analysis and transformation tasks. Concrete use cases include building JavaScript linters, code formatters, and static analysis tools that require precise syntactic representations of JavaScript code.",
      "description_length": 533,
      "index": 123,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Meta_cst_js",
      "library": "pfff-lang_js",
      "description": "This module converts JavaScript concrete syntax trees into OCaml values. It provides functions to serialize entire programs or arbitrary syntax nodes, supporting customizable precision for detailed or summarized output. Useful for analysis tools or code transformation pipelines requiring structured JavaScript representations.",
      "description_length": 327,
      "index": 124,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_js",
      "library": "pfff-lang_js",
      "description": "This module handles JavaScript file detection, source file discovery, and token extraction from abstract syntax trees. It works with file paths, JavaScript CST nodes, and token lists. Use it to filter JavaScript files in a project, collect source files from directories, or extract tokens from parsed JavaScript code for analysis or transformation tasks.",
      "description_length": 354,
      "index": 125,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Cst_js",
      "library": "pfff-lang_js",
      "description": "This module defines core syntactic constructs for representing JavaScript code as a concrete syntax tree, including expressions, statements, and program structures. It provides data types for literals, variables, function and class declarations, operators, and control flow constructs like conditionals and assignments. Operations include unwrapping parenthesized or bracketed expressions and extracting source information from nodes, enabling precise syntax analysis and transformation tasks such as code refactoring, linting, or transpilation.",
      "description_length": 545,
      "index": 126,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_parsing_js",
      "library": "pfff-lang_js",
      "description": "This module defines a single test value `unittest` that represents a test suite for parsing units in JavaScript. It operates on JavaScript string inputs to validate unit parsing logic, ensuring correct conversion and error handling. Concrete use cases include testing edge cases like invalid unit formats and confirming successful parsing of valid unit expressions.",
      "description_length": 365,
      "index": 127,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parsing_hacks_js",
      "library": "pfff-lang_js",
      "description": "This module adjusts JavaScript token streams to handle edge cases in parsing. It provides `fix_tokens` and `fix_tokens_ASI`, which correct token sequences, particularly around automatic semicolon insertion (ASI) and other syntactic ambiguities. These functions are used during preprocessing to ensure valid token streams for subsequent parsing stages.",
      "description_length": 351,
      "index": 128,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parse_js",
      "library": "pfff-lang_js",
      "description": "This module parses JavaScript code into concrete syntax trees and token lists. It provides functions to parse files or strings into programs, tokens, or generic AST nodes, with detailed parsing statistics. Use it to analyze JavaScript source code, extract program structures, or process raw tokens for linting or transformation tasks.",
      "description_length": 334,
      "index": 129,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_js",
      "library": "pfff-lang_js",
      "description": "This module implements a customizable AST traversal mechanism for JavaScript code parsed into a concrete syntax tree (CST). It provides functions to define custom behavior for visiting expressions, statements, object properties, and tokens, using a continuation-based approach. The traversal is controlled through a visitor record that specifies callbacks for different CST node types, enabling precise analysis or transformation of JavaScript code structures.",
      "description_length": 460,
      "index": 130,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Test_parsing_js",
      "library": "pfff-lang_js",
      "description": "This module tests JavaScript parsing and tokenization functionality. It processes JavaScript files by parsing them into abstract syntax trees and generating token streams for analysis. Use cases include validating parser correctness, debugging lexical issues, and ensuring consistent handling of JavaScript codebases.",
      "description_length": 317,
      "index": 131,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Flag_parsing_js",
      "library": "pfff-lang_js",
      "description": "This module defines mutable flags controlling JavaScript parsing behavior, specifically for JSX syntax and automatic semicolon insertion (ASI) debugging. It provides direct access to boolean reference cells that influence the parser's interpretation of source code. These flags are used to toggle support for JSX in JavaScript files and enable diagnostic output for ASI-related issues during parsing.",
      "description_length": 400,
      "index": 132,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lexer_js",
      "library": "pfff-lang_js",
      "description": "This module transforms JavaScript source code into tokens, handling lexical states like code, XHP tags, and backquoted strings while supporting JSX/XHP extensions through specialized parsing functions. It operates on `Lexing.lexbuf` inputs, utilizing `Buffer.t` for intermediate results and recursive state management helpers, with utilities for escape sequences and hexadecimal conversions. Designed for compilers or static analysis tools requiring precise tokenization of JavaScript with embedded markup or complex lexical contexts.",
      "description_length": 534,
      "index": 133,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Int",
      "library": "commons_core",
      "description": "This module defines a type synonym `t` for `int` and provides a `compare` function that orders integers by returning a negative, zero, or positive value based on the relationship between two integers. It is used in contexts requiring type abstraction or standardized comparison, such as building ordered collections or implementing sorting logic.",
      "description_length": 346,
      "index": 134,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Logger",
      "library": "commons_core",
      "description": "This module performs logging operations with a `log` function that takes an optional log level, a message, and an optional output destination. It works directly with strings and string options to handle message formatting and routing. Concrete use cases include writing error messages to stderr, tracing execution flow, and outputting logs to files or system logs.",
      "description_length": 364,
      "index": 135,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Console",
      "library": "commons_core",
      "description": "This module provides functions to execute operations with progress reporting, including wrapping list traversals and custom execution blocks. It works with lists and arbitrary functions that perform side effects, allowing progress indicators to be shown or hidden based on a boolean flag. Concrete use cases include long-running batch processing tasks and user-facing operations where visual feedback is needed.",
      "description_length": 411,
      "index": 136,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "ANSITerminal",
      "library": "commons_core",
      "description": "This module enables precise control over terminal text appearance and display behavior through operations that manipulate color, style, and cursor positioning. It defines structured representations for foreground/background colors and text attributes, while providing imperative actions to modify terminal state, such as rendering styled output, clearing regions, or saving/restoring cursor positions. Typical applications include building command-line interfaces with colored diagnostics, interactive text-based visualizations, or full-screen terminal user interfaces requiring dynamic layout updates.",
      "description_length": 602,
      "index": 137,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Math",
      "library": "commons_core",
      "description": "Contains mathematical constants like `pi` for use in calculations. Works with floating-point numbers for trigonometric and geometric operations. Useful in computations involving circles, angles, and scientific modeling.",
      "description_length": 219,
      "index": 138,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parallel",
      "library": "commons_core",
      "description": "This module provides parallel execution of functions across multiple cores or threads. It supports mapping tasks over lists and job batches, allowing controlled parallelism with explicit task limits. Concrete use cases include accelerating independent computations like file processing, network requests, or numerical calculations in parallel.",
      "description_length": 343,
      "index": 139,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_c",
      "library": "pfff-lang_c-analyze",
      "description": "Processes a directory of source files to construct a directed graph representing code entities and their dependencies. It defines hooks to track node definitions and usage edges during analysis, enabling integration with Prolog-based logic for querying relationships. The module works with graph structures composed of nodes and edges, where nodes represent code elements and edges capture dependencies or usage between them.",
      "description_length": 425,
      "index": 140,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Datalog_c",
      "library": "pfff-lang_c-analyze",
      "description": "This module translates C code into Datalog facts by analyzing expressions and instructions to extract relational data. It processes C AST structures like `expr`, `instr`, and `toplevel` definitions, using an environment to track variable scopes, type information, and generated facts. Concrete use cases include converting function calls and assignments into facts for static analysis or code querying.",
      "description_length": 402,
      "index": 141,
      "embedding_norm": 1.0
    },
    {
      "module_path": "C_to_generic",
      "library": "pfff-lang_c-analyze",
      "description": "Converts C abstract syntax trees to their generic equivalents. Transforms C-specific program and any nodes into a generalized AST representation. Useful for analyzing C code with tools that operate on the generic AST format.",
      "description_length": 224,
      "index": 142,
      "embedding_norm": 0.9999998807907104
    },
    {
      "module_path": "Meta_ast_cil",
      "library": "pfff-lang_c-analyze",
      "description": "Converts CIL intermediate representation instructions into OCaml values. Works with `Ast_cil.instr` and `Ocaml.v` types. Useful for translating low-level program analysis data into a format suitable for further processing or serialization.",
      "description_length": 239,
      "index": 143,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Test_analyze_c",
      "library": "pfff-lang_c-analyze",
      "description": "This module defines a function `actions` that returns a list of command-line actions for testing and analyzing C code. It works with the `Common.cmdline_actions` type to specify operations like parsing, type checking, and code generation. Concrete use cases include setting up test workflows for C analysis tools and integrating C-specific analysis into command-line interfaces.",
      "description_length": 378,
      "index": 144,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_cil",
      "library": "pfff-lang_c-analyze",
      "description": "This module defines core data structures for representing abstract syntax trees in a C-like intermediate language, including variables, lvalues, rvalues, and instructions. It supports operations for constructing and manipulating AST nodes such as assignments, function calls, and memory allocations. Concrete use cases include parsing C code into an intermediate representation and performing static analysis on program structures.",
      "description_length": 431,
      "index": 145,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Unit_analyze_js",
      "library": "pfff-lang_js-analyze",
      "description": "Contains a single unit test for analyzing JavaScript code using OUnit. The test verifies the correctness of parsing and processing JavaScript source files. Useful for validating transformations or linting rules applied to JavaScript codebases.",
      "description_length": 243,
      "index": 146,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Module_path_js",
      "library": "pfff-lang_js-analyze",
      "description": "Resolves a relative or absolute path string into a normalized filename based on the provided root and current working directory. Works with directory and file path strings, handling platform-specific path separators and resolving symbolic links. Useful for safely constructing file paths in cross-platform applications or build tools.",
      "description_length": 334,
      "index": 147,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Graph_code_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module constructs a dependency graph from JavaScript source files, analyzing variable kinds and expressions to determine entity types. It processes directories and file lists into a graph structure, while providing a specialized function to map qualified names to variables for AI tooling. The module handles abstract syntax trees, hashtables, and program representations to support code navigation and analysis workflows.",
      "description_length": 427,
      "index": 148,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Ast_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module defines core data structures for representing JavaScript abstract syntax trees, including expressions, statements, and program elements with associated tokens. It supports analysis and transformation tasks by modeling language features like variables, control flow, and module directives. Concrete use cases include static analysis tools, linters, and code refactoring systems targeting JavaScript.",
      "description_length": 410,
      "index": 149,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Map_ast_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module provides functions for transforming and traversing JavaScript AST nodes using a visitor pattern. It includes utilities for mapping over optional values and scopes, along with a default visitor and a function to create custom visitors. Concrete use cases include rewriting specific expressions or statements in a JavaScript AST during static analysis or code transformation tasks.",
      "description_length": 391,
      "index": 150,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Js_to_generic",
      "library": "pfff-lang_js-analyze",
      "description": "Converts JavaScript AST nodes to their generic AST equivalents. It handles whole programs and individual AST nodes. Useful for analyzing JavaScript code with tools that operate on the generic AST format.",
      "description_length": 203,
      "index": 151,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Comment_js",
      "library": "pfff-lang_js-analyze",
      "description": "Removes leading and trailing comment markers from a string. Works with standard comment syntax like `(* ... *)` or `// ...`. Useful for processing OCaml or C-style comments in source code analysis tools.",
      "description_length": 203,
      "index": 152,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Highlight_js",
      "library": "pfff-lang_js-analyze",
      "description": "Processes JavaScript programs to apply syntax highlighting by traversing the concrete syntax tree and token list. Uses a provided tag hook to associate categories with tokens based on the given preferences. Useful for generating highlighted source code output in editors or documentation tools.",
      "description_length": 294,
      "index": 153,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Transpile_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module translates JavaScript concrete syntax trees (CST) into abstract syntax trees (AST), handling specific constructs like XHP, patterns, variable declarations, and for-of loops. It processes data types such as expressions, names, property names, and statements, converting them into their AST equivalents. Concrete use cases include transforming parsed JavaScript code into a structured AST for further analysis, code generation, or compilation tasks.",
      "description_length": 459,
      "index": 154,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Stdlib_js",
      "library": "pfff-lang_js-analyze",
      "description": "Handles file path resolution and extraction of standard library files. Works with filenames and directory names as strings. Extracts specified source files into a target directory, used for deploying or organizing OCaml standard library files.",
      "description_length": 243,
      "index": 155,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Annotation_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module parses JavaScript programs to extract specific annotations like `ProvidesModule`, `ProvidesLegacy`, and `RunWhenReady` from comments. It processes programs and strings to produce lists of typed annotations paired with source location metadata. Useful for analyzing JavaScript modules and legacy code to determine load behavior and dependencies.",
      "description_length": 356,
      "index": 156,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Utils_js",
      "library": "pfff-lang_js-analyze",
      "description": "Converts JavaScript AST nodes to their string representation and loads files with a deferred execution mechanism. Works with JavaScript AST structures and file paths. Useful for debugging AST transformations and safely reading file contents in a controlled context.",
      "description_length": 265,
      "index": 157,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Database_light_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module computes a database structure from a list of file paths, using a specified verbosity flag to control output during processing. It operates on `Common.path` lists and produces a `Database_code.database` value, which represents the processed data. A typical use case involves initializing a database from source files, where the input paths point to JSON or similar structured data files.",
      "description_length": 398,
      "index": 158,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Module_pre_es6",
      "library": "pfff-lang_js-analyze",
      "description": "This module defines a hierarchy of shape types to represent JavaScript module structures, including objects, arrays, functions, classes, and mixins, with support for properties, inheritance, and dynamic construction. It provides operations to create and manipulate these shapes, such as `new_object` for object creation, `new_class` for class structures, and `string_of_shape` for debugging output. It is used to model JavaScript module dependencies and bindings, particularly for static analysis or transformation tasks.",
      "description_length": 521,
      "index": 159,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Visitor_ast_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module implements a visitor pattern for traversing and analyzing JavaScript abstract syntax trees. It provides hooks to process specific AST nodes like expressions, statements, top-level elements, properties, parameters, and tokens, enabling targeted inspection or transformation of code structure. Typical use cases include static analysis tools, linters, or code refactoring utilities that need to traverse and act on specific parts of a JavaScript program's syntax tree.",
      "description_length": 478,
      "index": 160,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Ast_js_build",
      "library": "pfff-lang_js-analyze",
      "description": "Converts concrete syntax trees (CSTs) into abstract syntax trees (ASTs) for JavaScript programs and expressions. It processes `Cst_js.program` and `Cst_js.any` inputs, transforming them into their corresponding `Ast_js` representations. This module is used during parsing to build structured, type-safe ASTs from raw CST nodes.",
      "description_length": 327,
      "index": 161,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Class_pre_es6",
      "library": "pfff-lang_js-analyze",
      "description": "Extracts fully qualified names from a JavaScript AST, mapping tokens to entity kinds and names. Works with CST (Concrete Syntax Tree) representations of JavaScript programs. Useful for analyzing or transforming JavaScript code by identifying named entities like functions, variables, and classes.",
      "description_length": 296,
      "index": 162,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lib_analyze_js",
      "library": "pfff-lang_js-analyze",
      "description": "Transforms JavaScript AST nodes by abstracting position information and extracts parse info from any AST element. Works with `Ast_js.any` and `Parse_info.t` types. Useful for analyzing or manipulating JavaScript code while tracking source locations.",
      "description_length": 249,
      "index": 163,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_analyze_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module defines a function `actions` that returns a list of command-line operations for analyzing JavaScript test files. It works with JavaScript ASTs and file paths to perform analysis tasks like detecting test patterns or linting. Concrete use cases include integrating with command-line tools for static analysis of JavaScript test suites.",
      "description_length": 346,
      "index": 164,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Tags_js",
      "library": "pfff-lang_js-analyze",
      "description": "Converts a list of file or directory paths into a list of filenames paired with their associated tags by parsing each file's contents. Works with lists of strings representing file paths and returns tuples of filenames and tag lists. Useful for extracting structured metadata from source files, such as identifying TODOs or specific annotations in codebases.",
      "description_length": 358,
      "index": 165,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast_js",
      "library": "pfff-lang_js-analyze",
      "description": "This module facilitates the conversion of JavaScript AST nodes\u2014including expressions, statements, variables, functions, classes, properties, and program structures\u2014into generic OCaml values (`Ocaml.v`) through a serialization process. It maps structured syntax elements to a uniform representation, enabling interoperability with OCaml runtime systems or static analysis tools. Use cases include marshaling JavaScript code for OCaml-based processing, transforming ASTs for compiler pipelines, or analyzing JavaScript programs within an OCaml ecosystem.",
      "description_length": 552,
      "index": 166,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Resolve_python",
      "library": "pfff-lang_python-analyze",
      "description": "Performs name resolution on a Python abstract syntax tree, ensuring all variable and function references are correctly bound to their definitions. It processes the program structure to validate scope and linkage, preparing it for further analysis or execution. Useful for static analysis tools and interpreters handling Python code.",
      "description_length": 332,
      "index": 167,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Highlight_python",
      "library": "pfff-lang_python-analyze",
      "description": "Processes Python programs to apply syntax highlighting by traversing the abstract syntax tree and token list. It uses a provided tag hook to mark up specific syntactic categories according to given highlighting preferences. This function is suitable for integrating into code editors or formatters that require precise Python syntax visualization.",
      "description_length": 347,
      "index": 168,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Python_to_generic",
      "library": "pfff-lang_python-analyze",
      "description": "Converts Python abstract syntax trees into a generic AST representation. It transforms Python-specific AST nodes into their generic counterparts using the `program` and `any` functions. This module is used when analyzing or processing Python code within a language-agnostic toolchain.",
      "description_length": 284,
      "index": 169,
      "embedding_norm": 0.9999998807907104
    },
    {
      "module_path": "Simple_format",
      "library": "pfff-h_files-format",
      "description": "This module processes text files by filtering out comment lines and parsing structured content. It provides a regular expression for identifying comment lines, reads and filters comments from a file, and parses title-colon-value entries into structured data. It is used for extracting configuration or metadata from human-readable files.",
      "description_length": 337,
      "index": 170,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Outline",
      "library": "pfff-h_files-format",
      "description": "Handles parsing and writing hierarchical outline documents using a tree structure. It supports operations to identify root nodes, apply custom regex patterns for parsing, and persist outlines to files. This module is used to process structured text files like markdown-like outlines with titles and stars.",
      "description_length": 305,
      "index": 171,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Source_tree",
      "library": "pfff-h_files-format",
      "description": "Handles tree structure transformations by mapping directories to subsystems. It loads and applies reorganization rules from a file, updating directory-to-subsystem assignments. Useful for refactoring project directories into modular subsystems.",
      "description_length": 244,
      "index": 172,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_parsing_java",
      "library": "pfff-lang_java",
      "description": "This module defines a single OUnit test case for parsing Java source code into abstract syntax trees. It validates correctness by comparing parsed output against expected AST structures for specific Java code inputs. The test targets edge cases like nested classes, method overloading, and exception handling in Java.",
      "description_length": 317,
      "index": 173,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Visitor_java",
      "library": "pfff-lang_java",
      "description": "This module defines a visitor pattern for traversing and transforming Java abstract syntax trees. It provides functions to customize behavior at various AST nodes like expressions, statements, types, and class declarations through a `visitor_in` record. Each field in the record corresponds to a specific AST element, enabling precise manipulation or analysis during traversal.",
      "description_length": 377,
      "index": 174,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_java",
      "library": "pfff-lang_java",
      "description": "This module defines core data structures for representing Java abstract syntax trees, including types, identifiers, modifiers, and compilation units. It provides functions to manipulate and extract information from AST elements, such as unwrapping annotated values and checking modifier properties. Concrete use cases include parsing Java source code, analyzing class hierarchies, and extracting metadata from Java programs.",
      "description_length": 424,
      "index": 175,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_helpers_java",
      "library": "pfff-lang_java",
      "description": "This module provides functions to inspect and manipulate Java tokens, including checking token properties like EOF or comment status, extracting token kinds and parse information, and transforming token metadata. It operates directly on `Parser_java.token` and `Parse_info.t` types. Concrete use cases include analyzing token structure during parsing, filtering comments, and adjusting token positions or attributes in preprocessing tasks.",
      "description_length": 439,
      "index": 176,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast_java",
      "library": "pfff-lang_java",
      "description": "Converts Java AST nodes into OCaml values, enabling analysis or transformation of Java code within OCaml tools. Works with `Ast_java.any`, a variant representing any Java AST node, and produces `Ocaml.v` values for further processing. Useful for building code analyzers, linters, or transpilers that operate on Java source code.",
      "description_length": 328,
      "index": 177,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_java",
      "library": "pfff-lang_java",
      "description": "This module defines command-line actions for parsing and processing Java test files. It provides a function `actions` that returns a list of available command-line operations, such as running tests or generating test reports. The module works directly with Java source files and test configurations, enabling concrete use cases like executing unit tests or analyzing test coverage from the command line.",
      "description_length": 403,
      "index": 178,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parse_java",
      "library": "pfff-lang_java",
      "description": "This module provides functions to parse Java source files and strings into abstract syntax trees (ASTs) and token lists. It supports operations like parsing entire programs, extracting tokens, and converting strings into AST elements. Concrete use cases include analyzing Java code structure, performing static analysis, and building tools that process Java source files.",
      "description_length": 371,
      "index": 179,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parser_java",
      "library": "pfff-lang_java",
      "description": "This module defines a token type representing lexical elements of Java code, including keywords, operators, literals, and structural symbols, each annotated with parsing metadata. It provides entry points for parsing Java source code into an abstract syntax tree (`goal`) and for parsing code fragments used in pattern matching (`sgrep_spatch_pattern`). Concrete use cases include static analysis tools, code transformation utilities, and syntactic pattern search in Java projects.",
      "description_length": 481,
      "index": 180,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parsing_hacks_java",
      "library": "pfff-lang_java",
      "description": "Replaces specific Java tokens in a list to correct parsing issues, such as adjusting malformed or ambiguous lexical elements. Works directly with lists of Java tokens produced by the Java parser. Useful when handling legacy or non-standard Java codebases where the default lexer produces incorrect or inconsistent tokens.",
      "description_length": 321,
      "index": 181,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_java",
      "library": "pfff-lang_java",
      "description": "This module provides functions to locate Java source files within directories or file lists and to extract token sequences from abstract syntax trees. It operates on file paths and Java AST nodes, producing lists of filenames and token streams. It is used for analyzing Java codebases by converting syntactic elements into processable token sequences.",
      "description_length": 351,
      "index": 182,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_java",
      "library": "pfff-lang_java",
      "description": "This module provides functions for lexing Java source code, converting raw input into tokens and handling lexical errors. It processes `lexbuf` structures to extract tokens, comments, and associated metadata, using a keyword table and internal lexing tables for efficient recognition. Concrete use cases include parsing Java identifiers, primitive types, and comments during the initial stages of compilation or static analysis.",
      "description_length": 428,
      "index": 183,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_lisp",
      "library": "pfff-lang_lisp",
      "description": "This module implements a lexer for parsing Lisp-like syntax, converting input text into tokens and handling string literals. It processes `Lexing.lexbuf` input, producing `Parser_lisp.token` values and managing internal parsing state through recursive helper functions. Concrete use cases include reading and tokenizing Lisp expressions from a file or interactive input.",
      "description_length": 370,
      "index": 184,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_lisp",
      "library": "pfff-lang_lisp",
      "description": "This module defines a token type for parsing Lisp-like syntax, including identifiers, numbers, strings, and delimiters with associated source information. It provides functions to inspect tokens, such as checking if a token is a comment or the end of input, and extracting source info or string representations. Use cases include lexical analysis for Lisp interpreters or tools processing s-expressions with precise source tracking.",
      "description_length": 432,
      "index": 185,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_lisp",
      "library": "pfff-lang_lisp",
      "description": "This module defines a Lisp-like abstract syntax tree with support for parsing and manipulating S-expressions. It includes operations for constructing and deconstructing symbolic expressions, handling atoms, special forms, and parenthesized lists. Concrete use cases include building interpreters or compilers for Lisp dialects and processing structured textual data.",
      "description_length": 366,
      "index": 186,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_parsing_lisp",
      "library": "pfff-lang_lisp",
      "description": "Traverses directories and files to collect source file paths. Works with lists of file and directory paths, filtering to return only file paths. Useful for gathering all source files in a project directory for processing or analysis.",
      "description_length": 233,
      "index": 187,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_lisp",
      "library": "pfff-lang_lisp",
      "description": "This module parses Lisp programs into abstract syntax trees and token lists. It provides functions to parse a file into a program and tokens, extract just the program, or retrieve the list of tokens. Use cases include analyzing Lisp code structure, tokenizing input for further processing, and building ASTs for interpretation or compilation.",
      "description_length": 342,
      "index": 188,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_lisp",
      "library": "pfff-lang_lisp",
      "description": "This module processes Lisp-like syntax files by tokenizing their contents and validating the structure against expected patterns. It operates on filenames and command-line actions, producing parsed tokens and executable commands. Use it to test syntax correctness of Lisp files or to generate command sequences from test inputs.",
      "description_length": 328,
      "index": 189,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Unit_program_lang",
      "library": "pfff-h_program-lang",
      "description": "This module defines a single unit test value that executes a suite of tests for a programming language interpreter or compiler. It uses the OUnit testing framework to structure and run test cases, focusing on validating core language operations and edge cases. Concrete use cases include testing expression evaluation, type checking, and runtime behavior of language constructs.",
      "description_length": 378,
      "index": 190,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_ast_fuzzy",
      "library": "pfff-h_program-lang",
      "description": "Converts a `trees` value from the `Ast_fuzzy` module into an `Ocaml.v` value, representing OCaml abstract syntax trees in a structured form. Works with `Ast_fuzzy.trees` and `Ocaml.v` data types. Useful for transforming parsed OCaml code into a format suitable for further analysis or manipulation.",
      "description_length": 298,
      "index": 191,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_fuzzy",
      "library": "pfff-h_program-lang",
      "description": "This module represents a simplified abstract syntax tree (AST) structure for fuzzy parsing, supporting nodes like parentheses, braces, angle brackets, and brackets with associated tokens. It includes operations to identify metavariables and handles token-wrapped strings, either as literals or placeholders like dots. Concrete use cases include building and analyzing incomplete or approximate code structures during parsing or transformation tasks.",
      "description_length": 449,
      "index": 192,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Error_code",
      "library": "pfff-h_program-lang",
      "description": "This module enables creating and managing errors and warnings with location tracking, severity levels, and path adjustments. It works with error data structures, token locations, and filename mappings to support tasks like capturing exceptions during parsing, comparing actual errors against expected line numbers, and filtering errors based on severity or context. Key patterns include exception handling with file context and using hash tables to map identifiers to error locations for precise reporting.",
      "description_length": 506,
      "index": 193,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Datalog_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines a set of fact types representing pointer analysis and program analysis relationships, such as variable-to-heap assignments, field accesses, and function calls. It provides functions to convert facts to string representations, output facts in bddbddb format for analysis, and generate explanations for tuples in bddbddb output. These operations support concrete use cases in static analysis, such as tracking memory references, analyzing control flow, and explaining data flow relationships in programs.",
      "description_length": 522,
      "index": 194,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Layer_coverage",
      "library": "pfff-h_program-lang",
      "description": "This module converts line coverage data into visualization layers and provides command-line actions for processing. It transforms `Coverage_code.lines_coverage` into red-green or heatmap layers for display. Use cases include generating visual coverage reports from test execution data.",
      "description_length": 285,
      "index": 195,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Coverage_code",
      "library": "pfff-h_program-lang",
      "description": "This module handles serialization and storage of test and line coverage data. It works with associative lists mapping filenames to coverage scores or detailed line coverage information. Functions convert coverage data to and from JSON, and persist them to disk through dedicated loading and saving operations.",
      "description_length": 309,
      "index": 196,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Layer_code",
      "library": "pfff-h_program-lang",
      "description": "This module manages layered data visualizations by defining structures for organizing files, metadata, and color-coded properties. It supports operations to build, filter, and analyze layers based on file information and categorized kinds, with concrete use cases in generating heat maps and red-green property visualizations. Functions include loading and saving layers, constructing indexed views, and converting layers to and from JSON for serialization.",
      "description_length": 457,
      "index": 197,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Comment_code",
      "library": "pfff-h_program-lang",
      "description": "This module handles the association of comments with code elements in a parsed AST. It provides functions to determine the position of comments relative to code elements, specifically identifying comments that appear before or after a given element. It operates on parsed code elements and comment data structures, using hook functions to navigate and link comments to the appropriate nodes in the AST. Use cases include formatting code with preserved comment placement and analyzing comment coverage in source files.",
      "description_length": 517,
      "index": 198,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Archi_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines a comprehensive set of source architecture categories and source kind variants to classify code components. It includes functions to convert architecture categories to strings and to detect duplicate directory names in a given path. Concrete use cases include organizing and analyzing large codebases by architectural layer or source type, and ensuring directory naming consistency during project setup or refactoring.",
      "description_length": 438,
      "index": 199,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_program_lang",
      "library": "pfff-h_program-lang",
      "description": "Analyzes program layer statistics from a given file and defines command-line actions for testing. Works with filenames and command-line interfaces. Useful for executing test workflows and gathering structural metrics from program files.",
      "description_length": 236,
      "index": 200,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Info_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines an `info_txt` type as an alias for `Outline.outline` and provides the `load` function to read and parse a file into this structure. It works with file paths and outline data, typically used to extract structured information from text files. A concrete use case is loading documentation or configuration outlines for further processing or display.",
      "description_length": 366,
      "index": 201,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_ast_fuzzy",
      "library": "pfff-h_program-lang",
      "description": "This module constructs and manipulates abstract syntax trees and token lists from token streams using customizable hooks. It processes concrete syntax trees into a fuzzy AST representation, enabling structural transformations and token extraction. Concrete use cases include parsing incomplete or malformed code, analyzing code structure with partial information, and building tools like code matchers or refactoring engines that tolerate syntax imperfections.",
      "description_length": 460,
      "index": 202,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Overlay_code",
      "library": "pfff-h_program-lang",
      "description": "This module manages file path mappings between original and overlay directories using hash tables and lists. It provides functions to create, load, save, and validate overlays, as well as adapt layers and databases to use overlay paths. Concrete use cases include redirecting file accesses during testing, managing configuration overlays, and adapting build artifacts to different directory structures.",
      "description_length": 402,
      "index": 203,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Highlight_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines a rich set of syntax highlighting categories and associated metadata for source code visualization. It includes functions to map categories, entity kinds, and usage definitions to visual styles such as foreground/background colors, font weights, and text decorations. These are used to generate syntax highlighting rules in code editors or static analysis tools, particularly for languages with complex type systems and macro-based syntax.",
      "description_length": 459,
      "index": 204,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Pleac",
      "library": "pfff-h_program-lang",
      "description": "This module processes code excerpts and skeletons to generate structured source files. It parses data files into sections of code, reads skeleton files defining output structure, and detects comment styles for wrapping generated code. It supports use cases like generating boilerplate code from templates or organizing code snippets into modular files with consistent formatting.",
      "description_length": 379,
      "index": 205,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Meta_parse_info",
      "library": "pfff-h_program-lang",
      "description": "This module converts parse information into a structured format using customizable precision settings. It operates on `Parse_info.t` data, allowing selective inclusion of full, token, or type information via the `dumper_precision` type. Use it to serialize parse details for debugging or analysis with varying levels of detail.",
      "description_length": 327,
      "index": 206,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Scope_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines a set of scope identifiers used to represent variable binding contexts in a compiler or interpreter. It includes functions to convert scope values to string representations and to map them to a specific value type used elsewhere in the system. Concrete use cases include tracking variable visibility during code analysis and generating appropriate bindings during compilation.",
      "description_length": 396,
      "index": 207,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Big_grep",
      "library": "pfff-h_program-lang",
      "description": "This module provides functions to build and query an index of entities within a large string, supporting both case-sensitive and case-insensitive searches. It uses a hash table to map positions in the string to entities, enabling efficient lookups. Concrete use cases include searching for the top N matching entities based on a query string, such as finding the most relevant code symbols in a large codebase.",
      "description_length": 410,
      "index": 208,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Archi_code_parse",
      "library": "pfff-h_program-lang",
      "description": "Parses architectural descriptions from files, converting them into structured source architecture representations. Operates on directory paths and file names to extract and model codebase structure. Useful for analyzing and visualizing dependencies and components in large OCaml projects.",
      "description_length": 288,
      "index": 209,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Archi_code_lexer",
      "library": "pfff-h_program-lang",
      "description": "This module defines low-level lexical analysis operations for parsing source code, specifically handling token categorization and state transitions. It works with `Lexing.lexbuf` input buffers and produces `source_archi` values representing categorized code elements. It is used to implement custom lexing rules for domain-specific syntax, such as parsing architectural descriptions in a code analysis tool.",
      "description_length": 407,
      "index": 210,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_generic",
      "library": "pfff-h_program-lang",
      "description": "This module provides core types and utilities for constructing and manipulating abstract syntax trees, including token, identifier, and resolved name representations, along with structural transformations for converting statements to fields, extracting definitions, and performing pattern-based checks on operators and expressions. It operates on AST elements like statements, fields, items, and arithmetic operators, enabling tasks such as name resolution, code analysis, and syntactic transformations during compilation or static analysis.",
      "description_length": 541,
      "index": 211,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Flag_parsing",
      "library": "pfff-h_program-lang",
      "description": "This module manages parsing and lexing behavior through mutable flags that control verbosity, error handling, and debugging. It provides functions to configure command-line options for tools that require fine-grained control over parsing processes. These flags are used to enable or disable features like error recovery, lexical error exceptions, and debug output during parsing.",
      "description_length": 379,
      "index": 212,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parse_info",
      "library": "pfff-h_program-lang",
      "description": "This module provides utilities for tracking source code positions and transformations during parsing, focusing on token metadata, error reporting, and position conversion. It operates on lex buffers, token lists, and change-tracking records like `changen`, enabling precise line/column number calculations and handling synthetic tokens. Key applications include parsing state management, file change tracking, and robust error diagnostics tied to source locations.",
      "description_length": 464,
      "index": 213,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Refactoring_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines data types to represent specific code refactoring operations, such as adding or removing interfaces, splitting members, and modifying type hints, along with their positions in source files. It includes a function to load a list of these refactoring actions from a file. Use cases include automating code transformations and applying structured edits during static analysis or codebase migrations.",
      "description_length": 416,
      "index": 214,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Meta_ast_generic_common",
      "library": "pfff-h_program-lang",
      "description": "This module converts arithmetic operators and increment/decrement expressions from a generic AST representation into OCaml values. It handles specific data types like `arithmetic_operator`, `incr_decr`, and tuples of `incr_decr` with `prefix_postfix`. Use cases include serializing AST nodes for analysis or transformation tools.",
      "description_length": 329,
      "index": 215,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lib_unparser",
      "library": "pfff-h_program-lang",
      "description": "This module processes lists of token-kind and parse-info pairs, transforming them into a structured format that represents original, removed, added, or esthetic elements. It provides functions to filter and modify these elements, such as removing esthetic elements between removed tokens or dropping lines that contain only removed tokens. It is used to generate readable string outputs from tokenized source code, particularly during code transformation or diff rendering tasks.",
      "description_length": 479,
      "index": 216,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Entity_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines types and conversions for representing code entities and their properties. It includes operations to convert entity kinds to and from strings, and a list of properties that capture characteristics like dynamic calls, global usage, and privacy. It is used to analyze and annotate code elements with metadata during static analysis or code processing tasks.",
      "description_length": 375,
      "index": 217,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Prolog_code",
      "library": "pfff-h_program-lang",
      "description": "This module defines a set of fact types representing program analysis data, such as entity locations, kinds, types, and relationships like inheritance or usage. It includes functions to convert facts to string representations and to parse entities from strings. It is used to encode and manipulate structured program analysis information in a Prolog-like format.",
      "description_length": 362,
      "index": 218,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Skip_code",
      "library": "pfff-h_program-lang",
      "description": "This module processes skip lists to filter and reorder files based on directory and file exclusion rules. It handles data types like `skip` that represent directories, files, and error skips, and operates on lists of filenames and directories. It is used to exclude specific files or directories from processing, reorder files to handle skip errors last, and filter files based on a skip list.",
      "description_length": 393,
      "index": 219,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Database_code",
      "library": "pfff-h_program-lang",
      "description": "This module manages a database of code entities, each identified by an integer ID and containing metadata such as name, file location, and usage statistics. It supports loading and saving databases to disk, merging multiple databases, and building sorted entity lists per file for efficient lookup. Key operations include adjusting external user counts, determining entity-highlight category correspondences, and generating completion suggestions based on file and directory structures.",
      "description_length": 486,
      "index": 220,
      "embedding_norm": 1.0
    },
    {
      "module_path": "R2c",
      "library": "pfff-h_program-lang",
      "description": "Converts error codes to JSON format or string representations. Works with error lists and string identifiers. Useful for logging or returning structured error responses in web services.",
      "description_length": 185,
      "index": 221,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Layer_parse_errors",
      "library": "pfff-h_program-lang",
      "description": "Handles visualization of parsing errors by generating layers for error density. Works with parsing statistics and directory paths to create red/green or heatmap layers. Useful for displaying error frequency across codebases in a visual diff interface.",
      "description_length": 251,
      "index": 222,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Tags_file",
      "library": "pfff-h_program-lang",
      "description": "This module defines a data structure for managing tags in a `TAGS` file, including operations to generate both standard and vi-compatible tag files, create individual tags, and add method tags in unambiguous contexts. It works with lists of files paired with their associated tags, where each tag includes metadata like name, line number, byte offset, and kind. Concrete use cases include generating navigation files for code editors and enriching source code with structured tag information.",
      "description_length": 492,
      "index": 223,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Token_helpers_ml",
      "library": "pfff-lang_ml",
      "description": "This module provides functions to inspect and transform tokens, including checking token types like EOF or comments, extracting token kinds and source information, and modifying token metadata. It operates on `Parser_ml.token` and `Parse_info.t` types, enabling precise manipulation of parsed elements. Concrete use cases include filtering comments during code analysis, tracking source locations for error reporting, and adjusting token information during preprocessing.",
      "description_length": 471,
      "index": 224,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Lib_parsing_ml",
      "library": "pfff-lang_ml",
      "description": "This module provides functions to locate and filter source files, including `.ml`, `.mli`, and `.cmt` files, from directories or file lists. It operates on file paths and parses intermediate representation data to extract source code information. Concrete use cases include scanning OCaml projects for specific file types and analyzing parsed syntax trees for tooling purposes.",
      "description_length": 377,
      "index": 225,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parse_ml",
      "library": "pfff-lang_ml",
      "description": "This module parses OCaml source files into concrete syntax trees and token lists. It provides functions to extract the program structure, individual tokens, or both, along with parsing statistics. Use it when analyzing or transforming OCaml code, such as in linters, refactoring tools, or syntax inspectors.",
      "description_length": 307,
      "index": 226,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Meta_cst_ml",
      "library": "pfff-lang_ml",
      "description": "Converts a concrete syntax tree (CST) node representing any OCaml construct into a corresponding value representation. Works with `Cst_ml.any` and `Ocaml.v` types. Useful for transforming parsed syntax trees into executable values during evaluation or analysis tasks.",
      "description_length": 267,
      "index": 227,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Parser_ml",
      "library": "pfff-lang_ml",
      "description": "This module defines a comprehensive set of lexical tokens for OCaml source code, including identifiers, literals, keywords, operators, and punctuation, each annotated with parsing metadata. It provides entry points for parsing OCaml interfaces and implementations into concrete syntax trees using a given lexer function. Concrete use cases include building custom parsers, linters, or transformers for OCaml code that require precise tokenization and syntactic analysis.",
      "description_length": 470,
      "index": 228,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Unit_parsing_ml",
      "library": "pfff-lang_ml",
      "description": "This module defines a single unit test value `unittest` that serves as an entry point for executing a suite of tests using the OUnit testing framework. It is specifically designed to work with OUnit's test type, enabling structured test case organization and execution. A concrete use case includes integrating with a test runner to validate the correctness of functions within an OCaml project.",
      "description_length": 395,
      "index": 229,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Visitor_ml",
      "library": "pfff-lang_ml",
      "description": "This module defines a visitor pattern for traversing and transforming OCaml abstract syntax trees. It provides a structured way to customize processing of various AST nodes such as expressions, patterns, type declarations, and module expressions through a record of per-node callbacks. It is used to implement custom analysis, rewriting, or code generation passes over parsed OCaml code.",
      "description_length": 387,
      "index": 230,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_ml",
      "library": "pfff-lang_ml",
      "description": "This module implements lexical analysis for parsing source code, converting raw character streams into tokens and handling string literals, comments, and keywords. It processes `Lexing.lexbuf` input buffers, using hash tables to map keywords to tokens and maintaining internal state for multi-state lexing. Concrete uses include scanning identifiers, parsing quoted strings with escapes, and extracting token metadata for error reporting.",
      "description_length": 438,
      "index": 231,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_parsing_ml",
      "library": "pfff-lang_ml",
      "description": "Parses OCaml source files into token streams for testing and analysis. It operates on filenames and produces structured token output, enabling validation of parsing logic against expected lexical forms. Useful for verifying compiler front-end behavior on sample code inputs.",
      "description_length": 274,
      "index": 232,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Cst_ml",
      "library": "pfff-lang_ml",
      "description": "This module defines core abstract syntax tree (AST) types for representing OCaml code, including types for tokens, names, type expressions, module expressions, and top-level constructs. It provides functions to extract string and token information from names, and utilities to flatten lists with specific delimiters like commas or pipes. Concrete use cases include parsing and manipulating OCaml source code, analyzing type declarations, and extracting module and identifier information from qualified names.",
      "description_length": 508,
      "index": 233,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Common2.Infix",
      "library": "commons",
      "description": "Implements function application and string pattern matching operations. Provides the pipeline operator `|>` for chaining function calls, `=~` for substring matching, and `==~` for regex matching. Useful for concise data transformation and validation workflows, such as filtering log entries or processing text input.",
      "description_length": 316,
      "index": 234,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Common2.IntMap",
      "library": "commons",
      "description": "This module implements a map data structure with integer keys and arbitrary value types. It provides standard operations for inserting, removing, and querying key-value pairs, as well as transforming and comparing maps. Use cases include tracking integer-indexed configurations, managing sparse arrays, and efficiently associating data with numeric identifiers.",
      "description_length": 361,
      "index": 235,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Common2.IntIntMap",
      "library": "commons",
      "description": "This module implements a map with integer pairs as keys and arbitrary values, supporting standard operations like insertion, lookup, iteration, and transformation. It provides functions for adding or removing key-value pairs, checking membership, applying functions over keys and values, and comparing or folding over the map's contents. Use cases include tracking 2D grid positions with associated data, managing sparse matrices, or handling configurations indexed by dual integer identifiers.",
      "description_length": 494,
      "index": 236,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Common2.StringSet",
      "library": "commons",
      "description": "This implementation provides set-theoretic operations for manipulating collections of unique strings, including union, intersection, difference, filtering, and partitioning. It operates on a string set type that supports efficient membership checks, element iteration, and conversion to ordered lists. Typical applications include managing unique string identifiers, processing categorical data, and implementing algorithms requiring fast set operations like dependency resolution or text analysis.",
      "description_length": 498,
      "index": 237,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Common2.BasicType",
      "library": "commons",
      "description": "Defines common data types used throughout the codebase, including type aliases like `filename` for string. Provides basic type definitions that simplify handling of file paths and similar string-based data. Useful for ensuring consistency in functions that read from or write to files.",
      "description_length": 285,
      "index": 238,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Common.SMap",
      "library": "commons",
      "description": "This module provides operations for creating, modifying, and querying sorted maps with string keys and arbitrary value types, supporting efficient ordered key operations like min/max retrieval, deterministic iteration, and order-preserving transformations. It works with polymorphic map structures that maintain physical equality optimizations and sorted key order, enabling use cases such as configuration management, ordered dictionary processing, or deterministic traversal of key-value pairs. Key features include merging, filtering, and conversion to/from sequences while preserving key order guarantees.",
      "description_length": 609,
      "index": 239,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Common2.ArithFloatInfix",
      "library": "commons",
      "description": "This module defines standard arithmetic operations for both floating-point and integer types, including addition, subtraction, multiplication, and division. It also provides in-place addition for floating-point references. These operations are used for direct numerical computations where precise control over arithmetic behavior is required.",
      "description_length": 342,
      "index": 240,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Map_",
      "library": "commons",
      "description": "This module implements purely applicative association tables using balanced binary trees, supporting key-value operations such as insertion, lookup, deletion, and iteration. It works with any ordered key type and values of arbitrary type, maintaining logarithmic time complexity for core operations. Concrete use cases include efficiently managing configuration settings, tracking symbol tables in compilers, and implementing caches with fast key-based access.",
      "description_length": 460,
      "index": 241,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Set_",
      "library": "commons",
      "description": "This module provides purely functional operations for creating, modifying, and querying sets of ordered elements, including insertion, membership checks, set algebra (union, intersection, difference), and transformations like splitting or extracting min/max values. It works with immutable balanced binary trees that enforce element ordering, ensuring logarithmic time complexity for key operations. These structures are ideal for maintaining sorted, unique collections where efficient lookups and deterministic order are required.",
      "description_length": 531,
      "index": 242,
      "embedding_norm": 1.0
    },
    {
      "module_path": "OUnit",
      "library": "commons",
      "description": "The module provides assertion primitives for validating conditions (e.g., equality checks, exception handling) and utilities to structure test suites using labeled groups or hierarchical lists. It operates on test cases organized as nodes in a tree structure, with support for tracking execution results, error events, and test paths during traversal. Specific use cases include automated unit testing workflows with setup/teardown hooks, command-line-driven test filtering, and generating diagnostic output in text or custom formats.",
      "description_length": 534,
      "index": 243,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "File_type",
      "library": "commons",
      "description": "This module defines a variant type representing different categories of files, such as source code, binary, text, media, and archives. It includes functions to determine a file's type from its name, check if a file is textual, or if it matches specific formats like JSON or Syncweb objects. These operations are used to classify and process files in build systems or code analysis tools.",
      "description_length": 387,
      "index": 244,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Common2",
      "library": "commons",
      "description": "This module provides utility operations for data manipulation, system interaction, and structured computation, working with primitives, collections, and custom types like vectors and file paths. It supports tasks such as file traversal with filtering, temporal calculations, and regression testing with score tracking, while integrating submodules for specialized data handling. The pipeline operator `|>` enables function chaining, `IntMap` and `IntPairMap` handle integer-keyed and pair-keyed data, and `StrSet` offers efficient string set operations for tasks like dependency resolution. Arithmetic functions support precise numerical operations, and type definitions like `filename` ensure consistency in file-based workflows.",
      "description_length": 730,
      "index": 245,
      "embedding_norm": 0.9999998807907104
    },
    {
      "module_path": "Ocaml",
      "library": "commons",
      "description": "The module enables conversion between OCaml values and structured representations (`v`), recursive transformation of algebraic data types (including tuples, sums, options, lists, and custom `either`/`either3`), and side-effect-driven traversal of boxed values via visitor patterns. It operates on both primitive types (booleans, integers, strings) and complex nested structures, supporting use cases like serialization, AST manipulation, and deep equality checks through its structural mapping and visitor-based operations.",
      "description_length": 523,
      "index": 246,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Common",
      "library": "commons",
      "description": "This module offers utilities for string manipulation, file I/O, list transformations, and CLI construction, working with core types like lists, hash tables, options, and `SMap`. It enables scripting file-processing pipelines, building CLI tools with structured arguments, and managing hierarchical data with type-safe operations. The included map module supports sorted, ordered key-value structures with efficient querying and deterministic iteration, ideal for configuration and dictionary processing. Together, they provide a cohesive toolkit for data transformation, structured workflows, and performance-critical scripting tasks.",
      "description_length": 634,
      "index": 247,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Dumper",
      "library": "commons",
      "description": "Converts any value to its string representation for debugging or logging. Works with all OCaml data types, including primitives, records, and variants. Useful for inspecting complex data structures during development or generating diagnostic output.",
      "description_length": 249,
      "index": 248,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ograph_simple",
      "library": "commons_ocollection",
      "description": "Converts a mutable graph structure into a string representation using provided key and node formatting functions, then writes the result to a file. Works with generic graph types containing keys, nodes, and edges. Useful for debugging or persisting graph state to disk with custom formatting.",
      "description_length": 292,
      "index": 249,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ograph_extended",
      "library": "commons_ocollection",
      "description": "This module provides depth-first search traversal functions for mutable graphs, allowing iteration over nodes with or without tracking the path. It supports graph visualization through customizable printing functions that generate output files compatible with Graphviz. These operations are used to analyze and visualize control flow graphs or dependency structures in program analysis tasks.",
      "description_length": 392,
      "index": 250,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Osetb",
      "library": "commons_ocollection",
      "description": "Represents an empty set value for polymorphic set types. Used to construct new sets with no elements, compatible with any element type. Directly initializes an empty collection for operations like union, intersection, or membership checks.",
      "description_length": 239,
      "index": 251,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Oset",
      "library": "commons_ocollection",
      "description": "This module defines infix operators for set operations like membership, union, intersection, difference, subset checks, and equality comparisons. It works with objects that support set-like methods such as `mem`, `union`, `inter`, `minus`, `is_subset_of`, and `is_equal`. These operators simplify set manipulations in functional pipelines, especially when composing transformations over custom or abstract set implementations.",
      "description_length": 426,
      "index": 252,
      "embedding_norm": 1.0
    },
    {
      "module_path": "SetPt",
      "library": "commons_ocollection",
      "description": "This module offers trie-based integer set operations using bitwise logic for efficient branching, supporting membership checks, insertions, deletions, and set algebra (union, intersection, difference, subset checks). It works with integer sets represented as binary tries, leveraging bitwise operations to handle sparse distributions and hierarchical structure. The design suits applications requiring fast set operations on integers with sparse or widely varying bit representations, such as network prefix routing, sparse bitset manipulations, or priority queue implementations with integer keys.",
      "description_length": 598,
      "index": 253,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "This module defines core data structures for representing OCaml abstract syntax trees, including types, expressions, patterns, and program items. It supports operations for constructing and deconstructing language elements like function calls, type applications, let bindings, and control structures. Concrete use cases include building custom compilers, linters, or code transformation tools that process OCaml source code.",
      "description_length": 424,
      "index": 254,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Test_analyze_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "This module defines a function `actions` that returns a list of command-line operations for analyzing test files. It works with file paths and command-line arguments to process and evaluate test cases. Concrete use cases include running test analysis tools from the command line, such as checking test coverage or validating test structure.",
      "description_length": 340,
      "index": 255,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ml_to_generic",
      "library": "pfff-lang_ml-analyze",
      "description": "This module provides polymorphic transformation functions for OCaml types like `option`, `list`, and tuples, alongside structural mappings between `Ast_ml` and `G` AST nodes. It operates on abstract syntax trees and parsing-related data, converting elements such as expressions, patterns, and type declarations while preserving source context through helper constructs like `tok` and directional operators. Its primary use cases involve AST normalization, parsing pipeline integration, and bidirectional syntax translation in compiler or language tooling workflows.",
      "description_length": 565,
      "index": 256,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Tags_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "Reads source files and directories to extract tag definitions, returning a list of filenames paired with their associated tags. Works with file paths and tag data structures, handling recursive directory traversal. Useful for generating tag databases or analyzing codebases with custom annotations.",
      "description_length": 298,
      "index": 257,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Unit_analyze_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "This module defines a single unit test value compatible with the OUnit testing framework. It allows developers to execute and integrate test cases within larger test suites or run them directly via OUnit's test runner. The test value typically contains assertions to verify correctness of functions or modules under test.",
      "description_length": 321,
      "index": 258,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Coverage_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "Converts coverage data keyed by filenames into a human-readable format using only the base names of files, given a directory path to resolve relative paths. Works with coverage data structures that map file paths to line coverage information. Useful for generating simplified coverage reports where full file paths are unnecessary or should be hidden.",
      "description_length": 351,
      "index": 259,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Database_light_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "This module processes a list of file paths to build a structured database representation. It parses and organizes data from specified input files into a typed database structure. Useful for initializing a database from disk with optional progress logging.",
      "description_length": 255,
      "index": 260,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Module_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "This module defines types for module names and long names, and provides functions to extract a module name from a filename and determine the top module from a graph node. It works with string-based identifiers and string lists for hierarchical names. Useful in code analysis tools for mapping file paths to module structures and resolving module dependencies from graph representations.",
      "description_length": 386,
      "index": 261,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_ml_build",
      "library": "pfff-lang_ml-analyze",
      "description": "This component provides utilities for structural mapping between concrete and abstract syntax representations during OCaml parsing, focusing on syntactic constructs like separated lists, parentheticals, and bracketed expressions. It operates on heterogeneous lists (`'a Common.either list`), `Cst_ml` nodes, and `A`-typed AST elements, preserving source positions while translating type definitions, match cases, let-bindings, and modular structures. Key use cases include converting labeled patterns, handling optional values in recursive translations, and bridging CST fragments to typed AST nodes for compiler intermediate representations.",
      "description_length": 642,
      "index": 262,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Graph_code_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "Builds a directed graph representing dependencies between OCaml source files. It takes a directory path and a list of filenames, parsing each file to determine module dependencies based on `open` statements and file structure. This function is used to analyze and visualize project structure or to determine compilation order in build systems.",
      "description_length": 343,
      "index": 263,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Highlight_ml",
      "library": "pfff-lang_ml-analyze",
      "description": "Visits an OCaml program's concrete syntax tree and token list, applying a tagging function to each parsed element based on its syntactic category. It works with OCaml's CST (`Cst_ml.program`) and lexer tokens (`Parser_ml.token`), using a configurable highlighting strategy. This module is used to implement syntax highlighting for OCaml code in editors or documentation tools.",
      "description_length": 376,
      "index": 264,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Token_helpers_nw",
      "library": "pfff-lang_nw",
      "description": "This module provides functions to inspect and transform tokens from the `Lexer_nw` module. It includes checks for end-of-file and comment tokens, extracts token kinds and parse info, and applies transformations to token metadata. These operations support tasks like syntax analysis, code filtering, and preprocessing.",
      "description_length": 317,
      "index": 265,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Lexer_nw",
      "library": "pfff-lang_nw",
      "description": "This module defines a lexer for parsing structured text with support for comments, symbols, numbers, and nested modes like verbatim blocks and noweb chunks. It processes input using lexing functions tied to state modes, tracking positions with `Parse_info.t`. It handles transitions between modes, tokenizing content appropriately in contexts like LaTeX or literate programming.",
      "description_length": 378,
      "index": 266,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Test_parsing_nw",
      "library": "pfff-lang_nw",
      "description": "This module processes network-related test files by tokenizing their contents and defining command-line actions for test execution. It operates on filenames and token streams, specifically handling network protocol test cases. Concrete use includes parsing and validating network message formats from test files.",
      "description_length": 312,
      "index": 267,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Ast_nw",
      "library": "pfff-lang_nw",
      "description": "This module processes abstract syntax trees (ASTs) parsed from the `Ast_fuzzy` module, providing functions to traverse, transform, and analyze tree structures representing program code. It works directly with the `program` type, which is an alias for `Ast_fuzzy.trees`, and offers operations such as node rewriting, pattern matching, and structural validation. Concrete use cases include implementing custom linters, performing code transformations, and extracting specific syntactic constructs from parsed OCaml code.",
      "description_length": 518,
      "index": 268,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Parse_nw",
      "library": "pfff-lang_nw",
      "description": "This module handles parsing of source code files into abstract syntax trees and token lists. It provides functions to parse a file into a structured program representation along with parsing statistics, extract only the token stream from a file, and perform fuzzy parsing for partial or incomplete input. Concrete use cases include building language tools like linters, code analyzers, and transformation utilities that require syntactic processing of OCaml-like code.",
      "description_length": 468,
      "index": 269,
      "embedding_norm": 1.0
    }
  ],
  "filtering": {
    "total_modules_in_package": 328,
    "meaningful_modules": 270,
    "filtered_empty_modules": 58,
    "retention_rate": 0.823170731707317
  },
  "statistics": {
    "max_description_length": 730,
    "min_description_length": 185,
    "avg_description_length": 384.68148148148146,
    "embedding_file_size_mb": 0.9811067581176758
  }
}