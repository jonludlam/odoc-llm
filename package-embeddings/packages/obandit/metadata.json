{
  "package": "obandit",
  "embedding_model": "BAAI/bge-base-en-v1.5",
  "embedding_dimension": 1024,
  "total_modules": 14,
  "creation_timestamp": "2025-06-18T16:32:49.581108",
  "modules": [
    {
      "module_path": "Obandit.MakeAlphaPhiUCB",
      "description": "Computes the inverse of the Legendre-Fenchel transform of a convex function using a given parameter alpha. Operates on integer action counts and floating-point values to model decision-making dynamics. Used to derive confidence bounds in reinforcement learning algorithms.",
      "description_length": 272,
      "index": 0,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeAlphaUCB",
      "description": "Provides functions to compute upper confidence bounds using a fixed number of actions and an alpha parameter for exploration. Operates on integers representing actions and floats for confidence calculations. Used to select optimal actions in reinforcement learning scenarios with bounded exploration.",
      "description_length": 300,
      "index": 1,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeUCB1",
      "description": "Provides functions to compute and manage the upper confidence bound for multi-armed bandit problems, including action selection and confidence interval calculations. Operates on integers representing action indices and floating-point values for reward estimates and confidence bounds. Used to implement exploration-exploitation strategies in reinforcement learning scenarios.",
      "description_length": 375,
      "index": 2,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeParametrizableEpsilonGreedy",
      "description": "Provides functions to compute action probabilities using an epsilon-greedy strategy, with a configurable number of actions and a rate that adjusts over time. Operates on integers representing action counts and floats for probability values. Used to implement exploration-exploitation trade-offs in reinforcement learning agents.",
      "description_length": 328,
      "index": 3,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeDecayingEpsilonGreedy",
      "description": "Provides functions to compute decaying epsilon-greedy exploration rates using action counts and hyperparameters $ c $ and $ d $. Operates on integers representing action counts and floats for exploration thresholds. Used to balance exploration and exploitation in reinforcement learning policies with diminishing exploration over time.",
      "description_length": 335,
      "index": 4,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeEpsilonGreedy",
      "description": "Provides functions to compute and apply epsilon-greedy action selection, using a fixed number of actions and an exploration rate. Operates on integers representing action indices and floats for probability calculations. Used to balance exploration and exploitation in reinforcement learning policies.",
      "description_length": 300,
      "index": 5,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeExp3",
      "description": "Provides functions to compute action rates based on a fixed or decaying schedule, using integer indices and floating-point values. Operates on integers representing action counts and returns corresponding float-based rates. Used to model dynamic decision-making processes where action frequency changes over time.",
      "description_length": 313,
      "index": 6,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeDecayingExp3",
      "description": "Provides functions to compute decay factors and probabilities for a three-layer exponential decay model. Operates on integers representing action counts and floating-point values for probability weights. Used to adjust action selection likelihoods in reinforcement learning scenarios with diminishing returns.",
      "description_length": 309,
      "index": 7,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeFixedExp3",
      "description": "Provides functions to retrieve the number of actions $ K $ and the fixed learning rate $ \\eta $, used in reinforcement learning algorithms. Operates on integer and floating-point values representing policy parameters. Used to configure agent behavior in environments with a fixed action space and learning rate.",
      "description_length": 311,
      "index": 8,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeHorizonExp3",
      "description": "Provides functions to compute and manage action sequences for a horizon-based optimization problem, using integers to represent the number of actions and the planning horizon. Operates on precomputed action sets and time-step configurations to generate optimized execution plans. Used to configure and analyze decision processes in dynamic environments with fixed action sets and time constraints.",
      "description_length": 397,
      "index": 9,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.WrapRange",
      "description": "Tracks and updates the state of a bandit algorithm, returning the next action and updated state after each reward is applied. Operates on the `bandit` type, which represents the internal state of the algorithm. Used to simulate sequential decision-making in reinforcement learning scenarios, such as selecting optimal actions in a multi-armed bandit problem.",
      "description_length": 358,
      "index": 10,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Obandit.WrapRange01",
      "description": "Computes the next action and updates the state of a bandit algorithm given a reward, using a specific internal representation. Operates on the `bandit` type, which encapsulates the algorithm's state and parameters. Used to simulate sequential decision-making in reinforcement learning scenarios where actions are selected based on accumulated rewards.",
      "description_length": 351,
      "index": 11,
      "embedding_norm": 1.0
    },
    {
      "module_path": "obandit",
      "description": "Provides functions to initialize and update bandit strategies, select actions, and compute rewards using EXP3, UCB1, and Epsilon-greedy algorithms. Works with arrays of weights, probabilities, and reward histories. Used to implement decision-making processes in scenarios like online advertising and resource allocation.",
      "description_length": 320,
      "index": 12,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit",
      "description": "Manages a multi-armed bandit algorithm, tracking state and selecting actions based on received rewards. It processes a float reward value to determine the next action index and updates its internal state accordingly. Works with a custom `bandit` type representing the algorithm's state throughout sequential decision-making.",
      "description_length": 324,
      "index": 13,
      "embedding_norm": 1.0
    }
  ],
  "filtering": {
    "total_modules_in_package": 14,
    "meaningful_modules": 14,
    "filtered_empty_modules": 0,
    "retention_rate": 1.0
  },
  "statistics": {
    "max_description_length": 397,
    "min_description_length": 272,
    "avg_description_length": 328.07142857142856,
    "embedding_file_size_mb": 0.051334381103515625
  }
}