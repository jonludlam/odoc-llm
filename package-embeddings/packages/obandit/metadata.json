{
  "package": "obandit",
  "embedding_model": "Qwen/Qwen3-Embedding-8B",
  "embedding_dimension": 4096,
  "total_modules": 13,
  "creation_timestamp": "2025-08-15T12:12:38.132441",
  "modules": [
    {
      "module_path": "Obandit.MakeAlphaPhiUCB",
      "library": "obandit",
      "description": "This module implements the $(\\alpha,\\psi)$-UCB algorithm for stochastic regret minimization. It maintains a bandit state with reward estimates and selection counts, using a custom parameter module `P` to define algorithm behavior. The `step` function selects an arm based on upper confidence bounds and updates the bandit state with the observed reward.",
      "description_length": 353,
      "index": 0,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeHorizonExp3",
      "library": "obandit",
      "description": "This module implements the Exp3 algorithm with a horizon-based learning rate for adversarial regret minimization. It maintains a bandit policy state and updates it based on observed rewards, selecting actions and returning the updated state. It works with a bandit policy type and is used in scenarios where rewards are chosen adversarially over a fixed time horizon.",
      "description_length": 367,
      "index": 1,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeDecayingEpsilonGreedy",
      "library": "obandit",
      "description": "This module implements a decaying epsilon-greedy strategy for multi-armed bandit problems, where the exploration rate decreases over time according to a predefined schedule. It maintains bandit estimates as a `bandit` type and provides `initialBandit` for initialization and `step` to select actions and update state based on observed rewards. It is suitable for scenarios requiring adaptive exploration in dynamic environments, such as online recommendation systems or A/B testing with time-sensitive user behavior.",
      "description_length": 516,
      "index": 2,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.WrapRange01",
      "library": "obandit",
      "description": "The module provides the `initialBandit` value and `step` function for working with bandits that operate within a fixed [0,1] range. It supports bandit algorithms like EXP, UCB, and Epsilon-greedy by managing reward scaling and action selection within this normalized range. This module is used when implementing bandit strategies that require bounded rewards, such as in recommendation systems or online advertising where rewards are binary or normalized scores.",
      "description_length": 462,
      "index": 3,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.WrapRange",
      "library": "obandit",
      "description": "This module implements the doubling trick heuristic for bandit algorithms, dynamically rescaling rewards to an unknown range by restarting the algorithm and adjusting the range bounds when outliers are observed. It works with any bandit algorithm and range parameterization, tracking and adapting reward intervals during execution. Concrete use cases include running EXP, UCB, or Epsilon-greedy algorithms in environments where reward bounds are not known a priori.",
      "description_length": 465,
      "index": 4,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeExp3",
      "library": "obandit",
      "description": "Implements the EXP3 algorithm for adversarial multi-armed bandit problems, using a parametrized learning rate to balance exploration and exploitation. Works with a bandit policy type representing the algorithm's internal state, and performs updates based on observed rewards. Useful for online decision-making scenarios where reward distributions are non-stationary or adversarially chosen.",
      "description_length": 390,
      "index": 5,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Obandit.MakeAlphaUCB",
      "library": "obandit",
      "description": "This module implements the \u03b1-UCB algorithm for stochastic multi-armed bandits, providing `initialBandit` to initialize the bandit state and `step` to select actions and update state based on rewards. It operates on the `bandit` type, which tracks reward estimates and arm statistics. Use it for sequential decision-making tasks with stationary reward distributions, such as online advertising or A/B testing.",
      "description_length": 408,
      "index": 6,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeEpsilonGreedy",
      "library": "obandit",
      "description": "Implements an epsilon-greedy multi-armed bandit algorithm with a fixed exploration rate. It maintains and updates bandit estimates using a step function that selects actions and returns updated state. This module is suitable for scenarios where you need to balance exploration and exploitation in a fixed ratio, such as A/B testing or online recommendation systems.",
      "description_length": 365,
      "index": 7,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeFixedExp3",
      "library": "obandit",
      "description": "This module implements the EXP3 algorithm for adversarial multi-armed bandit problems, using a decaying learning rate to minimize regret. It maintains a probability distribution over arms and updates weights based on observed losses. The `step` function selects an arm and updates the bandit state using the received reward, returning the chosen arm and the updated state.",
      "description_length": 372,
      "index": 8,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Obandit.MakeUCB1",
      "library": "obandit",
      "description": "Implements the UCB1 algorithm for stochastic multi-armed bandit problems. It maintains a bandit state with reward estimates and selection counts, updating them based on observed rewards. The `step` function selects an arm using the UCB1 policy and updates the bandit state with the received reward.",
      "description_length": 298,
      "index": 9,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Obandit.MakeParametrizableEpsilonGreedy",
      "library": "obandit",
      "description": "This module implements an epsilon-greedy multi-armed bandit algorithm with a parametrizable exploration rate. It maintains and updates bandit estimates using a provided parameter module P to determine exploration behavior. The module operates on the `bandit` type, which tracks action values and selection counts, and provides the `step` function to select actions and update the bandit state based on observed rewards.",
      "description_length": 419,
      "index": 10,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit.MakeDecayingExp3",
      "library": "obandit",
      "description": "This module implements the Exp3 algorithm with a decaying learning rate for adversarial bandit problems. It maintains a probability distribution over actions, updating weights based on observed losses to minimize regret. Use it when dealing with non-stochastic reward environments where the loss sequence may be adversarially chosen.",
      "description_length": 333,
      "index": 11,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Obandit",
      "library": "obandit",
      "description": "This module provides implementations of multi-armed bandit algorithms for action selection and regret minimization in sequential decision-making tasks. It operates on bandit states (`bandit` or `banditPolicy`) using strategies like UCB variants (\u03b1-UCB, UCB1), epsilon-greedy approaches (fixed/decaying/parametrizable), and EXP3-based methods, which adapt to stochastic or adversarial reward environments. Specific use cases include adversarial regret minimization, reward normalization in [0,1], and horizon-aware learning with decaying exploration/exploitation parameters.",
      "description_length": 573,
      "index": 12,
      "embedding_norm": 1.0
    }
  ],
  "filtering": {
    "total_modules_in_package": 13,
    "meaningful_modules": 13,
    "filtered_empty_modules": 0,
    "retention_rate": 1.0
  },
  "statistics": {
    "max_description_length": 573,
    "min_description_length": 298,
    "avg_description_length": 409.3076923076923,
    "embedding_file_size_mb": 0.18888568878173828
  }
}