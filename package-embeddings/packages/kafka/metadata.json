{
  "package": "kafka",
  "embedding_model": "BAAI/bge-base-en-v1.5",
  "embedding_dimension": 1024,
  "total_modules": 6,
  "creation_timestamp": "2025-06-18T16:29:44.989692",
  "modules": [
    {
      "module_path": "Kafka_helpers.Kafka_consumer",
      "description": "Processes Kafka messages by folding over partitions, topics, or custom queues, applying a user-defined accumulator function. Operates on Kafka messages, partitions, and offsets, allowing precise control over consumption flow. Used to aggregate data from specific partitions, process entire topics, or manage custom message streams with offset tracking.",
      "description_length": 352,
      "index": 0,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Kafka_helpers.Kafka_producer",
      "description": "Handles message streaming to Kafka with custom error handling and partitioning. Processes iterable data sources and emits messages to specified Kafka topics or partitions using configurable producer and topic properties. Supports error recovery strategies and direct error propagation for reliable message delivery.",
      "description_length": 315,
      "index": 1,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Kafka.Metadata",
      "description": "type topic_metadata = { title: string; author: string; timestamp: int; tags: string list; is_published: bool } Provides functions to extract titles and authors from metadata, validate publication status, and filter entries by tag. Operates on structured records containing textual and temporal data. Used to preprocess blog posts for display in a categorized archive.",
      "description_length": 367,
      "index": 2,
      "embedding_norm": 1.0
    },
    {
      "module_path": "kafka",
      "description": "Handles message production and consumption from Apache Kafka, supporting topic configuration, message serialization, and delivery guarantees. Operates with byte sequences, messages, and Kafka-specific metadata structures. Used to implement real-time data pipelines and event-driven architectures.",
      "description_length": 296,
      "index": 3,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Kafka_helpers",
      "description": "Processes Kafka messages by applying user-defined functions to aggregate data from partitions, topics, or custom queues, while tracking offsets for precise control. Enables streaming of iterable data to Kafka with customizable partitioning, error handling, and recovery strategies. Key data types include message records, partitions, offsets, and producer configurations. Examples include aggregating real-time data from specific partitions or reliably sending batched logs to a Kafka topic with custom error handling.",
      "description_length": 518,
      "index": 4,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Kafka",
      "description": "Manages structured blog post metadata with fields like title, author, timestamp, tags, and publication status. Offers extraction of key attributes, validation of publish status, and filtering by tags to organize content. Processes records to prepare them for display in a categorized archive. Enables tasks such as listing posts by a specific author or retrieving only published entries with certain tags.",
      "description_length": 405,
      "index": 5,
      "embedding_norm": 0.9999999403953552
    }
  ],
  "filtering": {
    "total_modules_in_package": 6,
    "meaningful_modules": 6,
    "filtered_empty_modules": 0,
    "retention_rate": 1.0
  },
  "statistics": {
    "max_description_length": 518,
    "min_description_length": 296,
    "avg_description_length": 375.5,
    "embedding_file_size_mb": 0.022246360778808594
  }
}