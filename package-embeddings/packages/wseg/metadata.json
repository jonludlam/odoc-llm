{
  "package": "wseg",
  "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
  "embedding_dimension": 1024,
  "total_modules": 3,
  "creation_timestamp": "2025-07-15T23:05:11.749954",
  "modules": [
    {
      "module_path": "Wseg.Dict",
      "library": "wseg",
      "description": "This module processes sequences of strings to build weighted dictionaries for word segmentation, using a tree-based structure where entries pair strings with numeric weights. It supports operations like normalizing counts into probabilities, generating segmentation candidates, and selecting results based on input strings and constraints. The child module offers no additional functionality, leaving all implementation and extension work to the parent. Example uses include training a dictionary on a corpus and querying it to segment new input strings probabilistically.",
      "description_length": 572,
      "index": 0,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Wseg.MMSEG",
      "library": "wseg",
      "description": "Implements the MMSEG algorithm for Chinese word segmentation using a sequence of rules to process lists of dictionary chunks. It applies matching strategies like maximum matching, average word length, variance minimization, and probability maximization to disambiguate word boundaries. Useful for tokenizing continuous Chinese text into meaningful words based on statistical and dictionary-driven criteria.",
      "description_length": 406,
      "index": 1,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Wseg",
      "library": "wseg",
      "description": "This module combines dictionary-building and segmentation algorithms to handle word segmentation tasks, particularly for Chinese text. It uses weighted tree-based dictionaries to model word probabilities and applies rule-based strategies like maximum matching and variance minimization to generate and select optimal segmentations. Key operations include training dictionaries from corpora, normalizing weights, and segmenting input strings based on learned statistics. Example uses include training on a Chinese corpus and segmenting new text into words using probabilistic and rule-driven methods.",
      "description_length": 599,
      "index": 2,
      "embedding_norm": 1.0
    }
  ],
  "filtering": {
    "total_modules_in_package": 4,
    "meaningful_modules": 3,
    "filtered_empty_modules": 1,
    "retention_rate": 0.75
  },
  "statistics": {
    "max_description_length": 599,
    "min_description_length": 406,
    "avg_description_length": 525.6666666666666,
    "embedding_file_size_mb": 0.011332511901855469
  }
}