{
  "package": "mecab",
  "embedding_model": "Qwen/Qwen3-Embedding-0.6B",
  "embedding_dimension": 1024,
  "total_modules": 9,
  "creation_timestamp": "2025-07-15T23:06:39.526963",
  "modules": [
    {
      "module_path": "Mecab.Unicode.StringIntf",
      "library": "mecab",
      "description": "This module represents a Unicode character sequence with indexed access, supporting operations like character retrieval, index navigation, and bounds checking. It works with Unicode characters (`UChar.t`) and integer indices, providing direct access to character data and positional traversal. Concrete use cases include text processing in MeCab's analysis pipeline, such as token boundary detection and character-level transformations during normalization.",
      "description_length": 457,
      "index": 0,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Mecab.Unicode.StringIntf-Buf",
      "library": "mecab",
      "description": "This module provides functions to manipulate Unicode character buffers, including creating buffers, adding characters or strings, and combining buffers. It works with the `buf` type for accumulating sequences of Unicode characters. Concrete use cases include building normalized text output during MeCab's morphological analysis and handling Unicode transformations efficiently.",
      "description_length": 378,
      "index": 1,
      "embedding_norm": 0.9999999403953552
    },
    {
      "module_path": "Mecab.Unicode.UTF8",
      "library": "mecab",
      "description": "This module offers utilities for manipulating UTF-8 strings with Unicode-aware operations such as normalization, case conversion, whitespace trimming, and character-class transformations (e.g., ASCII/kana width adjustments). It operates on `CamomileLibraryDefault.Camomile.UTF8.t` strings and `UChar.t` characters, enabling precise handling of Unicode properties. These tools are designed for text preprocessing in MeCab's morphological analysis pipeline, particularly for tasks like dictionary normalization (neologd), numeric form standardization, and converting between ASCII and full-width/half-width representations.",
      "description_length": 621,
      "index": 2,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Mecab.Unicode.Char",
      "library": "mecab",
      "description": "This module provides character classification operations for Unicode text processing, focusing on script-range detection (ASCII, Latin-1, Cyrillic, Japanese kana/kanji, etc.) and property checks (digits, whitespace, punctuation). It works with Unicode characters represented as `uchar` values, enabling predicate-based filtering for lexical analysis and normalization tasks. These utilities are specifically used in MeCab's morphological analysis pipeline to handle multilingual text segmentation and Japanese-specific character categorization.",
      "description_length": 544,
      "index": 3,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Mecab.Unicode.Make",
      "library": "mecab",
      "description": "This module offers operations for transforming and processing Unicode character sequences (`S.t`) through normalization, case conversion, filtering, and encoding standardization. It works with Unicode strings and lists, providing functions to handle full-width/half-width character conversion, neologd-style Japanese text normalization, and ASCII-centric transformations. These utilities are particularly useful for preprocessing multilingual text data before linguistic analysis or encoding-sensitive processing.",
      "description_length": 513,
      "index": 4,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Mecab.Tagger",
      "library": "mecab",
      "description": "This module enables configuration of morphological analysis parameters such as lattice output levels, partial parsing modes, and theta values, along with management of dictionary metadata including file names and character sets. It operates on tagger instances, dictionary information structures, strings, and node lists, supporting serialization via S-expressions for interoperability. Use cases include fine-tuning analysis",
      "description_length": 425,
      "index": 5,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Mecab.Unicode",
      "library": "mecab",
      "description": "This module provides Unicode text normalization specifically tailored for Japanese processing, offering character classification, UTF-8 encoding/decoding, and customizable string operations. It supports direct manipulation of Unicode character sequences with indexed access, efficient accumulation of characters via buffers, and UTF-8 string transformations including normalization, case conversion, and whitespace trimming. Key data types include `UChar.t` for individual characters, `buf` for mutable character buffers, and `CamomileLibraryDefault.Camomile.UTF8.t` for UTF-8 string handling. Example uses include preparing input for MeCab by standardizing numeric forms, converting kana widths, detecting token boundaries, and filtering characters based on script or category during morphological analysis.",
      "description_length": 808,
      "index": 6,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Mecab.Node",
      "library": "mecab",
      "description": "This module represents individual nodes in a parsed sentence, providing detailed morphological analysis. It includes data such as surface form, part-of-speech, node status, and statistical values when enabled. Use this module to extract and analyze linguistic features from MeCab's parsing results, such as identifying word boundaries, grammatical roles, or probabilities for ambiguous constructions.",
      "description_length": 400,
      "index": 7,
      "embedding_norm": 1.0
    },
    {
      "module_path": "Mecab",
      "library": "mecab",
      "description": "This module analyzes Japanese text by splitting it into morphemes with part-of-speech tags, operating on UTF-8 strings and integrating Unicode handling for correct character processing. It supports configuration of analysis parameters, dictionary metadata management, and serialization of structures for interoperability, enabling fine-tuning of parsing behavior. Child modules provide Unicode normalization, character manipulation, and node-based analysis of parsed sentences, allowing tasks like input standardization, token boundary detection, and extraction of linguistic features from analysis results. Specific capabilities include customizing parsing modes, transforming UTF-8 strings, and inspecting morpheme details such as surface forms and part-of-speech tags.",
      "description_length": 771,
      "index": 8,
      "embedding_norm": 1.0
    }
  ],
  "filtering": {
    "total_modules_in_package": 9,
    "meaningful_modules": 9,
    "filtered_empty_modules": 0,
    "retention_rate": 1.0
  },
  "statistics": {
    "max_description_length": 808,
    "min_description_length": 378,
    "avg_description_length": 546.3333333333334,
    "embedding_file_size_mb": 0.03314781188964844
  }
}